[
  {
    "objectID": "MP00.html",
    "href": "MP00.html",
    "title": "Will Peters STA9750-2024-FALL",
    "section": "",
    "text": "Hello,\nThank you for having a look at my Website\nIf you want to know more about me, feel free to check out my Linkedin\nThis will be further updated as we go along further.\n\nTest"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Will Peters STA9750-2024-FALL",
    "section": "",
    "text": "Hello,\nThank you for having a look at my Website\nIf you want to know more about me, feel free to check out my Linkedin\nThis will be further updated as we go along further."
  },
  {
    "objectID": "MP02.html",
    "href": "MP02.html",
    "title": "Will Peters STA9750-2024-FALL MP02",
    "section": "",
    "text": "Initially we start by installing all the libraries for R if you are interested in utilizing, and running it on your own.\n\nlibrary(scales) \nlibrary(stringr) \nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse) \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readr) \nlibrary(foreach)\n\n\nAttaching package: 'foreach'\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\nlibrary(doParallel)\n\nLoading required package: iterators\nLoading required package: parallel\n\nlibrary(ggplot2) \nlibrary(tidyr) \nlibrary(plotly) \n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(knitr)\n\nNext we install a function to download the data sources from the imdb website and running it across the different file names as below. This generates the parameter of Base_url, Fname_ext and FILE_URL and the function of get_imdb_file with the input being the fname you see below.\n\nget_imdb_file &lt;- function(fname){ \n  BASE_URL &lt;- \"https://datasets.imdbws.com/\" \n  fname_ext &lt;- paste0(fname, \".tsv.gz\") \n  if(!file.exists(fname_ext)){ \n    FILE_URL &lt;- paste0(BASE_URL, fname_ext) \n    download.file(FILE_URL, \n                  destfile = fname_ext) \n  } \n  as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE)) \n  }\n\nTITLE_RATINGS &lt;- get_imdb_file(\"title.ratings\") |&gt; filter(numVotes &gt;= 100)\n\nRows: 1490228 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): tconst\ndbl (2): averageRating, numVotes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n'TITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\") |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst))'\n\n[1] \"TITLE_PRINCIPALS &lt;- get_imdb_file(\\\"title.principals\\\") |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst))\"\n\nTITLE_PRINCIPALS &lt;- as.data.frame(readr::read_csv(\"title_principals_small.csv\", lazy=FALSE))\n\nRows: 6586689 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): tconst, nconst, category, job, characters\ndbl (1): ordering\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTITLE_BASICS &lt;- get_imdb_file(\"title.basics\") \n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 11176312 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (8): tconst, titleType, primaryTitle, originalTitle, startYear, endYear,...\ndbl (1): isAdult\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt; \n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\nTITLE_CREW &lt;- get_imdb_file(\"title.crew\") \n\nRows: 10517985 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): tconst, directors, writers\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTITLE_CREW &lt;- TITLE_CREW |&gt; \n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\nTITLE_EPISODES &lt;- get_imdb_file(\"title.episode\") \n\nRows: 8583561 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (4): tconst, parentTconst, seasonNumber, episodeNumber\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt; \n  semi_join(TITLE_RATINGS, join_by(tconst == tconst)) \nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt; \n  semi_join(TITLE_RATINGS, join_by(parentTconst == tconst))\n\nNAME_BASICS &lt;- get_imdb_file(\"name.basics\") \n\nRows: 13886887 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): nconst, primaryName, birthYear, deathYear, primaryProfession, known...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; filter(str_count(knownForTitles,\",\") &gt; 1)\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1, TITLE_EPISODES_2) |&gt; distinct() \n\nThe purpose of the below lines is to remove the components of the TITLE_EPISODES as the bit of code above binds it into the data total.\n\nrm(TITLE_EPISODES_1) \nrm(TITLE_EPISODES_2)\n\nThe mutating feature is transforming the columns and providing the datatype such that it will be easier to read and transform later. Task 1\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n  mutate(birthYear = as.numeric(birthYear), \n         deathYear = as.numeric(deathYear)) \n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `birthYear = as.numeric(birthYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt; \n  mutate(startYear = as.numeric(startYear), \n         endYear = as.numeric(endYear),\n         runtimeMinutes = as.numeric(runtimeMinutes), \n         isAdult = as.logical(isAdult))\n\nWarning: There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `startYear = as.numeric(startYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt; \n  mutate(seasonNumber = as.numeric(seasonNumber), \n         episodeNumber = as.numeric(episodeNumber))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `seasonNumber = as.numeric(seasonNumber)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt; \n  mutate(ordering = as.numeric(ordering))\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt; \n  mutate(averageRating = as.numeric(averageRating), \n         numVotes = as.numeric(numVotes))\n\nprint(TITLE_BASICS |&gt; \n        count(titleType, name = \"Countrows\")|&gt;\n        rename(Type=titleType,'Number of media'=Countrows))\n\n           Type Number of media\n1         movie          132336\n2         short           16735\n3     tvEpisode          156904\n4  tvMiniSeries            5939\n5       tvMovie           15056\n6      tvSeries           30021\n7       tvShort             410\n8     tvSpecial            3066\n9         video            9347\n10    videoGame            4693\n\n\nTask 2\n1.\nAnswer is in our data set there are 132,171 Movies, 29,964 TV Series and 156,592 Episodes\n2.\n\nkable(NAME_BASICS |&gt; \n         filter (is.na(deathYear),birthYear&gt;1906) |&gt; \n         arrange(birthYear) |&gt; \n         rename(Name = primaryName, Year_of_Birth = birthYear,Year_of_Death = deathYear,Profession = primaryProfession) |&gt;\n         select(-Profession,-nconst,-knownForTitles) |&gt;\n         head(10),align = \"l\") \n\n\n\n\nName\nYear_of_Birth\nYear_of_Death\n\n\n\n\nAlberto Albani Barbieri\n1907\nNA\n\n\nYevdokiya Alekseyeva\n1907\nNA\n\n\nMalcolm Baker-Smith\n1907\nNA\n\n\nVineta Bastian-Klinger\n1907\nNA\n\n\nGretel Berndt\n1907\nNA\n\n\nEdgar Blatt\n1907\nNA\n\n\nRonald Brantford\n1907\nNA\n\n\nElisa Carreira\n1907\nNA\n\n\nHemchandra Chunder\n1907\nNA\n\n\nJohn Clein\n1907\nNA\n\n\n\n\n\nThe data is showing the list of 10 people as being the oldest living under the assumption that the death year NA is accurate, as the oldest living person was born in 1907. Overall inaccurate, it would require an additional data source to validate the information’\n‘3.’\n\nkable (TITLE_RATINGS |&gt; \n         filter(averageRating == 10.0,numVotes &gt;= 200000) |&gt; \n         inner_join(TITLE_BASICS |&gt; \n              select(tconst,primaryTitle),join_by(tconst == tconst)) |&gt;\n         rename(EpisodeTitle = primaryTitle) |&gt; \n         inner_join(TITLE_EPISODES |&gt; \n              select(tconst,parentTconst),join_by(tconst == tconst)) |&gt;\n         inner_join(TITLE_BASICS |&gt; \n              select(tconst,primaryTitle),join_by(parentTconst == tconst)) |&gt;  \n         rename(SeriesTitle = primaryTitle) |&gt;\n         select(-tconst,-parentTconst) |&gt;\n         rename(Rating = averageRating,'Number of votes' = numVotes,Episode=EpisodeTitle,Series=SeriesTitle),align = \"l\")\n\n\n\n\nRating\nNumber of votes\nEpisode\nSeries\n\n\n\n\n10\n230179\nOzymandias\nBreaking Bad\n\n\n\n\n\n‘As you can see with a huge 230,000 votes it is the episode Ozymandis in the series Breaking Bad that received a perfect 10’\n‘4.’\n\nkable(NAME_BASICS |&gt; \n         filter(primaryName == \"Mark Hamill\") |&gt; \n         separate_longer_delim(knownForTitles,\",\") |&gt; \n         inner_join(TITLE_RATINGS,join_by(knownForTitles == tconst)) |&gt; \n         inner_join(TITLE_BASICS |&gt; \n               select(tconst,primaryTitle),join_by(knownForTitles == tconst)) |&gt; \n         arrange(desc(numVotes)) |&gt;\n         select(-nconst,-birthYear,-deathYear,-primaryProfession,-knownForTitles) |&gt;\n         rename(Name = primaryName,Rating = averageRating,'Number of Votes' = numVotes,Title=primaryTitle),align=\"l\")\n\n\n\n\n\n\n\n\n\n\nName\nRating\nNumber of Votes\nTitle\n\n\n\n\nMark Hamill\n8.6\n1475025\nStar Wars: Episode IV - A New Hope\n\n\nMark Hamill\n8.7\n1406127\nStar Wars: Episode V - The Empire Strikes Back\n\n\nMark Hamill\n8.3\n1140557\nStar Wars: Episode VI - Return of the Jedi\n\n\nMark Hamill\n6.9\n682965\nStar Wars: Episode VIII - The Last Jedi\n\n\n\n\n\n‘My argument for this would be the Starwars episodes are Mark Hamills primary claim to fame, specifically Episode, 4,5, 6 and 8 in that order’\n‘5.’\n\nkable(TITLE_EPISODES |&gt; \n        count(parentTconst, name = \"Countrows\") |&gt; \n        filter(Countrows &gt;= 12) |&gt; \n        inner_join(TITLE_BASICS |&gt; \n               filter(titleType == \"tvSeries\")|&gt; \n               select (primaryTitle,tconst), join_by(parentTconst == tconst))|&gt; \n        inner_join(TITLE_RATINGS,join_by (parentTconst == tconst)) |&gt; \n        arrange(desc(averageRating)) |&gt; \n        select(-parentTconst) |&gt;\n        rename('Number of episodes'=Countrows,'Tv series'=primaryTitle,'Number of votes'=numVotes,Rating=averageRating)  |&gt;\n        head(10),align = \"l\")\n\n\n\n\nNumber of episodes\nTv series\nRating\nNumber of votes\n\n\n\n\n101\nJogandofoddaci\n9.8\n178\n\n\n318\nCraft Games\n9.7\n150\n\n\n134\nChoufli Hal\n9.7\n2930\n\n\n212\nPrime Time\n9.7\n181\n\n\n74\nFriday Five Sharp\n9.7\n4516\n\n\n66\nFreaking Fucking Games\n9.6\n136\n\n\n238\nMarmadesam\n9.6\n907\n\n\n171\nOneyPlays\n9.6\n226\n\n\n170\nThe Why Files\n9.6\n868\n\n\n12\nArkadas Canlisi\n9.6\n4185\n\n\n\n\n\n‘Tied for first would be Craft games, Jogandofoddaci, Chofli Hal, Prime time and Friday Five Sharp each having over 50 episodes but low vote counts’\n‘6.’\n\nGRAPH_INFORMATION &lt;- TITLE_BASICS |&gt; \n  filter(primaryTitle == \"Happy Days\") |&gt; \n  inner_join(TITLE_EPISODES,join_by (tconst == parentTconst)) |&gt; \n  inner_join (TITLE_RATINGS,join_by (tconst.y == tconst))\n\nggplot(GRAPH_INFORMATION,aes(x = seasonNumber, y = averageRating)) + \n  geom_point(color = \"blue\", size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title=\"Happy Days Seasonal performance\", x = \"Season\", y = \"Rating\") + \n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n‘From this analysis we can see that the data tended to be on a seasonal decline, however my argument would be that Season 7,8 were the lowest points and then went out with better episodes in season 12’\n‘Task 3 I am going to be saying the success metrics is the rounded average rating such that 5.4 is treated as 5 and 5.6 is treated as 6 added with the number of digits in the votes’\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt; \n  mutate(Success_measure = round(averageRating) + ceiling(log10(abs(numVotes))))\n\n‘I have created a separe dataset for the list of successful movies with wider details to not show too much information’\n\nSuccessful_Movies &lt;- TITLE_RATINGS |&gt;\n  inner_join(TITLE_BASICS |&gt; \n               filter(titleType == \"movie\"), join_by(tconst == tconst)) |&gt; \n  arrange(desc(Success_measure),desc(averageRating))\n\n‘3.1’\n\nkable(Successful_Movies |&gt; \n        select(-tconst,-titleType,-originalTitle,-isAdult,-endYear,-runtimeMinutes,-genres) |&gt;\n        head(5)|&gt;\n        rename(Rating = averageRating,'Number of votes'=numVotes,'Success measure' = Success_measure,Title=primaryTitle,'Release Year' = startYear),align = \"l\")\n\n\n\n\n\n\n\n\n\n\n\nRating\nNumber of votes\nSuccess measure\nTitle\nRelease Year\n\n\n\n\n9.3\n2953606\n16\nThe Shawshank Redemption\n1994\n\n\n9.2\n2059043\n16\nThe Godfather\n1972\n\n\n9.0\n1391342\n16\nThe Godfather Part II\n1974\n\n\n9.0\n1481655\n16\nSchindler’s List\n1993\n\n\n9.0\n2022057\n16\nThe Lord of the Rings: The Return of the King\n2003\n\n\n\n\n\n‘The top 5 movies were The Shawshank Redemption, The Godfather, The Godfather Part II, Schindlers List, The Lord of the Rings: The Return of the King all of which were iconic movies and box office successes’\n‘3.2’\n\nkable(TITLE_RATINGS |&gt;\n        filter(numVotes &gt;= 10000) |&gt; \n        inner_join(TITLE_BASICS |&gt; \n                     filter(titleType == \"movie\"), join_by(tconst == tconst)) |&gt; \n        arrange(Success_measure,averageRating) |&gt; \n        head(5)|&gt;\n        select(-tconst,-originalTitle,-titleType,-isAdult,-endYear,-runtimeMinutes,-genres)|&gt;\n        rename(Rating = averageRating,'Number of Votes' = numVotes,'Success Measure' = Success_measure,Title = primaryTitle,'Release Year' = startYear), align = \"l\")\n\n\n\n\n\n\n\n\n\n\n\nRating\nNumber of Votes\nSuccess Measure\nTitle\nRelease Year\n\n\n\n\n1.0\n10212\n6\n321 Action\n2020\n\n\n1.0\n74159\n6\nReis\n2017\n\n\n1.0\n39461\n6\nCumali Ceber: Allah Seni Alsin\n2017\n\n\n1.2\n14932\n6\nDaniel the Wizard\n2004\n\n\n1.2\n20685\n6\n15/07: Break of Dawn\n2021\n\n\n\n\n\n‘Given my metrics I came up with 321 Action (10210 Votes 1.0 rating success measure of 6),Reis (74155 votes,1 Average Rating, success measure of 6) and Cumali Ceber: Allah Seni Alsin (39456 votes, 1 average rating, success measure of 6)’\n‘3.3’\n‘For my actor I selected Brad Pitt and a score above 11’\n\nkable(NAME_BASICS |&gt; \n        filter(primaryName == \"Brad Pitt\") |&gt; \n        separate_longer_delim(knownForTitles, \",\") |&gt; \n        inner_join(TITLE_RATINGS |&gt; \n            filter(Success_measure &gt;= \"10\"), join_by(knownForTitles == tconst)) |&gt;\n        inner_join(TITLE_BASICS |&gt; \n            select(primaryTitle,tconst), join_by(knownForTitles == tconst))|&gt;\n      select(-nconst,-knownForTitles,-deathYear,-primaryProfession)|&gt;\n        rename(Name = primaryName,'Year of Birth' = birthYear,Rating = averageRating,'Number of Votes' = numVotes,'Success measure' = Success_measure,Title=primaryTitle),align=\"l\")\n\n\n\n\n\n\n\n\n\n\n\n\nName\nYear of Birth\nRating\nNumber of Votes\nSuccess measure\nTitle\n\n\n\n\nBrad Pitt\n1963\n8.8\n2385636\n16\nFight Club\n\n\nBrad Pitt\n1963\n6.5\n552901\n12\nMr. & Mrs. Smith\n\n\nBrad Pitt\n1963\n7.6\n474560\n14\nMoneyball\n\n\nBrad Pitt\n1963\n8.0\n654805\n14\n12 Monkeys\n\n\n\n\n\n‘Brad Pitt has been in 4 movies above 10 success points with the 4 being Fight Club, Moneyball, 12 Monkeys, Mr & Mrs Smith’\n‘3.4’\n‘The spot check validation is average Ratings above 9 and lowest success_measure (i.e.minimal view)’\n\nkable(TITLE_RATINGS |&gt; \n        filter(Success_measure &lt;= 12) |&gt; \n        filter(averageRating &gt;= 9) |&gt; \n        inner_join(TITLE_BASICS |&gt; \n        filter(titleType == \"movie\"), join_by(tconst == tconst)) |&gt; \n        arrange(numVotes,Success_measure,desc(averageRating)) |&gt; \n        select(-tconst,-titleType,-originalTitle,-isAdult,-endYear,-runtimeMinutes,-genres) |&gt;\n        rename('Release year' = startYear, Title = primaryTitle,Rating = averageRating,'Success measure' = Success_measure,'Number of votes' = numVotes) |&gt;\n        head(3),align = \"l\")\n\n\n\n\n\n\n\n\n\n\n\nRating\nNumber of votes\nSuccess measure\nTitle\nRelease year\n\n\n\n\n9.9\n100\n12\nYou Are the Apple of My Eye\n2024\n\n\n9.8\n100\n12\nPop Lock ’n Roll\n2016\n\n\n9.6\n100\n12\nAramudaitha Kombu\n2023\n\n\n\n\n\n‘The three movies were Aramudaitha Kombu, Carving the Divie and Pop Lock n Roll which I have never heard of and so did not perform well in terms of the success measure that I crafted’\n‘3.5’\n‘I think a numerical threshold for a project to be a success is if the success criteria is above 14 because that means that either the project got above 9.5 and 1000+ reviews or over a million reviews and was above a 6.5 average rating.’\n‘Task 4’\n‘for this one genre is aggregated with each title potentially having multiple genres to treat this we are going to include a table with each row being a genre per movie’\n‘Had to also remove nulls’\n\nTITLE_BASICS_GENRE &lt;- TITLE_BASICS |&gt; \n  separate_longer_delim(genres, \",\") |&gt;\n  filter(titleType == \"movie\") |&gt; \n  inner_join(TITLE_RATINGS |&gt; \n               select (Success_measure,tconst), join_by(tconst == tconst)) |&gt;\n  filter(Success_measure &gt;= 14) |&gt; \n  mutate(decade=round(startYear,-1))\n\n\ngenre_counts &lt;- TITLE_BASICS_GENRE |&gt; \n  group_by(decade, genres) |&gt; \n  summarize(count = n())\n\n`summarise()` has grouped output by 'decade'. You can override using the\n`.groups` argument.\n\n\n‘Added a filter to only show where count over 12 so to not over lap it’\n\nggplot(genre_counts, aes(x = decade, y = count, fill = genres)) +\n           geom_bar(stat = \"identity\") + \n           geom_text(aes(label = ifelse(count &gt; 12, count, \"\")), position =                  position_stack(vjust = 0.5)) + \n           labs(title = \"Number of Movies by Genre and Decade\") + \n           xlab(\"Decade\") + \n           ylab(\"Number of Movies\") + \n           scale_x_continuous(breaks = seq(1920, 2020, by = 10), labels = paste0            (seq(1920, 2020, by = 10), \"s\")) + \n           ggtitle(label = \"Number of Movies by Genre and Decade\")\n\n\n\n\n\n\n\n\n‘Drama seems to be the most successful from 1920-2020’\n\ntotal_successes &lt;- genre_counts |&gt; \n  group_by(decade) |&gt; \n  summarize(total_success = sum(count))\n\n\ngenre_counts_total &lt;- genre_counts |&gt; \n  left_join(total_successes, by = \"decade\") |&gt; \n  mutate(percentage_of_count = count/total_success)\n\n\nggplot(genre_counts_total, aes(x = decade, y = percentage_of_count, color = genres,group = genres, text = paste0(round(percentage_of_count*100, 0), \"%\"))) +\n           geom_line() + \n           geom_point() + \n           labs(title = \"Percentage of Total Successes by Genre and Year\", x = \"Year\", y = \"Percentage of Total Successes\") + \n           scale_x_continuous(breaks = seq(1920, 2020, by = 10), labels = paste0(seq(1920, 2020, by = 10), \"s\")) + \n           scale_y_continuous(labels = scales::percent) + ggtitle(label = \"Number of Movies by Genre and Decade\")\n\n\n\n\n\n\n\n\n‘4.2’ ‘I would claim the genre that has had the most consistent success movies would be Drama movies always being more than 20% of the share of successful movies by decade’ ‘I would claim that the genre that fell out of favor the most is Romance, as in 1930 it had a 20% share of the success movies however that has dropped to 3% in the 2020s’\n‘4.3 Drama has produced the most successes in 2010s and 2020s, primarily driven by the sheer volume although not having a bad success conversion of movies, the conversion tends to be middle of the pack when compared to other genres’\n\nkable(TITLE_BASICS |&gt; \n         separate_longer_delim(genres, \",\") |&gt; \n         filter(titleType == \"movie\") |&gt; \n         inner_join(TITLE_RATINGS |&gt; \n         select (Success_measure,tconst), join_by(tconst == tconst)) |&gt; \n         mutate(Success=Success_measure &gt;= 14) |&gt; \n         mutate(decade=round(startYear,-1)) |&gt; \n         filter(decade &gt;= 2010) |&gt; \n         group_by(decade,genres) |&gt; \n         summarize(movies = n(), successful_movies = sum(Success==TRUE)) |&gt;\n         mutate(percentage_success = percent(successful_movies/movies)) |&gt;\n         filter(successful_movies != 0)) \n\n`summarise()` has grouped output by 'decade'. You can override using the\n`.groups` argument.\n\n\n\n\n\ndecade\ngenres\nmovies\nsuccessful_movies\npercentage_success\n\n\n\n\n2010\nAction\n3745\n61\n1.629%\n\n\n2010\nAdventure\n1925\n50\n2.597%\n\n\n2010\nAnimation\n855\n20\n2.339%\n\n\n2010\nBiography\n1268\n32\n2.524%\n\n\n2010\nComedy\n8816\n42\n0.476%\n\n\n2010\nCrime\n2953\n38\n1.287%\n\n\n2010\nDrama\n14322\n144\n1.005%\n\n\n2010\nFamily\n1359\n10\n0.736%\n\n\n2010\nFantasy\n1120\n20\n1.786%\n\n\n2010\nHistory\n984\n8\n0.813%\n\n\n2010\nHorror\n3465\n6\n0.173%\n\n\n2010\nMusic\n807\n2\n0.248%\n\n\n2010\nMusical\n302\n2\n0.662%\n\n\n2010\nMystery\n1599\n28\n1.751%\n\n\n2010\nRomance\n3925\n22\n0.561%\n\n\n2010\nSci-Fi\n958\n21\n2.192%\n\n\n2010\nSport\n532\n5\n0.940%\n\n\n2010\nThriller\n3760\n34\n0.904%\n\n\n2010\nWar\n410\n7\n1.707%\n\n\n2010\nWestern\n105\n2\n1.905%\n\n\n2020\nAction\n5542\n61\n1.1007%\n\n\n2020\nAdventure\n2787\n47\n1.6864%\n\n\n2020\nAnimation\n1264\n20\n1.5823%\n\n\n2020\nBiography\n2011\n29\n1.4421%\n\n\n2020\nComedy\n11448\n44\n0.3843%\n\n\n2020\nCrime\n4107\n27\n0.6574%\n\n\n2020\nDocumentary\n5421\n3\n0.0553%\n\n\n2020\nDrama\n19861\n125\n0.6294%\n\n\n2020\nFamily\n1858\n3\n0.1615%\n\n\n2020\nFantasy\n1610\n10\n0.6211%\n\n\n2020\nHistory\n1461\n12\n0.8214%\n\n\n2020\nHorror\n5665\n3\n0.0530%\n\n\n2020\nMusic\n1063\n7\n0.6585%\n\n\n2020\nMusical\n335\n1\n0.2985%\n\n\n2020\nMystery\n2870\n13\n0.4530%\n\n\n2020\nRomance\n4619\n14\n0.3031%\n\n\n2020\nSci-Fi\n1561\n11\n0.7047%\n\n\n2020\nSport\n814\n2\n0.2457%\n\n\n2020\nThriller\n6728\n20\n0.2973%\n\n\n2020\nWar\n534\n3\n0.5618%\n\n\n2020\n\n106\n1\n0.9434%\n\n\n\n\n\n‘4.4 Action as genre has become more popular for successful movies moving from 2% in the 1950s to 14% in the 2020s so has seen a considerable rise’\n‘Overall I would personally target an adventure movie although not appearing anywhere in these metrics, it tends to have one of the highest percentage success across all movies above 2% of all adventure movies made’\n‘Task 5’\n\nActor_success&lt;- Successful_Movies |&gt;\n        filter(Success_measure &gt;= 10) |&gt;\n        separate_longer_delim(genres, \",\")|&gt;\n        filter(genres == \"Adventure\") |&gt;\n        inner_join(NAME_BASICS |&gt;\n                   separate_longer_delim(primaryProfession, \",\")|&gt;\n                   separate_longer_delim(knownForTitles, \",\")|&gt; \n                   filter(primaryProfession == \"director\"|primaryProfession == \"actor\")|&gt;\n                   select(knownForTitles,primaryName,birthYear,primaryProfession),\n                   join_by(tconst == knownForTitles)) |&gt;\n        group_by(primaryName,primaryProfession,startYear,birthYear) |&gt;\n        summarize(number_of_successful_movies = n(),Number_of_Votes = sum(numVotes,na.rm = FALSE),Average_rating = mean(averageRating)) |&gt;\n        filter(!is.na(birthYear))|&gt;\n        arrange(desc(number_of_successful_movies))\n\n`summarise()` has grouped output by 'primaryName', 'primaryProfession',\n'startYear'. You can override using the `.groups` argument.\n\nkable(Actor_success |&gt;\n        group_by(primaryName,primaryProfession,birthYear) |&gt;\n        summarize(Successes = sum(number_of_successful_movies,na.rm = FALSE))|&gt;\n        arrange(desc(Successes))|&gt;\n        rename(Name = primaryName, Profession = primaryProfession)|&gt;\n        head(100),align = \"l\")\n\n`summarise()` has grouped output by 'primaryName', 'primaryProfession'. You can\noverride using the `.groups` argument.\n\n\n\n\n\nName\nProfession\nbirthYear\nSuccesses\n\n\n\n\n‘Chico’ Hernandez\nactor\n1958\n4\n\n\n‘Chico’ Hernandez\ndirector\n1958\n4\n\n\nAaron Blaise\ndirector\n1968\n4\n\n\nAaron C. Fitzgerald\nactor\n1978\n4\n\n\nAaron Fors\nactor\n1989\n4\n\n\nAaron Kozak\nactor\n1983\n4\n\n\nAdam Austin\nactor\n1965\n4\n\n\nAdam Brown\nactor\n1980\n4\n\n\nAdam J. Ely\nactor\n1979\n4\n\n\nAdam May\ndirector\n1979\n4\n\n\nAdriano Cirulli\ndirector\n1973\n4\n\n\nAidan Turner\nactor\n1983\n4\n\n\nAkihiro Tomikawa\nactor\n1968\n4\n\n\nAkira Kushida\nactor\n1948\n4\n\n\nAl Matthews\nactor\n1942\n4\n\n\nAl Zinnen\nactor\n1903\n4\n\n\nAlan Lee\nactor\n1947\n4\n\n\nAlan Ritchson\nactor\n1982\n4\n\n\nAlan Ritchson\ndirector\n1982\n4\n\n\nAlbert R. Broccoli\nactor\n1909\n4\n\n\nAlbert Uderzo\nactor\n1927\n4\n\n\nAlbert Uderzo\ndirector\n1927\n4\n\n\nAlbie Woodington\nactor\n1952\n4\n\n\nAlec Mills\ndirector\n1932\n4\n\n\nAleksandr Kraevskiy\nactor\n1980\n4\n\n\nAles Kosnar\nactor\n1972\n4\n\n\nAlex Cannon\ndirector\n1977\n4\n\n\nAlex McCormack\nactor\n1985\n4\n\n\nAlfred Enoch\nactor\n1988\n4\n\n\nAllen Jo\nactor\n1977\n4\n\n\nAlvaro Zendejas\ndirector\n1983\n4\n\n\nAmit Soni\nactor\n1982\n4\n\n\nAmy Eglen\ndirector\n1993\n4\n\n\nAmy Johnston\ndirector\n1990\n4\n\n\nAndrew Adamson\ndirector\n1966\n4\n\n\nAndrew Harvey\ndirector\n1981\n4\n\n\nAndrew Jack\nactor\n1944\n4\n\n\nAndrew Lesnie\nactor\n1956\n4\n\n\nAndrew Stanton\nactor\n1965\n4\n\n\nAndrew Tamandl\ndirector\n1968\n4\n\n\nAngelo Ragusa\nactor\n1952\n4\n\n\nAnthony Daniels\nactor\n1946\n4\n\n\nAnthony Russo\ndirector\n1970\n4\n\n\nAntonio Funaro\nactor\n1982\n4\n\n\nAntonio Molina\nactor\n1954\n4\n\n\nAri Ross\ndirector\n1979\n4\n\n\nArnold Vosloo\nactor\n1962\n4\n\n\nArt Stevens\ndirector\n1915\n4\n\n\nArthur Max\nactor\n1946\n4\n\n\nAsher Blinkoff\nactor\n2008\n4\n\n\nAssis Eloy\nactor\n1985\n4\n\n\nAttila Illés\nactor\n1988\n4\n\n\nBarry Blanchard\nactor\n1959\n4\n\n\nBarry R. Koper\nactor\n1955\n4\n\n\nBeau Brasseaux\nactor\n1989\n4\n\n\nBen Collins\nactor\n1975\n4\n\n\nBenedict Wong\nactor\n1970\n4\n\n\nBernard Bresslaw\nactor\n1934\n4\n\n\nBernard Lee\nactor\n1908\n4\n\n\nBill Roberts\ndirector\n1899\n4\n\n\nBill Thompson\nactor\n1913\n4\n\n\nBilly Dee Williams\nactor\n1937\n4\n\n\nBob Anderson\nactor\n1922\n4\n\n\nBob Barlen\ndirector\n1980\n4\n\n\nBob Carlson\ndirector\n1906\n4\n\n\nBob Gale\ndirector\n1951\n4\n\n\nBob Mano\nactor\n1955\n4\n\n\nBob Peterson\nactor\n1961\n4\n\n\nBob Simmons\nactor\n1922\n4\n\n\nBobby Block\nactor\n1991\n4\n\n\nBogdan Draghici\ndirector\n1982\n4\n\n\nBonnie Wright\ndirector\n1991\n4\n\n\nBrad Abrell\nactor\n1965\n4\n\n\nBrad Heiner\nactor\n1964\n4\n\n\nBrad Jeffries\nactor\n1960\n4\n\n\nBradley Everett Wilson\ndirector\n1980\n4\n\n\nBrent Spiner\nactor\n1949\n4\n\n\nBrent Spiner\ndirector\n1949\n4\n\n\nBrian Goldner\nactor\n1963\n4\n\n\nBrian M. Rosen\nactor\n1971\n4\n\n\nBrian Magner\nactor\n1978\n4\n\n\nBrian Mainolfi\ndirector\n1971\n4\n\n\nBrock Little\nactor\n1967\n4\n\n\nBruno Mars\nactor\n1985\n4\n\n\nBryan Adams\nactor\n1959\n4\n\n\nBryon Weiss\nactor\n1963\n4\n\n\nBryon Weiss\ndirector\n1963\n4\n\n\nC. Andrew Nelson\nactor\n1962\n4\n\n\nCal Brunker\ndirector\n1978\n4\n\n\nCan Bolat\nactor\n1998\n4\n\n\nCarlos Saldanha\ndirector\n1965\n4\n\n\nChad Sellers\nactor\n1982\n4\n\n\nCharles Haugk\nactor\n1953\n4\n\n\nChris Antonini\nactor\n1977\n4\n\n\nChris Buck\ndirector\n1958\n4\n\n\nChris Cason\nactor\n1974\n4\n\n\nChris Clarke\nactor\n1977\n4\n\n\nChris Cossey\nactor\n1984\n4\n\n\nChris Daniels\nactor\n1976\n4\n\n\nChris Dawson\nactor\n1965\n4\n\n\n\n\nkable(Actor_success |&gt;\n        group_by(primaryName,primaryProfession,birthYear) |&gt;\n        summarize(Successes = sum(number_of_successful_movies,na.rm = FALSE))|&gt;\n        filter(birthYear &gt;= 1990) |&gt;\n        arrange(desc(Successes))|&gt;\n        rename(Name = primaryName, Profession = primaryProfession)|&gt;\n        head(100),align = \"l\")\n\n`summarise()` has grouped output by 'primaryName', 'primaryProfession'. You can\noverride using the `.groups` argument.\n\n\n\n\n\nName\nProfession\nbirthYear\nSuccesses\n\n\n\n\nAmy Eglen\ndirector\n1993\n4\n\n\nAmy Johnston\ndirector\n1990\n4\n\n\nAsher Blinkoff\nactor\n2008\n4\n\n\nBobby Block\nactor\n1991\n4\n\n\nBonnie Wright\ndirector\n1991\n4\n\n\nCan Bolat\nactor\n1998\n4\n\n\nDaniel Pentkowski\nactor\n1991\n4\n\n\nDavid De Juan\ndirector\n1990\n4\n\n\nElliott Cattell\ndirector\n1991\n4\n\n\nGeorge Redstone\ndirector\n2000\n4\n\n\nIvan Sorgente\nactor\n1993\n4\n\n\nJackey Mishra\nactor\n1998\n4\n\n\nJacob Batalon\nactor\n1996\n4\n\n\nJohn Boyega\nactor\n1992\n4\n\n\nJosh Hutcherson\nactor\n1992\n4\n\n\nJosh Hutcherson\ndirector\n1992\n4\n\n\nKanata Hongô\nactor\n1990\n4\n\n\nLiam Hemsworth\nactor\n1990\n4\n\n\nLuke Scott\nactor\n1994\n4\n\n\nMathew Yanagiya\nactor\n1990\n4\n\n\nRicky Arietta\ndirector\n1991\n4\n\n\nShu Watanabe\nactor\n1991\n4\n\n\nSôta Fukushi\nactor\n1993\n4\n\n\nTrey Brown\nactor\n1993\n4\n\n\nViolet Columbus\ndirector\n1994\n4\n\n\nWilliam Melling\nactor\n1994\n4\n\n\nAaron Dismuke\nactor\n1992\n3\n\n\nAjay Lobo\nactor\n1997\n3\n\n\nAleks Le\nactor\n1999\n3\n\n\nAlex R. Wagner\ndirector\n1991\n3\n\n\nAlexander Canton\nactor\n1995\n3\n\n\nAmaan Shaikh\nactor\n2000\n3\n\n\nAnastasia Zabarchuk\ndirector\n1997\n3\n\n\nArchie Renaux\nactor\n1997\n3\n\n\nArvind Kashyap\nactor\n1992\n3\n\n\nBart the Bear\nactor\n2000\n3\n\n\nBehnam Taheri\ndirector\n1990\n3\n\n\nBilly Jackson\nactor\n1995\n3\n\n\nBooboo Stewart\nactor\n1994\n3\n\n\nBrayden Patterson\nactor\n1992\n3\n\n\nChandler Frantz\nactor\n1998\n3\n\n\nChristopher Painter\nactor\n1992\n3\n\n\nCody Simpson\nactor\n1997\n3\n\n\nCody Simpson\ndirector\n1997\n3\n\n\nCooper Cowgill\nactor\n1995\n3\n\n\nDaryl Sabara\nactor\n1992\n3\n\n\nDavide Anselmi\ndirector\n1999\n3\n\n\nDonte Paris\nactor\n1998\n3\n\n\nEdouard Calemard\ndirector\n1992\n3\n\n\nElan Garfias\nactor\n1999\n3\n\n\nElias Mlayeh\nactor\n1991\n3\n\n\nFinneas O’Connell\nactor\n1997\n3\n\n\nGeorgie Henley\ndirector\n1995\n3\n\n\nGianni Biasetti Jr.\nactor\n1992\n3\n\n\nHarry Holland\nactor\n1999\n3\n\n\nHarry Holland\ndirector\n1999\n3\n\n\nHayato Onozuka\nactor\n1993\n3\n\n\nIan Chen\nactor\n2006\n3\n\n\nIzaac Wang\nactor\n2007\n3\n\n\nJack Millar\ndirector\n1991\n3\n\n\nJacob Bertrand\nactor\n2000\n3\n\n\nJacob Bertrand\ndirector\n2000\n3\n\n\nJacob Lofland\nactor\n1996\n3\n\n\nJacob Smith\nactor\n1990\n3\n\n\nJake Cherry\nactor\n1996\n3\n\n\nJohn Bell\nactor\n1997\n3\n\n\nJordan Fry\nactor\n1993\n3\n\n\nJoshua R. Jones\nactor\n1990\n3\n\n\nJustice Smith\nactor\n1995\n3\n\n\nKai Lydgate\nactor\n1992\n3\n\n\nKhaled Elkodosy\nactor\n2001\n3\n\n\nKristen Stewart\ndirector\n1990\n3\n\n\nLance Breakwell\nactor\n1993\n3\n\n\nMarvin Jaacks\nactor\n1996\n3\n\n\nMax Charles\nactor\n2003\n3\n\n\nMelody Wayfare\ndirector\n1994\n3\n\n\nMichaela Jill Murphy\ndirector\n1994\n3\n\n\nMustard\nactor\n1990\n3\n\n\nNaomi Scott\ndirector\n1993\n3\n\n\nNathan Velasquez\nactor\n1996\n3\n\n\nNiall Horn\ndirector\n1997\n3\n\n\nNicholas Bird\nactor\n1994\n3\n\n\nNikita Hopkins\nactor\n1991\n3\n\n\nOmar Fathy Saber\nactor\n1998\n3\n\n\nPeregrine Kitchener-Fellowes\nactor\n1991\n3\n\n\nPeter Varnai\ndirector\n1991\n3\n\n\nReed Buck\nactor\n1994\n3\n\n\nRofique Khan\nactor\n1997\n3\n\n\nRoss Simanteris\nactor\n1994\n3\n\n\nRyan Cunningham\ndirector\n1990\n3\n\n\nRyunosuke Kamiki\nactor\n1993\n3\n\n\nRyô Yoshizawa\nactor\n1994\n3\n\n\nScotty Cook\nactor\n1991\n3\n\n\nShameik Moore\nactor\n1995\n3\n\n\nShane Baumel\nactor\n1997\n3\n\n\nShôma Kai\nactor\n1997\n3\n\n\nSkandar Keynes\nactor\n1991\n3\n\n\nSo Okuno\nactor\n2000\n3\n\n\nSwae Lee\nactor\n1993\n3\n\n\nTake That\nactor\n1990\n3\n\n\n\n\n\nI will select James Cameron as the director given his experience in the industry and successful performance and Orlando Bloom who had successes in Lord of the rings and Liam Hemsworth who has had vast experience in the industry and an already created persona.\n\nggplot(Actor_success |&gt;\n                 mutate(Decade = floor(startYear/10)*10) |&gt;\n                 group_by(Decade,primaryName) |&gt;\n                 filter(primaryName == \"James Cameron\"|primaryName == \"Orlando Bloom\"|primaryName == \"Liam Hemsworth\") |&gt;\n                 summarize(Number_of_Votes = mean(Number_of_Votes))\n               , aes(x = Decade, y = Number_of_Votes,group = primaryName,colour = primaryName)) +\n          geom_bar(stat=\"Identity\") +\n          labs(title = \"Number of Adventure Movies by Decade\") +\n          xlab(\"Year\") +\n          ylab(\"Number of Votes\") +\n          scale_x_continuous(breaks = seq(1920, 2020, by = 10), labels = paste0(seq(1920, 2020, by = 10), \"s\")) +\n          ggtitle(label = \"Number of Advenutre movies votes by Year\")\n\n`summarise()` has grouped output by 'Decade'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nNow is the perfect time for you three to team up and make an attempt to recapture the nostalgic market, during your peak times 2000s and 2010s, you were very successful particularly in the adventure category and have all had exposure to the movie category. The decline in votes is common across the industry and is waiting for a revival, similar to what you did back in 2000s.\n\nggplot(TITLE_RATINGS|&gt;\n                  inner_join(TITLE_BASICS |&gt;\n                               separate_longer_delim(genres, \",\")|&gt;\n                               filter(genres == \"Adventure\") |&gt;\n                               filter(titleType == \"movie\") |&gt;\n                               select(tconst,startYear),join_by(tconst == tconst)) |&gt;\n                  mutate(Decade = floor(startYear/10)*10) |&gt;\n                  group_by(Decade) |&gt;\n                  summarize(Number_of_Votes = mean(numVotes))\n                , aes(x = Decade, y = Number_of_Votes)) +\n           geom_line() +\n           labs(title = \"Number of Adventure Movies by Decade\") +\n           xlab(\"Year\") +\n           ylab(\"Number of Votes\") +\n           scale_x_continuous(breaks = seq(1920, 2020, by = 10), labels = paste0(seq(1920, 2020, by = 10), \"s\")) +\n           ggtitle(label = \"Number of Advenutre movies votes by Year\")\n\n\n\n\n\n\n\n\nTask 6\n\nkable(TITLE_RATINGS|&gt;\n  inner_join(TITLE_BASICS |&gt;\n               separate_longer_delim(genres, \",\")|&gt;\n               filter(genres == \"Adventure\") |&gt;\n               filter(titleType == \"movie\") |&gt;\n               select(tconst,startYear,primaryTitle),join_by(tconst == tconst)) |&gt;\n    filter(numVotes&gt;=100000) |&gt;\n    filter(startYear &lt;= 1999) |&gt;\n    filter(startYear &gt;= 1985) |&gt;\n    select(-tconst) |&gt;\n    filter(averageRating &gt;=7.0) |&gt;\n    rename(Rating = averageRating, 'Number of votes' = numVotes, Title = primaryTitle, Year = startYear,'Success measure'=Success_measure),align = \"l\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nRating\nNumber of votes\nSuccess measure\nYear\nTitle\n\n\n\n\n8.5\n1338094\n15\n1985\nBack to the Future\n\n\n7.7\n304938\n14\n1985\nThe Goonies\n\n\n8.4\n787933\n14\n1986\nAliens\n\n\n7.2\n157263\n13\n1986\nBig Trouble in Little China\n\n\n7.0\n150140\n13\n1986\nHighlander\n\n\n7.3\n153326\n13\n1986\nLabyrinth\n\n\n8.1\n451763\n14\n1986\nStand by Me\n\n\n8.0\n186529\n14\n1986\nCastle in the Sky\n\n\n7.8\n466089\n14\n1987\nPredator\n\n\n8.0\n457779\n14\n1987\nThe Princess Bride\n\n\n7.1\n211171\n13\n1987\nSpaceballs\n\n\n7.4\n100905\n13\n1988\nThe Land Before Time\n\n\n7.7\n220879\n14\n1988\nWho Framed Roger Rabbit\n\n\n7.2\n131613\n13\n1988\nWillow\n\n\n7.5\n197562\n14\n1989\nThe Abyss\n\n\n7.8\n584607\n14\n1989\nBack to the Future Part II\n\n\n7.5\n412097\n14\n1989\nBatman\n\n\n8.2\n822658\n14\n1989\nIndiana Jones and the Last Crusade\n\n\n7.6\n295670\n14\n1989\nThe Little Mermaid\n\n\n7.4\n490163\n13\n1990\nBack to the Future Part III\n\n\n8.0\n296111\n14\n1990\nDances with Wolves\n\n\n7.5\n218647\n14\n1990\nThe Hunt for Red October\n\n\n7.5\n360557\n14\n1990\nTotal Recall\n\n\n8.6\n1198743\n16\n1991\nTerminator 2: Judgment Day\n\n\n7.6\n177729\n14\n1991\nThelma & Louise\n\n\n8.0\n477950\n14\n1992\nAladdin\n\n\n7.7\n105517\n14\n1992\nPorco Rosso\n\n\n7.6\n192146\n14\n1992\nThe Last of the Mohicans\n\n\n7.0\n115846\n13\n1993\nCool Runnings\n\n\n8.2\n1089950\n15\n1993\nJurassic Park\n\n\n8.5\n1167510\n15\n1994\nThe Lion King\n\n\n7.0\n120889\n13\n1994\nMaverick\n\n\n7.3\n399613\n13\n1994\nSpeed\n\n\n7.0\n208702\n13\n1994\nStargate\n\n\n7.7\n320610\n14\n1995\nApollo 13\n\n\n7.5\n104440\n14\n1995\nDead Man\n\n\n7.6\n413328\n14\n1995\nDie Hard with a Vengeance\n\n\n7.2\n273448\n13\n1995\nGoldenEye\n\n\n7.1\n385728\n13\n1995\nJumanji\n\n\n8.3\n1091017\n15\n1995\nToy Story\n\n\n7.0\n616988\n13\n1996\nIndependence Day\n\n\n7.2\n478320\n13\n1996\nMission: Impossible\n\n\n7.4\n363692\n13\n1996\nThe Rock\n\n\n7.6\n133508\n14\n1996\nStar Trek: First Contact\n\n\n7.1\n138489\n13\n1997\nAnastasia\n\n\n7.0\n261389\n13\n1997\nAustin Powers: International Man of Mystery\n\n\n7.6\n516157\n14\n1997\nThe Fifth Element\n\n\n7.3\n260900\n13\n1997\nHercules\n\n\n7.3\n625000\n13\n1997\nMen in Black\n\n\n8.3\n444748\n14\n1997\nPrincess Mononoke\n\n\n7.1\n158630\n13\n1997\nSeven Years in Tibet\n\n\n7.1\n182406\n13\n1999\nThree Kings\n\n\n7.3\n328112\n13\n1997\nStarship Troopers\n\n\n7.9\n631324\n14\n1999\nToy Story 2\n\n\n7.1\n471238\n13\n1999\nThe Mummy\n\n\n7.2\n320135\n13\n1998\nA Bug’s Life\n\n\n7.3\n231450\n13\n1999\nDogma\n\n\n7.5\n305740\n14\n1998\nFear and Loathing in Las Vegas\n\n\n7.7\n323586\n14\n1998\nMulan\n\n\n7.2\n149896\n13\n1998\nThe Prince of Egypt\n\n\n7.3\n252688\n13\n1999\nTarzan\n\n\n8.1\n234826\n14\n1999\nThe Iron Giant\n\n\n7.4\n181820\n13\n1999\nGalaxy Quest\n\n\n\n\n\nThe Movie I have selected to recreate would be to produce a Dead Man sequel, with anew cast however I think would be interesting for Jim Jarmusch as the original director to provide his creative feedback, and insight and Johnny Depp as the primary actor from the original to provide consultative services on it.\nTask 7\n’Pitch:\nWhy This Project?\n\nReviving a Classic: Dead Man remains a cult classic, and a sequel would tap into a passionate fanbase.\nProven Talent: Orlando Bloom iconic performance and James Camerons visionary directing skills aswell as longevity being successful across decades offer a strong foundation. Particularly during the 2000’s around when Dead Man first released.\nRising Star: Liam Hemsworth, a proven action star, adds a contemporary appeal. Adventure Renaissance:\nAdventure films have enjoyed a resurgence and growth in popularity up around 200-500% over the past 30-40 years, with audiences craving escapism and thrilling narratives.\nNostalgia and Innovation: The film can blend nostalgia for the original with modern storytelling techniques, creating a fresh and exciting experience.’\n\nClassic 90’s style teaser\nFrom director James Cameron, the visionary mind behind Avatar; and From actor Orlando Bloom, beloved star of Lord of the Rings; and From Liam Hemsworth, Hollywood icon of Adventure, Comes the timeless tail Dead Mans Return A story of Legacy and Redemption, The Changing West, and The Power of Friendship Coming soon to a theater near you."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Will Peters STA9750-2024-FALL MP03",
    "section": "",
    "text": "Given that the election just happened, I thought I might take the opportunity to learn the results of previous elections, particularly as an Australian, I can admit the U.S Electoral College System is confusing\nSo to start off with, installing all the libraries that I used at some point in this project, hidden out of convenience so it can flow easier."
  },
  {
    "objectID": "mp03.html#us-election-votes",
    "href": "mp03.html#us-election-votes",
    "title": "Will Peters STA9750-2024-FALL MP03",
    "section": "US Election Votes",
    "text": "US Election Votes\nTo start off with, we need to download and extract the votes for state-wide presidential and house level votes, here is where I add a note that the incomplete data, means you will need to add in District of Columbia (D.C), did you know that is what D.C stood for? At least when it comes the states not to be confused with Detective Comics for DC Comics.\nI have included the sources for the president and house votes, in case you want to follow along.\nPresident votes\nHouse votes\n\nPresident_votes &lt;- as.data.frame(readr::read_csv(\"1976-2020-president.csv\", lazy=FALSE)) \n# As you can see I called the presidents file 1976-2020-president.csv for simplicity#\n\nHouse_votes &lt;- as.data.frame(readr::read_csv(\"1976-2022-house.csv\", lazy=FALSE)) \n# As you can see I called the house votes file 1976-2020-house.csv for simplicity#\n\nAs I mentioned earlier we need to add in the D.C. votes for each year, otherwise we will have incomplete data, in this case Democrats have won each year in our time period all 3 seats except 2000 where they won 2 out of 3, and as such going to be allocating all seats in all elections to the Democrats.\n\n#Extracting all the years that D.C appeared in the data#\nexisting_dc_years &lt;- House_votes |&gt;\n  filter(state == \"DISTRICT OF COLUMBIA\") |&gt;\n  pull(year) |&gt;\n  unique()\n\n#Calculating all the available years#\nall_years &lt;- unique(House_votes$year)\n#Comparing missing years of D.C to wider population#\nmissing_dc_years &lt;- setdiff(all_years, existing_dc_years)\n\n#populating the data frame for house votes, with some filler D.C information#\ndc_entries &lt;- data.frame(\n  year = missing_dc_years,\n  state = \"DISTRICT OF COLUMBIA\",\n  state_po = NA,\n  state_fips = NA,\n  state_cen = NA,\n  state_ic = NA,\n  office = \"US HOUSE\",\n  district = 0,\n  stage = \"SPECIAL\",\n  runoff = NA,\n  special = NA,\n  candidate = \"D.C.\",\n  party = \"DEMOCRAT\",\n  writein = NA,\n  mode = NA,\n  candidatevotes = 1,\n  totalvotes = 1,\n  unofficial = NA,\n  version = NA,\n  fusion_ticket = NA)\n\n#Combining the missing D.C votes with the wider House_votes population# \nHouse_votes &lt;- rbind(House_votes, dc_entries)"
  },
  {
    "objectID": "mp03.html#congressional-boundaries",
    "href": "mp03.html#congressional-boundaries",
    "title": "Will Peters STA9750-2024-FALL MP03",
    "section": "Congressional Boundaries",
    "text": "Congressional Boundaries\nSo to prepare for the Congressional Boundaries and the wider geo-spatial mapping, I did have to cheat a little bit, and copy and paste the table in the download period to extract the dates. So you will see the read_csv rather than an html extract given the table_wrapper used for the data. If you have figured that out well done, and reachout on my github as you can color me impressed.\n\n#Starts off with the file for the District mapping# \nDistrict_Mapping &lt;- as.data.frame(readr::read_csv(\"District Mapping.csv\", lazy=FALSE)) |&gt;\n  # Extract years from using regex\n  mutate(Start_Year = as.numeric(str_extract(Period, \"\\\\d{4}\")),\n         End_Year = as.numeric(str_extract(str_sub(Period, -4), \"\\\\d{4}\"))) |&gt;\n  # Create a list-column with sequences of years between two years so if 1803-1805 will create a nested list of 1803,1804,1805 to capture the entire period#\n  rowwise() |&gt;\n  mutate(Year = list(Start_Year:End_Year)) |&gt;\n  # Expand each row for each year in the list#\n  unnest(Year) |&gt;\n  # Select only the relevant columns#\n  select(District_file = `District file`, District_Number = `District Number`, Year) |&gt;\n  mutate(District_Number = str_sub(District_Number,end = -3)) |&gt;\n  #Treating the District as a 3 character string with \"0\" added for any 1,2 digit districts#\n  mutate(District_Number = as.character(sprintf(\"%03d\",as.numeric(District_Number))))\n\nIf you are following along so far, I thought it might be helpful for a small gift of a table to be shown of how we are treating the District table that we will be using in the mapping.\n\nkable(District_Mapping |&gt;\n        tail(10))\n\n\n\n\nDistrict_file\nDistrict_Number\nYear\n\n\n\n\ndistricts111.zip\n111\n2009\n\n\ndistricts111.zip\n111\n2010\n\n\ndistricts112.zip\n112\n2011\n\n\ndistricts112.zip\n112\n2012\n\n\ndistricts112.zip\n112\n2013\n\n\ndistricts113.zip\n113\n2013\n\n\ndistricts113.zip\n113\n2014\n\n\ndistricts114.zip\n114\n2015\n\n\ndistricts114.zip\n114\n2016\n\n\ndistricts114.zip\n114\n2017\n\n\n\n\n\n\nDownloading Congressional Zips\nThis is where we start utilize the automated downloading of the ZIP files, which include the sections of different districts used in the election and the spatial analysis of them, thank you to Jefferey Lewis, who put this together, as creating from scratch the spatial analysis of Congressional Districts would be a real challenge. Given that it is 2 separate websites, one for 1976-2014 that we are analyzing and a separate one for 2014-2022 which is the latest direct information from the government I am going to coin these two as Tasks 1 (Shape Files from 1976-2012) and Task 2 (Shape Files from 2014-2022)\n\nTask 1\n\n#Start off with the base URL#\nurl_base &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n#For the congressional boundaries and to be respectful, only utilizing districts 94-112 which covers 1976-2014\nnumbers_for_congress &lt;- sprintf(\"%03d\", 94:112)\n\n#Creating a function to download and read the district zip file\ndownload_and_read_district &lt;- function(number) {\n\n    # Define the base URL and construct the full URL and destination file name#\n    url_base &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n    #combining the url used to extract from the web#\n    file_url &lt;- paste0(url_base, \"districts\", number, \".zip\")\n    #telling it where to store the Congressional district data output# \n    dest_file &lt;- paste0(\"districts\", number, \".zip\")\n    \n    # Check if the file already exists before downloading#\n    if (!file.exists(dest_file)) {\n      download.file(file_url, destfile = dest_file)\n      cat(\"Downloaded:\", dest_file, \"\\n\")\n    } else {\n      cat(\"File already exists:\", dest_file, \"\\n\")\n    }\n}\n#Well done in following along the creation of the function#\n\n#now going through a loop and running for each number in numbers for congress defined above applying it to download onto the local drive the districts Zip file#\nfor(number in numbers_for_congress) {\n  download_and_read_district(number)\n}\n\nFile already exists: districts094.zip \nFile already exists: districts095.zip \nFile already exists: districts096.zip \nFile already exists: districts097.zip \nFile already exists: districts098.zip \nFile already exists: districts099.zip \nFile already exists: districts100.zip \nFile already exists: districts101.zip \nFile already exists: districts102.zip \nFile already exists: districts103.zip \nFile already exists: districts104.zip \nFile already exists: districts105.zip \nFile already exists: districts106.zip \nFile already exists: districts107.zip \nFile already exists: districts108.zip \nFile already exists: districts109.zip \nFile already exists: districts110.zip \nFile already exists: districts111.zip \nFile already exists: districts112.zip \n\n\nWell done we have now downloaded all the relevant zip files that we will be needing from this source, these will later come in handy but for early stages, we can keep these in storage.\n\n\nTask 2\nDownloading the information for 2014-2022 from the government website directly, this will be useful as more and more information is stored, but half the role is always aggregating the data and ensuring data quality.\n\n#Website for 2014-2022 data on the census.gov link, to be later shown#\nurl_base_2 &lt;- \"https://www2.census.gov/geo/tiger\"\n#years for census source of data#\nnumbers_for_census &lt;-  sprintf(\"%04d\",2014:2022)\n\n#creation of tiger data to be downloaded#\n  download_and_read_tiger &lt;- function(number) {\n    numeric_year &lt;- as.numeric(number)\n    # Define the base URL and construct the full URL and destination file name\n    cd_desc &lt;- ifelse(numeric_year &gt;= 2018,\"116\",\n                      ifelse(numeric_year &gt;= 2016,\"115\",\"114\"))\n    #used because the ending cd was not consistent, could use a wild card or number here\n    file_url &lt;- paste0(url_base_2, \"/TIGER\", numeric_year,\"/CD/tl_\",numeric_year,\"_us_cd\",cd_desc,\".zip\")\n    #generates file url#\n    dest_file &lt;- paste0(\"Tiger\", numeric_year, \".zip\")\n    \n    # Check if the file already exists before downloading\n    if (!file.exists(dest_file)) {\n      download.file(file_url, destfile = dest_file)\n      cat(\"Downloaded:\", dest_file, \"\\n\")\n    } else {\n      cat(\"File already exists:\", dest_file, \"\\n\")\n    }\n  }\n  \n  #same as before loops through each number in census to run the function#\n  for(number in numbers_for_census) {\n    download_and_read_tiger(number)\n  }\n\nFile already exists: Tiger2014.zip \nFile already exists: Tiger2015.zip \nFile already exists: Tiger2016.zip \nFile already exists: Tiger2017.zip \nFile already exists: Tiger2018.zip \nFile already exists: Tiger2019.zip \nFile already exists: Tiger2020.zip \nFile already exists: Tiger2021.zip \nFile already exists: Tiger2022.zip \n\n\nI appreciate your patience to get to this stage, but as you can imagine there is a lot of Data preparation before we get to the stage of demonstrating and visualizing the data."
  },
  {
    "objectID": "mp03.html#preliminary-analysis-of-vote-count-data",
    "href": "mp03.html#preliminary-analysis-of-vote-count-data",
    "title": "Will Peters STA9750-2024-FALL MP03",
    "section": "Preliminary analysis of Vote Count Data",
    "text": "Preliminary analysis of Vote Count Data\nSo now we get to the stage where we can Explore the Vote Count Data, if you consider this our third task of automating the process. We are going to be investigating just on the House of Representatives and seeing which states have gained or lost power over the past 46 years (1976-2022). Interestingly as a basketball fan 1976 was when the NBA and ABA merged, obviously not being born I have only heard stories about this.\n\nTask 3\nSo to start it off we want to see what the change in the number of house votes were, or who gained and lost the most power.\n\n#Remember House_votes is where we pulled in the csv file, if you are following along#\nChanges_in_house &lt;- House_votes |&gt;\n  group_by(year, state_po) |&gt;\n  summarise(max_year_state = max(district, na.rm = TRUE)) |&gt;\n  arrange(year, desc(max_year_state), state_po) |&gt;\n  mutate(max_year_state = ifelse(max_year_state == 0,1, max_year_state))\n#not going to go into too much detail but essentially if there is more than one district the max number for a district, will be how many districts that state had, if it only had one that the district would be 0, so including an ifelse to turn that 0 into a 1.\n\n`1976_2022_Change` &lt;- Changes_in_house |&gt;\n  filter (year %in%  c(1976,2022)) |&gt;\n  pivot_wider (\n    names_from = year,\n    values_from = max_year_state\n) |&gt;\n  mutate (`change` = `2022` - `1976`) |&gt;\n  arrange(`change`)\n#For this table, we are only keeping the start and end 1976 - 2022 and calculating the difference#\n\nkable(\n        bind_rows(\n          `1976_2022_Change` |&gt; slice_max(`change`, n = 1),   # Top result based on 2021 values\n          `1976_2022_Change` |&gt; slice_min(`change`, n = 1)    # Bottom result based on 2021 values\n        )\n)#joining the top and bottom results of who has won and lost the most\n\n\n\n\nstate_po\n1976\n2022\nchange\n\n\n\n\nTX\n24\n38\n14\n\n\nNY\n39\n26\n-13\n\n\n\n\n\nSo that is fascinating that New York has lost the most states and Texas has gained the most, now each person has there own theories, my reason would be Covid made a lot of jobs remote, and so New York lost it’s charm compared to pre-Covid times, whereas Texas and Florida were a lot more open so drew people to the freedom.\nNow who here knew about fusion voting which is candidates appearing multiple times on the ballot under multiple parties, such as Michael Zumbluskas who appeared in Republican, Conservative and Parent. This is something I had to get my head around, but thankfully only applies to New York and Connecticut.\n\nWinner_by_Unique_party &lt;- House_votes |&gt;\n  group_by(year, state_po,district) |&gt;\n  slice_max(candidatevotes, n=1, with_ties = FALSE) |&gt;\n  ungroup()\n#Calculating who would win, by the candidate votes directly, hence why no grouping by candidate\n\nWinner_by_Individual &lt;- House_votes |&gt;\n  group_by(year, state_po,district, candidate) |&gt;\n  summarise(candidatevotes = sum(candidatevotes)) |&gt;\n  group_by(year, state_po,district) |&gt;\n  slice_max(candidatevotes, n=1, with_ties = FALSE) |&gt;\n  ungroup()\n#As you can see grouping by candidate this time to aggregate across all parties. \n\nFusion_advantage &lt;- Winner_by_Individual |&gt;\n  left_join(Winner_by_Unique_party, by = c(\"year\",\"state_po\",\"district\",\"candidate\")) |&gt;\n  filter(is.na(office)) |&gt;\n  select(year,state_po,district,candidate)\n#Comparing where the individual Won due to the voting structure rather than the way it occurs in other states#\n\nkable(Fusion_advantage)\n\n\n\n\nyear\nstate_po\ndistrict\ncandidate\n\n\n\n\n1976\nNY\n29\nEDWARD W PATTISON\n\n\n1980\nNY\n3\nGREGORY W CARMAN\n\n\n1980\nNY\n6\nJOHN LEBOUTILLIER\n\n\n1984\nNY\n20\nJOSEPH J DIOGUARDI\n\n\n1986\nNY\n27\nGEORGE C WORTLEY\n\n\n1992\nCT\n2\nSAM GEJDENSON\n\n\n1992\nNY\n3\nPETER T KING\n\n\n1994\nNY\n1\nMICHAEL P FORBES\n\n\n1996\nNY\n1\nMICHAEL P FORBES\n\n\n1996\nNY\n30\nJACK QUINN\n\n\n1996\nTX\n9\nNICK LAMPSON\n\n\n2000\nCT\n2\nROB SIMMONS\n\n\n2006\nNY\n25\nJAMES T WALSH\n\n\n2006\nNY\n29\nJOHN R “RANDY” KUHL JR\n\n\n2010\nNY\n13\nMICHAEL G GRIMM\n\n\n2010\nNY\n19\nNAN HAYMORTH\n\n\n2010\nNY\n24\nRICHARD L HANNA\n\n\n2010\nNY\n25\nANN MARIE BUERKLE\n\n\n2012\nNY\n27\nCHRIS COLLINS\n\n\n2018\nNY\n1\nLEE M ZELDIN\n\n\n2018\nNY\n24\nJOHN M KATKO\n\n\n2018\nNY\n27\nCHRIS COLLINS\n\n\n2022\nNY\n4\nANTHONY P D’ESPOSITO\n\n\n2022\nNY\n17\nMICHAEL V LAWLER\n\n\n2022\nNY\n22\nBRANDON M WILLIAMS\n\n\n\n\n\nThe Texas examples here are due to the special election, still sorting out how those should be treated going forward, of course I could manually patch those out but I deem that to be unfair, and an inconsistent approach to data, as if it breaks it needs to be fixed.\nFinally I wanted to look into a larger topic, that the president is elected not by the house constituents (granted my knowledge is limited on this) but the district votes are for the house of representatives and president is at a state wide level.\nSo to start with we will prepare the data by reviewing if the Party or the President got more votes state by state.\n\nVotes_Pres_vs_House &lt;- President_votes |&gt;\n  #Starting off with the number of votes for the president#\n  group_by(year,state_po,party_simplified) |&gt;\n  summarise(candidatevotes = sum(candidatevotes)) |&gt;\n  #grouping by year, state and party and calculating how many votes did the president get#\n   inner_join(House_votes |&gt;\n               group_by(year,state_po,party) |&gt;\n               summarise(house_votes = sum(candidatevotes)), by = c(\"year\",\"state_po\",\"party_simplified\"=\"party\")) |&gt;\n  #calculating how many votes the party got at a state level#\n  mutate (Winner = ifelse(candidatevotes&gt;=house_votes,\"Candidate_favorable\",\"House_favorable\"))\n#Calculating if the candidate or the house was favorable\n\nyearly_summary &lt;- Votes_Pres_vs_House |&gt;\n  group_by(year, Winner) |&gt;\n  summarize(count = n()) |&gt;\n  ungroup()\n#Preparing the data in aggregate for candidate vs house favorable#\n\nyearly_summary_Rep &lt;- Votes_Pres_vs_House |&gt;\n  filter(party_simplified == \"REPUBLICAN\") |&gt;\n  group_by(year, Winner) |&gt;\n  summarize(count = n()) |&gt;\n  ungroup()\n#applying above logic for just Republicans#\n\nyearly_summary_Dem &lt;- Votes_Pres_vs_House |&gt;\n  filter(party_simplified == \"DEMOCRAT\") |&gt;\n  group_by(year, Winner) |&gt;\n  summarize(count = n()) |&gt;\n  ungroup()\n#applying above logic for just Democrats#\n\nAfter all that work in preparing the data tables, it comes down to how is it shown in the graphs, the visuals\n\nggplot(yearly_summary, aes(x = year, y = count, color = Winner)) +\n  geom_line(size = 1) +\n  geom_point() +\n  labs(title = \"Year-over-Year Comparison of Candidate vs House Favorable\",\n       x = \"Year\",\n       y = \"Count\",\n       color = \"Preference\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see this shows that the candidate is generally favorable but there are occasions where the House favor-ability is much higher\n\nggplot(yearly_summary_Rep, aes(x = year, y = count, color = Winner)) +\n  geom_line(size = 1) +\n  geom_point() +\n  labs(title = \"Year-over-Year Comparison of Candidate vs House Favorable\",\n       x = \"Year\",\n       y = \"Count\",\n       color = \"Preference\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nA lot more noticeable in variance until we see in 2020 where they almost perfectly align.\n\nggplot(yearly_summary_Dem, aes(x = year, y = count, color = Winner)) +\n  geom_line(size = 1) +\n  geom_point() +\n  labs(title = \"Year-over-Year Comparison of Candidate vs House Favorable\",\n       x = \"Year\",\n       y = \"Count\",\n       color = \"Preference\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn more recent years we are seeing the democratic Candidate is preferred over the house_favorable numbers.\nWhat are the takeaways for this it could be due to no simple party identifiers in the house data, or people like to hedge the bets voting one way for president and the other for house."
  },
  {
    "objectID": "mp03.html#extracting-zip-file-information",
    "href": "mp03.html#extracting-zip-file-information",
    "title": "Will Peters STA9750-2024-FALL MP03",
    "section": "Extracting ZIP file information",
    "text": "Extracting ZIP file information\n\nTask 4\n\nread_shp_from_zip &lt;- function(number) {\ntemp_dir &lt;- tempdir()\nzip_contents &lt;- unzip(paste0(\"districts\", number, \".zip\"), \n                      exdir = temp_dir)\nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\ndistricts &lt;- read_sf(fname_shp)\ndistricts\n}\n#so this creates the function read_shp_from_zip file with naming convention districts###.ZIP, note this is how we initially automatically pulled in all the information.\n\n\n\nfor (number in numbers_for_congress) {\n  joined_data &lt;- inner_join(District_Mapping,\n    data.frame(District_Number = number),\n   by = \"District_Number\")\n  year &lt;- joined_data$Year\n  districts &lt;- read_shp_from_zip(number)\n  for (year in joined_data$Year) {\n   variable_name &lt;- paste0(\"districts\",year)\n  assign(variable_name,districts)\n}\n}\n#note here is that we are not only running through the extraction of the districts but we are also converting it from district_id to districtyear i.e. will show as district2008\n\nUnique_state &lt;- districts2013 |&gt;\n  mutate(ID = substring(ID, 0, 3)) |&gt;\n  group_by(ID, STATENAME) |&gt;\n  select(ID, STATENAME) |&gt;\n as.data.frame() |&gt;\n  select(-geometry) |&gt;\n  mutate(STATENAME = toupper(STATENAME)) |&gt;\n unique()\n#Maybe an overdoing of the code but this is for future alignment, as in 2014-2022 the ID was used but no state was referenced, and so looping in the statename. #\n\n\nread_shp_from_zip_2 &lt;- function(year) {\n  temp_dir &lt;- tempdir()\n zip_contents &lt;- unzip(paste0(\"Tiger\", year, \".zip\"), \n                        exdir = temp_dir)\n  fname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n districts &lt;- read_sf(fname_shp)\n  districts\n}\n#Secondly reading in of Tiger files 2014-2022\n\nCD_Years_available &lt;- 2014:2022\n\nfor (year in CD_Years_available) {\n  variable_name &lt;- paste0(\"districts\",year)\n  districts &lt;- read_shp_from_zip_2(year)\n  districts &lt;- districts |&gt;\n    mutate(STATEFP = sprintf(\"%03d\",as.numeric(STATEFP))) |&gt;\n    inner_join(Unique_state, by = c(\"STATEFP\" = \"ID\"))\n  assign(variable_name,districts)\n}\n#similarly pulling in the Districts information by year available and the State_Name from the Unique_states above#\n\ndistricts_in_question &lt;- 1976:2022\n#Combining all districts processing into one#\nprocess_district &lt;- function(district_data) {\n  district_data |&gt;\n    group_by(STATENAME) |&gt;\n    mutate(geometry = st_make_valid(geometry),\n           STATENAME = toupper(STATENAME)) |&gt;\n    summarize(geometry = st_union(geometry), .groups = \"drop\")\n}\n#Creating the function to aggregate all the maps to State Level visualization\n\n# Example usage for a dynamically named variable\nfor (i in districts_in_question) {\n  # Construct the variable name\n  variable_name &lt;- paste0(\"districts\", sprintf(\"%04d\", i))\n  # Check if the variable exists and process it if it does\n  if (exists(variable_name, envir = .GlobalEnv)) {\n    # Get the data, process it, and assign it back to the variable\n    assign(variable_name, process_district(get(variable_name)))\n  }\n}\n#applying the loop to the function above and outputting as district-year\n\nfor (year in 1976:2022) {\n  # Dynamically create the table name\n  table_name &lt;- paste0(\"districts\", year)\n  \n  # Check if the table exists in the environment\n  if (exists(table_name)) {\n    # Get the table, add the Year column, and assign it back\n    temp_table &lt;- get(table_name)\n    temp_table$Year &lt;- year\n   assign(table_name, temp_table)\n  }\n}\n#Creating a column with the given year in each table\n\nyears &lt;- 1976:2022\n\n# Dynamically collect all tables into a list and bind them\ndistricts_combined &lt;- bind_rows(\n  lapply(years, function(year) {\n    table_name &lt;- paste0(\"districts\", year)\n    if (exists(table_name)) {\n      get(table_name)  # Retrieve each table by name\n    } else {\n      NULL  # Skip if the table doesn't exist\n    }\n  })\n)\n#creating code to aggregate all into a combined yearly analysis with the year in the file\n\nSince that has now all been aggregated and all the mapping has been brought into the r instance we are going to go through the visualization of the 2000 Presidential Election."
  },
  {
    "objectID": "mp03.html#what-are-the-ecv-allocation-schemes",
    "href": "mp03.html#what-are-the-ecv-allocation-schemes",
    "title": "Will Peters STA9750-2024-FALL MP03",
    "section": "What are the ECV Allocation Schemes?",
    "text": "What are the ECV Allocation Schemes?\n\nState-Wide Winner-Take All\nThis is using the approach that the top winning presidential candidate in that state gets all the seats in the State\n\nState_winner_take_all &lt;- President_votes |&gt;\n  group_by(state, year) |&gt;\n  # Filter for rows where candidatevotes matches the maximum for each state and year\n  filter(candidatevotes == max(candidatevotes, na.rm = TRUE)) |&gt;\n  select(state, year, party_simplified) |&gt;\n  left_join(\n    House_votes |&gt;\n      group_by(state, year) |&gt;\n      summarize(Num_districts = max(district, na.rm = FALSE) + 2) |&gt;\n      mutate(Num_districts = ifelse(Num_districts == 2, 3, Num_districts)),\n    by = c(\"state\", \"year\")\n  ) |&gt;\n  group_by(year, party_simplified) |&gt;\n  # Summarize the total Num_districts by party for each year\n  summarize(Num_districts = sum(Num_districts, na.rm = TRUE), .groups = \"drop\")\n\n\nggplot(State_winner_take_all, aes(x = year, y = Num_districts, color = party_simplified)) +\n  geom_line() +\n  labs(title = \"Seats won Over Time by Party\",\n       x = \"Year\",\n       y = \"Seats won\") +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10),  # Adjust facet label text size\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All\nThis provides each state with each District going to the winning party, except for the additional State seats (2 seats) going to the majority presidential candidate in that state.\n\ndistrict_winner &lt;- House_votes |&gt;\n  group_by(state, district, year) |&gt;\n  # Use slice_max to keep the row with the maximum votes, including party_simplified\n  slice_max(order_by = candidatevotes, n = 1, with_ties = FALSE) |&gt;\n  # Select necessary columns\n  summarize(candidate = first(candidate),\n            party = first(party),  # Retain the party of the top candidate\n            district_ecvs = 1, .groups = \"drop\") |&gt;\n  group_by(year,party) |&gt;\n  summarize(seats_won = sum(district_ecvs, na.rm = TRUE))\n\nstate_winner &lt;- House_votes |&gt;\n  group_by(state, year, candidate, party) |&gt;\n  # Sum votes across all districts to get state-wide totals\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  # Find the candidate with the highest state-wide vote\n  group_by(state, year) |&gt;\n  slice_max(order_by = total_votes, n = 1, with_ties = FALSE) |&gt;\n  # Assign two at-large ECVs to the state-wide winner\n  mutate(at_large_ecvs = 2) |&gt;\n  select(state, year, candidate, party, at_large_ecvs) |&gt;\n  group_by(year,party) |&gt;\n  summarize(seats_won = sum(at_large_ecvs,na.rm = TRUE))\n\ndistrict_and_state_winner_take_all &lt;- bind_rows(\n    district_winner |&gt; select(year, party, seats_won),\n    state_winner |&gt; select(year, party, seats_won)) |&gt;\n    group_by(year, party) |&gt;\n    # Sum up ECVs from districts and at-large for each candidate in each state\n    summarize(seats_won = sum(seats_won), .groups = \"drop\")\n  # Summarize district results\n\nggplot(district_and_state_winner_take_all, aes(x = year, y = seats_won, color = party)) +\n  geom_line() +\n  labs(title = \"Seats won Over Time by Party\",\n       x = \"Year\",\n       y = \"Seats won\") +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10),  # Adjust facet label text size\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\nState-Wide Proportional\nThis method applies the proportion of votes the candidate got across the number of seats to the individual state.\n\nstate_proportional &lt;- President_votes |&gt;\n  group_by(state, year) |&gt;\n  mutate(total_votes = sum(candidatevotes)) |&gt;\n  #Calculating number of votes at each candidate received by the number of total votes\n  left_join(House_votes |&gt;\n              group_by(state,year) |&gt;\n              summarize(Num_districts = max(district, na.rm = FALSE)+2)\n            ,join_by(\"state\",\"year\")) |&gt;\n  mutate(candidate_percentage = candidatevotes / total_votes,\n         candidate_ecvs = round(candidate_percentage * Num_districts)) |&gt;\n  select(state, year, candidate, candidate_ecvs, party_simplified) |&gt;\n  group_by(party_simplified,year) |&gt;\n  summarize(seats_won = sum(candidate_ecvs)) |&gt;\n  filter(seats_won != 0)\n\nggplot(state_proportional, aes(x = year, y = seats_won, color = party_simplified)) +\n  geom_line() +\n  labs(title = \"Seats won Over Time by Party\",\n       x = \"Year\",\n       y = \"Seats won\") +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10),  # Adjust facet label text size\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\nNational Proportional\nThis is pretty obvious as the majority rules logic, with the President getting the most votes winning the majority of seats.\n\nnational_proportional &lt;- President_votes |&gt;\n  group_by(candidate, year,party_simplified) |&gt;\n  summarize(candidate_total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(year_total_votes = sum(candidate_total_votes)) |&gt;\n  ungroup() |&gt;\n  mutate(national_percentage = candidate_total_votes / year_total_votes,\n         candidate_ecvs = round(national_percentage * 538)) |&gt;  # 538 total ECVs\n  filter(candidate_ecvs != 0) |&gt;\n  group_by(year,party_simplified) |&gt;\n  summarize(candidate_ecvs = sum(candidate_ecvs))\n\nggplot(national_proportional, aes(x = year, y = candidate_ecvs, color = party_simplified)) +\n  geom_line() +\n  labs(title = \"ECV Allocation Over Time by Party\",\n       x = \"Year\",\n       y = \"Electoral College Votes (ECVs)\") +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10),  # Adjust facet label text size\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nTake 1984 as a prime example the Republicans would have won in a popularity contest, being the national and state apportionment however it is clear that the Democrats won under the other two methodologies. Since it is representative of overall population in key areas it is right to say that the Democrats could make a claim to have won, however I think given the overwhelming republican popularity, they may have contested the election."
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "Will Peters STA9750-2024-FALL MP03",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://cdmaps.polisci.ucla.edu/↩︎\nhttps://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html↩︎\nMIT Election Data and Science Lab, 2017, “U.S. House 1976-2022,” https://doi.org/10.7910/DVN/IGOUN2, Harvard Dataverse, v13, UNF:6: Ky5FkettbvohjTSN/IvldA== [fileUNF].↩︎\nMIT Election Data and Science Lab, 2017, “U.S. President 1976-2020,” https://doi.org/10.7910/DVN/42MVDX, Harvard Dataverse, v8, UNF:6:F0opd1IRbeY190yVfzglUw== [fileUNF].↩︎"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "",
    "text": "As an introduction to this review of College Retirement Plans, I will note that the numbers being used are not predictors of future performance and one should consider there own investment goals, risk appetite and length of savings when implementing a plan to review and implement the investment strategy that best aligns with each person’s goals.\nTo get started, this review can be useful beyond a Retirement plan, as it shows the performance of different asset classes, across the years and the compound annual growth rate (CAGR) in the time frame that we are analyzing. This topic is of great interest as the original plan was you work for 45 years (approximately from 20-65/retirement age) and you are balancing the need to sustain and enjoy yourself in the short term, consider holidays, renting, restaurants. This is compared to the long term goals of investments supporting you whilst you are in retirement mode, and so the goal is save up enough money such that you can live off the investments or the fruits of you labors.\nFor this analysis I will be looking at a CUNY employee who wants to save and live a cash strapped life in retirement and as such will be working for only 16-18 years with a very restrictive lifestyle trying to adopt the Financial Independence Retire Early (FIRE) movement.\nAs with most of my analysis, I want it to be interactive and provide my code, however will be hiding the code, so it isn’t overwhelming and can be read as a report.\n\n\nshow the code for data libraries utilized in the report\n\n\nlibrary(lubridate)\nlibrary(httr2)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(fuzzyjoin)\nlibrary(purrr)\n\n\n\n\nFor the research that will be performed I needed to have an api key for both AlphaVantage and Fred. It is also a fun experiment, as you start to get more skilled in extracting through API’s instead of reading in csv and excel files every time.\n\n\nTask1/2 read in the code for password extraction and saving it for detailed use\n\n\nalpha_key &lt;- readLines(\"Alphakey.txt\")\nFred_key &lt;- readLines(\"Fred Key.txt\")\n\n\n\n\nSo for the first piece of analysis that we will be doing, it will be extracting information from AlphaVantage and so will be establishing the AlphaVantage queries.\n\n\nSetting up base functionality of AlphaVantage\n\n\n#Define the API endpoint and parameters \nbase_url_Alpha &lt;- \"https://www.alphavantage.co/query\" \nfunction_type &lt;- \"TIME_SERIES_MONTHLY_ADJUSTED\"\n\n\nTo start off we will begin by extracting details around the U.S Equities market, this part of the investigation, shows the impact that compounding investment, and the CAGR of the U.S market shows how quickly a dollar invested will grow (compared to the dollar stored under the mattress).\n\n\nExtracting U.S Markets\n\n\nsymbol &lt;- \"SPY\" # Replace with your desired stock symbol\nresponse &lt;- request(base_url_Alpha) |&gt;\n  req_url_query(\n    `function` = function_type,\n    symbol = symbol,\n    outputsize = \"full\", # Full historical data\n    apikey = alpha_key\n  ) |&gt;\n  req_perform()\n\n# Parse the JSON response\ndata &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n# Extract time series data\nU.S_Equity_Market &lt;- data[[\"Monthly Adjusted Time Series\"]]\nU.S_Equity_Market_df &lt;- as.data.frame(do.call(rbind, U.S_Equity_Market))\nU.S_Equity_Market_df &lt;- do.call(rbind, U.S_Equity_Market)\nU.S_Equity_Market_df &lt;- as.data.frame(U.S_Equity_Market_df, stringsAsFactors = FALSE)\nU.S_Equity_Market_df$Date &lt;- rownames(U.S_Equity_Market_df)\nrownames(U.S_Equity_Market_df) &lt;- NULL\n\n\n# Convert columns to apprORPiate types\nU.S_Equity_Market_df &lt;- U.S_Equity_Market_df |&gt;\n  mutate(across(-Date, as.numeric)) |&gt;\n  mutate(date = as.Date(Date)) |&gt;\n  mutate(market = \"U.S Equities\") |&gt;\n  select(date,`5. adjusted close`,market)\n\n\n# Display the first few rows\nhead(U.S_Equity_Market_df)\n\n        date 5. adjusted close       market\n1 2024-11-29          602.5500 U.S Equities\n2 2024-10-31          568.6400 U.S Equities\n3 2024-09-30          573.7600 U.S Equities\n4 2024-08-30          561.9538 U.S Equities\n5 2024-07-31          549.1232 U.S Equities\n6 2024-06-28          542.5534 U.S Equities\n\n\n\nNext we move onto downloading the performance of the International equities market, denoted by MSCI. The investments in international markets,is a useful strategy to remove country specifc risks.\n\n\nExtracting International Equities\n\n\nsymbol &lt;- \"MSCI\" # Replace with your desired stock symbol\n\n# Create and perform the request\nresponse &lt;- request(base_url_Alpha) |&gt;\n  req_url_query(\n    `function` = function_type,\n    symbol = symbol,\n    outputsize = \"full\", # Full historical data\n    apikey = alpha_key\n  ) |&gt;\n  req_perform()\n# Parse the JSON response\ndata &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n# Extract time series data\nInternational_Equity_Market &lt;- data[[\"Monthly Adjusted Time Series\"]]\nInternational_Equity_Market_df &lt;- as.data.frame(do.call(rbind, International_Equity_Market))\nInternational_Equity_Market_df &lt;- do.call(rbind, International_Equity_Market)\nInternational_Equity_Market_df &lt;- as.data.frame(International_Equity_Market_df, stringsAsFactors = FALSE)\nInternational_Equity_Market_df$Date &lt;- rownames(International_Equity_Market_df)\nrownames(International_Equity_Market_df) &lt;- NULL\n\n\n# Convert columns to apprORPiate types\nInternational_Equity_Market_df &lt;- International_Equity_Market_df |&gt;\n  mutate(across(-Date, as.numeric)) |&gt;\n  mutate(date = as.Date(Date)) |&gt;\n  mutate(market = \"International Equities\") |&gt;\n  select(date,`5. adjusted close`,market)\n\n\n\nhead(International_Equity_Market_df)\n\n        date 5. adjusted close                 market\n1 2024-11-29          609.6300 International Equities\n2 2024-10-31          569.6652 International Equities\n3 2024-09-30          581.3637 International Equities\n4 2024-08-30          579.0300 International Equities\n5 2024-07-31          537.7750 International Equities\n6 2024-06-28          479.0908 International Equities\n\n\n\nIf the reader is more risk averse then they should consider Bonds it yields a lower rate of return, however is less risky and perfect for a rainy day fund or more conservative investment strategy.\n\n\nExtracting Bond information\n\n\nsymbol &lt;- \"BND\" # Replace with your desired stock symbol\nfunction_type &lt;- \"TIME_SERIES_MONTHLY_ADJUSTED\"\n\n# Create and perform the request\nresponse &lt;- request(base_url_Alpha) |&gt;\n  req_url_query(\n    `function` = function_type,\n    symbol = symbol,\n    outputsize = \"full\", # Full historical data\n    apikey = alpha_key\n  ) |&gt;\n  req_perform()\n\n# Parse the JSON response\ndata &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n# Extract time series data\nBond_Market &lt;- data[[\"Monthly Adjusted Time Series\"]]\nBond_Market_df &lt;- as.data.frame(do.call(rbind, Bond_Market))\nBond_Market_df &lt;- do.call(rbind, Bond_Market)\nBond_Market_df &lt;- as.data.frame(Bond_Market_df, stringsAsFactors = FALSE)\nBond_Market_df$Date &lt;- rownames(Bond_Market_df)\nrownames(Bond_Market_df) &lt;- NULL\n\n\n# Convert columns to apprORPiate types\nBond_Market_df &lt;- Bond_Market_df |&gt;\n  mutate(across(-Date, as.numeric)) |&gt;\n  mutate(date = as.Date(Date)) |&gt;\n  mutate(market = \"Bond Market\") |&gt;\n  select(date,`5. adjusted close`,market)\n\nhead(Bond_Market_df)\n\n        date 5. adjusted close      market\n1 2024-11-29           73.6000 Bond Market\n2 2024-10-31           72.8215 Bond Market\n3 2024-09-30           74.6557 Bond Market\n4 2024-08-30           73.6870 Bond Market\n5 2024-07-31           72.6335 Bond Market\n6 2024-06-28           70.9621 Bond Market\n\n\n\n\n\n\nSo similar to before we are now also going to be establishing the Fred query where we will be including details of Inflation, Short-term debt returns and Wage Growth. This is always one of the controversies over the last few decades as to whether wage growth is keeping up with capital investments, and so I separated these two tables, as I don’t want to start an argument over the kitchen table.\n\n\nExtracting Inflation information\n\n\nbase_url_Fred &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\nseries_id &lt;- \"CPIAUCSL\" # This is the seried ID for Inflation or the Consumer Price Index (CPI)\n\n\n  \n  # Build API Request this should be familiar to most people who have read my assignments\n  response &lt;- request(base_url_Fred) |&gt;\n    req_url_query(\n      api_key = Fred_key,\n      file_type = \"json\",\n      series_id = \"CPIAUCSL\"\n    ) |&gt;\n    req_perform()\n  \n  # Parse Response\n  data &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n  # Extract Observations\n  inflation_df &lt;- data$observations\n  inflation_df &lt;- inflation_df |&gt;\n    mutate(\n      date = as.Date(date),\n      value = as.numeric(value)\n    ) |&gt;\n    select(date, value) |&gt;\n    arrange(date) |&gt;\n    mutate(\n      inflation = 1 + (value / lag(value) - 1) ,\n      Information = \"Inflation\"# Monthly percentage change\n    )# Retain only date and CPI value\n\n\nSo now that inflation has been analysed, we can move onto wage data, there are a few sources, so the next analysis could be utilizing a different base. For the purposes of my investigation I used this source and that is part of the risk and challenge that you face is what is the correct metric and how much stress testing do you apply.\n\n\nWage data\n\n\n#Build API Request \nresponse &lt;- request(base_url_Fred) |&gt; \n  req_url_query( \n    api_key = Fred_key, \n    file_type = \"json\",\n    series_id = \"CES0500000003\" ) |&gt; \n  req_perform()\n# Parse Response\ndata &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n\n# Extract Observations\nobservations &lt;- data$observations\n\n# Convert to Data Frame\nwage_data &lt;- data$observations\nwage_data &lt;- wage_data |&gt;\n  mutate(\n    date = as.Date(date),\n    value = as.numeric(value)\n  ) |&gt;\n  select(date, value) # Retain only date and wage value\n\n# Calculate Wage Growth (Month-over-Month or Year-over-Year)\nwage_data &lt;- wage_data |&gt;\n  arrange(date) |&gt;\n  mutate(\n    wage_growth_rate = 1 + (value / lag(value) - 1) ,\n    Information = \"wage_data\"# Monthly percentage change\n  )\n\n\nFinally we look at the Short-term debt returns or Treasury-yield data, which has a maturity (day the debt expires and the original investment is repaid) is 1-3 years.\n\n\nTreasury_yield_data\n\n\n#Treasury Yield Data#\n    \n      # Build API Request\n      response &lt;- request(base_url_Fred) |&gt;\n        req_url_query(\n          api_key = Fred_key,\n          file_type = \"json\",\n          series_id = \"GS2\"\n        ) |&gt;\n        req_perform()\n      \n      # Parse Response\n      data &lt;- resp_body_json(response, simplifyVector = TRUE)\n      treasury_yield_data &lt;- data$observations |&gt;\n        mutate(\n          date = as.Date(date),\n          value = as.numeric(value)\n        ) |&gt;\n        select(date, value)\n      \n      # Extract Observations\n      treasury_yield_data &lt;- treasury_yield_data |&gt;\n        arrange(date) |&gt;\n        mutate(\n          treasury_return = 1 + (value / lag(value) - 1) ,\n          Information = \"short-term debt\"# Monthly percentage change\n        )"
  },
  {
    "objectID": "mp04.html#market-investments",
    "href": "mp04.html#market-investments",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "Market Investments",
    "text": "Market Investments\nI am going to start by stacking the market investments. Side note, as someone who is in his late 20’s my time horizon is 30-40 years and as such I tend to be invested in riskier asset classes to take advantage of higher CAGR with more risk.\n\n\nInvestment Market performance\n\n\n      Combined_Market_df &lt;- bind_rows(\n        U.S_Equity_Market_df,\n        International_Equity_Market_df,\n        Bond_Market_df\n      )  #Combining the three investment classes into one table to compare like for like performance. \n      \n      Normalized_Market_df &lt;- Combined_Market_df |&gt;\n        group_by(market) |&gt;\n        mutate(\n          adjusted_close = `5. adjusted close`,\n          base_value = adjusted_close[date == as.Date(\"2008-01-31\")], # Get value on 2008-01-01 for each market\n          normalized_close = adjusted_close / base_value              # Normalize adjusted close\n        ) |&gt;\n        ungroup()      \n      \n      \n      ggplot(data = Normalized_Market_df |&gt; filter(date &gt;= as.Date(\"2008-01-01\")), aes(x = date, y = `normalized_close`, color = market)) +\n        geom_line(size = 1) +  # Line plot with separate colors for each market\n        labs(\n          title = \"Market Performance Over Time\",\n          x = \"Date\",\n          y = \"Adjusted Close Return (as of 2008)\",\n          color = \"Market\"\n        ) +\n        theme_minimal() +\n        scale_color_manual(values = c(\"U.S Equities\" = \"blue\", \n                                      \"International Equities\" = \"green\", \n                                      \"Bond Market\" = \"red\"))\n\n\n\n\n\n\n\n\n\nThe graph above shows the performance of these investments from 2008-2024 to show how much a dollar invested would have grown over time."
  },
  {
    "objectID": "mp04.html#opr",
    "href": "mp04.html#opr",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "OPR",
    "text": "OPR\n\n\nshow the code for OPR\n\n\nOPR &lt;- Salary_for_individual |&gt;\n  mutate (row = seq_len(nrow(Salary_for_individual)),\n          U.S_Equity = Monthly_US_Equities^12,\n          International_Equity = Monthly_International_Equities^12,\n          Bond = Monthly_Bond^12,\n          salary_contribution = (ifelse(row &lt;= 7,0.08,0.1)+Percentage)*Salary,\n          OPR_Return = 0.54*U.S_Equity + 0.36*International_Equity +0.1*Bond)\n\n\nOPR &lt;- OPR |&gt;\n  mutate(Benefit_OPR = accumulate(\n    seq_along(salary_contribution),\n    ~ .x * OPR_Return[.y] + salary_contribution[.y],\n    .init = 0\n  )[-1])\n\nOPR_Amount &lt;- max(OPR$Benefit_OPR)\n\n\n\n\nshow the code for data preparation\n\n\nhead(TRS)\n\n[1] 68580.8\n\nhead(OPR_Amount)\n\n[1] 383256.5\n\n\n\nOPR\n\n\nshow the code for data preparation\n\n\nPost_retirment_OPR &lt;- data.frame(Year = 1:20) |&gt;\n    mutate(Balance_increase = ifelse(Year &lt; 10, 0.47*(Monthly_US_Equities^12)+0.32*(Monthly_International_Equities^12)+0.21*(Monthly_Bond),0.34*(Monthly_US_Equities^12)+0.23*(Monthly_International_Equities^12)+0.43*(Monthly_Bond)))\n\ninitial_balance &lt;- 244235\ngrowth_rate_1 &lt;- 0.47*(Monthly_US_Equities^12)+0.32*(Monthly_International_Equities^12)+0.21*(Monthly_Bond)\ngrowth_rate_2 &lt;- 0.34*(Monthly_US_Equities^12)+0.23*(Monthly_International_Equities^12)+0.43*(Monthly_Bond)\ntotal_years &lt;- 20\nsplit_year &lt;- 10\n\nsimulate_balance &lt;- function(X, balance, rate_1, rate_2, split_year, total_years) {\n  for (year in 1:total_years) {\n    if (year &lt;= split_year) {\n      balance &lt;- balance * (rate_1) - X\n    } else {\n      balance &lt;- balance * (rate_2) - X\n    }\n  }\n  return(balance)\n}\n\n# Function to find consistent withdrawal amount\nfind_withdrawal &lt;- function(initial_balance, rate_1, rate_2, split_year, total_years) {\n  uniroot(\n    function(X) simulate_balance(X, initial_balance, rate_1, rate_2, split_year, total_years),\n    lower = 0, upper = initial_balance\n  )$root\n}\n\n# Calculate consistent withdrawal amount\nconsistent_withdrawal &lt;- find_withdrawal(initial_balance, growth_rate_1, growth_rate_2, split_year, total_years)\n\n# Output the result\ncat(\"The consistent annual withdrawal amount is:\", round(consistent_withdrawal, 2), \"n\")\n\nThe consistent annual withdrawal amount is: 26921.39 n\n\n\n\nTRS\n\n\nshow the code for data preparation\n\n\nInflation_rate &lt;- Monthly_average_Inflation^12 \n\nsimulate_balance_no_split &lt;- function(X, balance, rate, total_years) {\n  for (year in 1:total_years) {\n    balance &lt;- balance * (rate) - X\n  }\n  return(balance)\n}\n\n\n\n# Function to find consistent withdrawal amount\nfind_withdrawal_no_split &lt;- function(TRS, Inflation_rate , total_years) {\n  uniroot(\n    function(X) simulate_balance_no_split(X, initial_balance, Inflation_rate, total_years),\n    lower = 0, upper = initial_balance\n  )$root\n}\n\nconsistent_withdrawal_no_split &lt;- find_withdrawal_no_split(initial_balance, Inflation_rate , total_years) + Pension_Salary\n\ncat(\"The consistent annual withdrawal amount is:\", round(consistent_withdrawal_no_split, 2), \"n\")\n\nThe consistent annual withdrawal amount is: 40436.76 n"
  },
  {
    "objectID": "mp04.html#trs",
    "href": "mp04.html#trs",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "TRS",
    "text": "TRS\nCreating a Teachers Retirement System (TRS) table (note this is for Texas, we are using N.Y TRS, however I think the texas page does a better job of describing it) and functionality to calculate.\n\n\nshow the code for salary table for TRS\n\n\n salary_table &lt;- data.frame(\n        Range = c(1,2,3,4,5), # Example ranges\n        Upper_Salary = c(45000, 55000,75000, 100000,NA), # Starting salaries\n        Lower_salary = c(NA,45001,55001, 75001, 100001), # Ending salaries\n        Percentage = c(0.03,0.035,0.045,0.0575,0.06) #Percentage of the employee payout into the pension \n      )\n\n\nSo the advantage of the TRS is a long term payment of pension to ex-employees based on tenure, the table below details the calculation but the 20 year mark seems to be when the transition of methodology applies.\n\n\nshow the code for pension calculation\n\n\ncalculate_pension &lt;- function(N, FAS) {\n        if (N &lt; 20) {\n          result &lt;- 0.0167 * FAS * N\n        } else if (N == 20) {\n          result &lt;- 0.0175 * FAS * N\n        } else if (N &gt; 20) {\n          result &lt;- (0.35 + 0.02 * (N - 20)) * FAS\n        }\n        return(result)\n      }\n\n\nFor the purposes of the inflation adjustment it will be between 1-3 % based on CPI changes annually, this is beneficial when inflation is below 2% but can be disadvantageous when above 6%.\n\n\nshow the code for inflation adjustment\n\n\n    calculate_inflation_adjustment &lt;- function(CPI) {\n        # Calculate the raw inflation adjustment (50% of CPI)\n        raw_adjustment &lt;- 0.5*CPI\n        \n        # Round up to the nearest tenth of a percent\n        rounded_adjustment &lt;- ceiling(raw_adjustment*10)/10\n        \n        # Cap the adjustment between 1% and 3%\n        adjustment &lt;- pmin(pmax(rounded_adjustment,1),3)\n        \n        return(adjustment)\n      } \n\n\nThis is now the base calculation of the Salary for individual over the 16 years starting at a salary of $60,000 and growing with the wage_growth for year over year.\n\n\nshow the code for TRS preparation summary\n\n\nSalary_for_individual &lt;- Cost_of_capital |&gt;\n        mutate(Salary = cumulative_wage_growth*60000) |&gt;\n        mutate(compound_Inflation = 1 + (value_Inflation / lag(value_Inflation,12) - 1)) |&gt;\n        mutate(compound_Inflation = ifelse(is.na(compound_Inflation),cumulative_inflation,compound_Inflation)) |&gt;\n        filter(month(date) == 12) |&gt;\n        cross_join(salary_table) |&gt;\n        filter(Salary &gt;= Lower_salary & Salary &lt;= Upper_Salary) |&gt;\n        mutate(check = (compound_Inflation-1)*100) |&gt;\n        mutate(inflation_adjustment = calculate_inflation_adjustment(check),\n               pension_contribution = Salary*Percentage)\n\nSalary_for_individual$Benefit &lt;- accumulate(\n  seq_along(Salary_for_individual$pension_contribution),\n  ~ .x * (1 + (Salary_for_individual$inflation_adjustment[.y] / 100)) + Salary_for_individual$pension_contribution[.y],\n  .init = 0\n)[-1] #Calculates the Pension benefit adding previous years value and adding the pension contribution plus the compounding of the inflation adjustment.\n\nhead(Salary_for_individual |&gt;\n       mutate(Wage_growth = wage_growth_rate,Inflationary_expectations = short_term_expectations) |&gt;\n       select(date,Wage_growth,Salary,Inflationary_expectations,inflation, compound_Inflation,inflation_adjustment,pension_contribution))\n\n        date Wage_growth   Salary Inflationary_expectations inflation\n1 2008-12-31    1.003201 62182.33 deflationary expectations 0.9917665\n2 2009-12-31    1.000447 63372.70 inflationary expectations 1.0005202\n3 2010-12-31    1.001320 64506.38 inflationary expectations 1.0040166\n4 2011-12-31    1.001294 65810.11 inflationary expectations 1.0002377\n5 2012-12-31    1.004232 67255.55 deflationary expectations 0.9998789\n6 2013-12-31    1.000828 68530.94 inflationary expectations 1.0026442\n  compound_Inflation inflation_adjustment pension_contribution\n1          0.9997777                  1.0             2798.205\n2          1.0281412                  1.5             2851.771\n3          1.0143779                  1.0             2902.787\n4          1.0306207                  1.6             2961.455\n5          1.0175950                  1.0             3026.500\n6          1.0151284                  1.0             3083.892\n\n\n\nFor the final piece of the TRS you need to calculate the Final Average Salary, of the last 3 years of working and the average (mean) that will be utilizing the Pension salary and calculating the Pension\n\n\nshow the code for Pension Salary payout\n\n\nFAS &lt;- Salary_for_individual |&gt;\n  tail(3) |&gt;\n  summarize(FAS = mean(Salary)) |&gt;\n  pull(FAS) #Calculates as the mean of the final 3 years of salary\n\nN &lt;- as.numeric(nrow(Salary_for_individual))\n  \nPension_Salary &lt;- calculate_pension(N,FAS)\n\nTRS &lt;- max(Salary_for_individual$Benefit)\n\n\nFinally assuming 20 years of post working experience, the payout annually of the TRS is going to be calculated as below.\n\n\nTRS total payout of full balance\n\n\nInflation_rate &lt;- Monthly_average_Inflation^12  #Annualized monthly rate\n\ntotal_years &lt;- 20\n\nsimulate_balance_no_split &lt;- function(X, balance, rate, total_years) {\n  for (year in 1:total_years) {\n    balance &lt;- balance * (rate) - X\n  }\n  return(balance)\n} #Simulating balances for the 20 years, whilst the balance is reducing\n\n\n\n# Function to find consistent withdrawal amount\nfind_withdrawal_no_split &lt;- function(TRS, Inflation_rate , total_years) {\n  uniroot(\n    function(X) simulate_balance_no_split(X, TRS, Inflation_rate, total_years),\n    lower = 0, upper = TRS\n  )$root\n}\n\nconsistent_withdrawal_no_split &lt;- find_withdrawal_no_split(TRS, Inflation_rate , total_years) + Pension_Salary #Calculating annual payout taking the balance all the way down to 0\n\ncat(\"The consistent annual withdrawal amount is:$\", round(consistent_withdrawal_no_split, 2))\n\nThe consistent annual withdrawal amount is:$ 29269.93"
  },
  {
    "objectID": "mp04.html#setting-up-apis-for-analysis",
    "href": "mp04.html#setting-up-apis-for-analysis",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "",
    "text": "For the research that will be performed I needed to have an api key for both AlphaVantage and Fred. It is also a fun experiment, as you start to get more skilled in extracting through API’s instead of reading in csv and excel files every time.\n\n\nTask1/2 read in the code for password extraction and saving it for detailed use\n\n\nalpha_key &lt;- readLines(\"Alphakey.txt\")\nFred_key &lt;- readLines(\"Fred Key.txt\")\n\n\n\n\nSo for the first piece of analysis that we will be doing, it will be extracting information from AlphaVantage and so will be establishing the AlphaVantage queries.\n\n\nSetting up base functionality of AlphaVantage\n\n\n#Define the API endpoint and parameters \nbase_url_Alpha &lt;- \"https://www.alphavantage.co/query\" \nfunction_type &lt;- \"TIME_SERIES_MONTHLY_ADJUSTED\"\n\n\nTo start off we will begin by extracting details around the U.S Equities market, this part of the investigation, shows the impact that compounding investment, and the CAGR of the U.S market shows how quickly a dollar invested will grow (compared to the dollar stored under the mattress).\n\n\nExtracting U.S Markets\n\n\nsymbol &lt;- \"SPY\" # Replace with your desired stock symbol\nresponse &lt;- request(base_url_Alpha) |&gt;\n  req_url_query(\n    `function` = function_type,\n    symbol = symbol,\n    outputsize = \"full\", # Full historical data\n    apikey = alpha_key\n  ) |&gt;\n  req_perform()\n\n# Parse the JSON response\ndata &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n# Extract time series data\nU.S_Equity_Market &lt;- data[[\"Monthly Adjusted Time Series\"]]\nU.S_Equity_Market_df &lt;- as.data.frame(do.call(rbind, U.S_Equity_Market))\nU.S_Equity_Market_df &lt;- do.call(rbind, U.S_Equity_Market)\nU.S_Equity_Market_df &lt;- as.data.frame(U.S_Equity_Market_df, stringsAsFactors = FALSE)\nU.S_Equity_Market_df$Date &lt;- rownames(U.S_Equity_Market_df)\nrownames(U.S_Equity_Market_df) &lt;- NULL\n\n\n# Convert columns to apprORPiate types\nU.S_Equity_Market_df &lt;- U.S_Equity_Market_df |&gt;\n  mutate(across(-Date, as.numeric)) |&gt;\n  mutate(date = as.Date(Date)) |&gt;\n  mutate(market = \"U.S Equities\") |&gt;\n  select(date,`5. adjusted close`,market)\n\n\n# Display the first few rows\nhead(U.S_Equity_Market_df)\n\n        date 5. adjusted close       market\n1 2024-11-29          602.5500 U.S Equities\n2 2024-10-31          568.6400 U.S Equities\n3 2024-09-30          573.7600 U.S Equities\n4 2024-08-30          561.9538 U.S Equities\n5 2024-07-31          549.1232 U.S Equities\n6 2024-06-28          542.5534 U.S Equities\n\n\n\nNext we move onto downloading the performance of the International equities market, denoted by MSCI. The investments in international markets,is a useful strategy to remove country specifc risks.\n\n\nExtracting International Equities\n\n\nsymbol &lt;- \"MSCI\" # Replace with your desired stock symbol\n\n# Create and perform the request\nresponse &lt;- request(base_url_Alpha) |&gt;\n  req_url_query(\n    `function` = function_type,\n    symbol = symbol,\n    outputsize = \"full\", # Full historical data\n    apikey = alpha_key\n  ) |&gt;\n  req_perform()\n# Parse the JSON response\ndata &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n# Extract time series data\nInternational_Equity_Market &lt;- data[[\"Monthly Adjusted Time Series\"]]\nInternational_Equity_Market_df &lt;- as.data.frame(do.call(rbind, International_Equity_Market))\nInternational_Equity_Market_df &lt;- do.call(rbind, International_Equity_Market)\nInternational_Equity_Market_df &lt;- as.data.frame(International_Equity_Market_df, stringsAsFactors = FALSE)\nInternational_Equity_Market_df$Date &lt;- rownames(International_Equity_Market_df)\nrownames(International_Equity_Market_df) &lt;- NULL\n\n\n# Convert columns to apprORPiate types\nInternational_Equity_Market_df &lt;- International_Equity_Market_df |&gt;\n  mutate(across(-Date, as.numeric)) |&gt;\n  mutate(date = as.Date(Date)) |&gt;\n  mutate(market = \"International Equities\") |&gt;\n  select(date,`5. adjusted close`,market)\n\n\n\nhead(International_Equity_Market_df)\n\n        date 5. adjusted close                 market\n1 2024-11-29          609.6300 International Equities\n2 2024-10-31          569.6652 International Equities\n3 2024-09-30          581.3637 International Equities\n4 2024-08-30          579.0300 International Equities\n5 2024-07-31          537.7750 International Equities\n6 2024-06-28          479.0908 International Equities\n\n\n\nIf the reader is more risk averse then they should consider Bonds it yields a lower rate of return, however is less risky and perfect for a rainy day fund or more conservative investment strategy.\n\n\nExtracting Bond information\n\n\nsymbol &lt;- \"BND\" # Replace with your desired stock symbol\nfunction_type &lt;- \"TIME_SERIES_MONTHLY_ADJUSTED\"\n\n# Create and perform the request\nresponse &lt;- request(base_url_Alpha) |&gt;\n  req_url_query(\n    `function` = function_type,\n    symbol = symbol,\n    outputsize = \"full\", # Full historical data\n    apikey = alpha_key\n  ) |&gt;\n  req_perform()\n\n# Parse the JSON response\ndata &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n# Extract time series data\nBond_Market &lt;- data[[\"Monthly Adjusted Time Series\"]]\nBond_Market_df &lt;- as.data.frame(do.call(rbind, Bond_Market))\nBond_Market_df &lt;- do.call(rbind, Bond_Market)\nBond_Market_df &lt;- as.data.frame(Bond_Market_df, stringsAsFactors = FALSE)\nBond_Market_df$Date &lt;- rownames(Bond_Market_df)\nrownames(Bond_Market_df) &lt;- NULL\n\n\n# Convert columns to apprORPiate types\nBond_Market_df &lt;- Bond_Market_df |&gt;\n  mutate(across(-Date, as.numeric)) |&gt;\n  mutate(date = as.Date(Date)) |&gt;\n  mutate(market = \"Bond Market\") |&gt;\n  select(date,`5. adjusted close`,market)\n\nhead(Bond_Market_df)\n\n        date 5. adjusted close      market\n1 2024-11-29           73.6000 Bond Market\n2 2024-10-31           72.8215 Bond Market\n3 2024-09-30           74.6557 Bond Market\n4 2024-08-30           73.6870 Bond Market\n5 2024-07-31           72.6335 Bond Market\n6 2024-06-28           70.9621 Bond Market\n\n\n\n\n\n\nSo similar to before we are now also going to be establishing the Fred query where we will be including details of Inflation, Short-term debt returns and Wage Growth. This is always one of the controversies over the last few decades as to whether wage growth is keeping up with capital investments, and so I separated these two tables, as I don’t want to start an argument over the kitchen table.\n\n\nExtracting Inflation information\n\n\nbase_url_Fred &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\nseries_id &lt;- \"CPIAUCSL\" # This is the seried ID for Inflation or the Consumer Price Index (CPI)\n\n\n  \n  # Build API Request this should be familiar to most people who have read my assignments\n  response &lt;- request(base_url_Fred) |&gt;\n    req_url_query(\n      api_key = Fred_key,\n      file_type = \"json\",\n      series_id = \"CPIAUCSL\"\n    ) |&gt;\n    req_perform()\n  \n  # Parse Response\n  data &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n  # Extract Observations\n  inflation_df &lt;- data$observations\n  inflation_df &lt;- inflation_df |&gt;\n    mutate(\n      date = as.Date(date),\n      value = as.numeric(value)\n    ) |&gt;\n    select(date, value) |&gt;\n    arrange(date) |&gt;\n    mutate(\n      inflation = 1 + (value / lag(value) - 1) ,\n      Information = \"Inflation\"# Monthly percentage change\n    )# Retain only date and CPI value\n\n\nSo now that inflation has been analysed, we can move onto wage data, there are a few sources, so the next analysis could be utilizing a different base. For the purposes of my investigation I used this source and that is part of the risk and challenge that you face is what is the correct metric and how much stress testing do you apply.\n\n\nWage data\n\n\n#Build API Request \nresponse &lt;- request(base_url_Fred) |&gt; \n  req_url_query( \n    api_key = Fred_key, \n    file_type = \"json\",\n    series_id = \"CES0500000003\" ) |&gt; \n  req_perform()\n# Parse Response\ndata &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n\n# Extract Observations\nobservations &lt;- data$observations\n\n# Convert to Data Frame\nwage_data &lt;- data$observations\nwage_data &lt;- wage_data |&gt;\n  mutate(\n    date = as.Date(date),\n    value = as.numeric(value)\n  ) |&gt;\n  select(date, value) # Retain only date and wage value\n\n# Calculate Wage Growth (Month-over-Month or Year-over-Year)\nwage_data &lt;- wage_data |&gt;\n  arrange(date) |&gt;\n  mutate(\n    wage_growth_rate = 1 + (value / lag(value) - 1) ,\n    Information = \"wage_data\"# Monthly percentage change\n  )\n\n\nFinally we look at the Short-term debt returns or Treasury-yield data, which has a maturity (day the debt expires and the original investment is repaid) is 1-3 years.\n\n\nTreasury_yield_data\n\n\n#Treasury Yield Data#\n    \n      # Build API Request\n      response &lt;- request(base_url_Fred) |&gt;\n        req_url_query(\n          api_key = Fred_key,\n          file_type = \"json\",\n          series_id = \"GS2\"\n        ) |&gt;\n        req_perform()\n      \n      # Parse Response\n      data &lt;- resp_body_json(response, simplifyVector = TRUE)\n      treasury_yield_data &lt;- data$observations |&gt;\n        mutate(\n          date = as.Date(date),\n          value = as.numeric(value)\n        ) |&gt;\n        select(date, value)\n      \n      # Extract Observations\n      treasury_yield_data &lt;- treasury_yield_data |&gt;\n        arrange(date) |&gt;\n        mutate(\n          treasury_return = 1 + (value / lag(value) - 1) ,\n          Information = \"short-term debt\"# Monthly percentage change\n        )"
  },
  {
    "objectID": "mp04.html#cost-of-capital",
    "href": "mp04.html#cost-of-capital",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "Cost of Capital",
    "text": "Cost of Capital\nSo I define the Cost_of_capital being more direct investments or day to day aspects of life, this includes information around Wage and Inflation instead of the opportunity cost of investing in bonds, and equities\n\n\nshow the code for Cost Of Capital\n\n\nCost_of_capital &lt;- wage_data |&gt;\n        full_join(inflation_df, by = \"date\", suffix = c(\"_Wage\", \"_Inflation\")) |&gt;\n        full_join(treasury_yield_data, by = \"date\", suffix = c(\"\",\"_Short_Term\")) |&gt;\n        mutate(value_short_term = value) |&gt;\n        filter(date &gt;= as.Date(\"2008-01-01\")) |&gt;\n        mutate(date = ceiling_date(date, \"month\") - days(1)) |&gt;\n        arrange(date) |&gt;\n        mutate(cumulative_wage_growth = cumprod(wage_growth_rate),\n               short_term_expectations = ifelse(treasury_return &gt;= 1,\"inflationary expectations\",\"deflationary expectations\"),\n               cumulative_inflation = cumprod(inflation)) #This is joining the inflation, bond and treasury yield from 2008 and with the short_term_expectations being inflationary or deflationary\n      \n      head(Cost_of_capital |&gt;\n        mutate(Wage_growth = cumulative_wage_growth,inflationary_expectations = short_term_expectations,inflation = cumulative_inflation) |&gt;\n        select(date,Wage_growth,inflationary_expectations,inflation))\n\n        date Wage_growth inflationary_expectations inflation\n1 2008-01-31    1.001417 deflationary expectations  1.003448\n2 2008-02-29    1.004251 deflationary expectations  1.005874\n3 2008-03-31    1.008975 deflationary expectations  1.009473\n4 2008-04-30    1.009920 inflationary expectations  1.011809\n5 2008-05-31    1.014171 inflationary expectations  1.017797\n6 2008-06-30    1.016060 inflationary expectations  1.028461\n\n      #shows a sample of the data over a few of the key fields\n      \n      pivoted_Cost_of_capital &lt;- Cost_of_capital |&gt;\n        select(date, cumulative_wage_growth, short_term_expectations, cumulative_inflation) |&gt;\n        pivot_longer(\n          cols = starts_with(\"cumulative\"), # Select columns to pivot\n          names_to = \"metric\",              # Name for the new 'metric' column\n          values_to = \"value\"               # Name for the new 'value' column\n        )\n      \n      ggplot(data = pivoted_Cost_of_capital |&gt; filter (metric != \"short_term_expectations\"), aes(x = date, y = value, color = metric)) +\n        geom_line(size = 1) +  # Line plot with separate colors for each market\n        labs(\n          title = \"Market Performance Over Time\",\n          x = \"Date\",\n          y = \"Adjusted Close Return (as of 2008)\",\n          color = \"Market\"\n        ) +\n        theme_minimal() +\n        scale_color_manual(values = c(\"cumulative_wage_growth\" = \"blue\", \n                                      \"cumulative_inflation\" = \"red\"))\n\n\n\n\n\n\n\n\n\nThis presents the performance of the wage growth vs inflation to show over time how inflation and wages have grown since 2008, I don’t want to draw too many parallels but which do you think was higher operational expense increase or market investment growth?"
  },
  {
    "objectID": "mp04.html#cumulative-growth-of-investments",
    "href": "mp04.html#cumulative-growth-of-investments",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "Cumulative Growth of investments",
    "text": "Cumulative Growth of investments\nNext we want to develop the cumulative growth of the different asset classes, with the purpose to be later to get a base line long term growth per period.\n\n\nshow the code for cumulative growth of investment asset classes\n\n\n     U.S_Equity_Market_df &lt;- U.S_Equity_Market_df |&gt;\n        arrange(U.S_Equity_Market_df$date) |&gt;\n        mutate(Equities_Performance = 1 + (`5. adjusted close` / lag(`5. adjusted close`) - 1))|&gt;\n        mutate(Equities_Performance = ifelse(is.na(Equities_Performance),1,Equities_Performance))\n     \ntail(U.S_Equity_Market_df, n = 1)\n\n          date 5. adjusted close       market Equities_Performance\n300 2024-11-29            602.55 U.S Equities             1.059634\n\n      International_Equity_Market_df &lt;- International_Equity_Market_df |&gt;\n        arrange(International_Equity_Market_df$date) |&gt;\n        mutate(Equities_Performance = 1 + (`5. adjusted close` / lag(`5. adjusted close`) - 1)) |&gt;\n        mutate(Equities_Performance = ifelse(is.na(Equities_Performance),1,Equities_Performance))\n          \ntail(International_Equity_Market_df, n = 1)      \n\n          date 5. adjusted close                 market Equities_Performance\n204 2024-11-29            609.63 International Equities             1.070155\n\n      Bond_Market_df &lt;- Bond_Market_df |&gt;\n        arrange(Bond_Market_df$date) |&gt;\n        mutate(Equities_Performance = 1 + (`5. adjusted close` / lag(`5. adjusted close`) - 1)) |&gt;\n        mutate(Equities_Performance = ifelse(is.na(Equities_Performance),1,Equities_Performance))\n\ntail(Bond_Market_df, n = 1)\n\n          date 5. adjusted close      market Equities_Performance\n211 2024-11-29              73.6 Bond Market             1.010691"
  },
  {
    "objectID": "mp04.html#average-monthly-change-of-investments",
    "href": "mp04.html#average-monthly-change-of-investments",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "Average Monthly Change of investments",
    "text": "Average Monthly Change of investments\nAverage Monthly changes of the wage, Inflation, U.S Equities, International Equities and Bond to provide the consistent performance to estimate future performance\n\n\nshow the code for data Monthly average returns of data\n\n\n#General formula is the product of all the wage growth rates to the power of 1/row count\n  Monthly_average_wage_growth &lt;- prod(Cost_of_capital$wage_growth_rate)^(1/nrow(Cost_of_capital))\n      Monthly_average_Inflation &lt;- prod(Cost_of_capital$inflation)^(1/nrow(Cost_of_capital))\n      Monthly_US_Equities &lt;- prod(U.S_Equity_Market_df$Equities_Performance)^(1/nrow(U.S_Equity_Market_df))\n      Monthly_International_Equities &lt;- prod(International_Equity_Market_df$Equities_Performance)^(1/nrow(International_Equity_Market_df))\n      Monthly_Bond &lt;- prod(Bond_Market_df$Equities_Performance)^(1/nrow(Bond_Market_df))"
  },
  {
    "objectID": "mp04.html#opr-1",
    "href": "mp04.html#opr-1",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "OPR",
    "text": "OPR\n\n\nshow the code for data preparation\n\n\nSalary_for_individual_Monte_Carlo &lt;- replicate(20, {\n  \n  # Generate random `value_Inflation` for this simulation\n  Cost_of_capital |&gt;\n    mutate(\n      Salary = cumulative_wage_growth * rnorm(10,mean = 60000, sd = 5000),\n      compound_Inflation = 1 + (value_Inflation / lag(value_Inflation, 12) - 1),\n      compound_Inflation = ifelse(is.na(compound_Inflation), cumulative_inflation, compound_Inflation),\n      compound_Inflation = rnorm(1,mean = compound_Inflation,sd = (compound_Inflation-1)),\n      check = (compound_Inflation - 1) * 100,\n      inflation_adjustment = calculate_inflation_adjustment(check)\n    ) |&gt;\n    filter(month(date) == 12) |&gt;\n    cross_join(salary_table) |&gt;\n    filter(Salary &gt;= Lower_salary & Salary &lt;= Upper_Salary) |&gt;\n    mutate(pension_contribution = Salary * Percentage) |&gt;\n    mutate(\n      Benefit = accumulate(\n        seq_along(pension_contribution),\n        ~ .x * (1 + (inflation_adjustment[.y] / 100)) + pension_contribution[.y],\n        .init = 0\n      )[-1]\n    )\n}, simplify = FALSE)\n\n# Analyze Results\n# Combine results into a single data frame\nall_simulations &lt;- bind_rows(Salary_for_individual_Monte_Carlo, .id = \"Simulation\")\n\n\n\n\nshow the code for data preparation\n\n\n# Example analysis: Final Benefit across simulations\nfinal_benefits &lt;- all_simulations |&gt;\n  group_by(Simulation) |&gt;\n  summarize(Final_Benefit = max(Benefit, na.rm = TRUE))\n\nprint(final_benefits)\n\n# A tibble: 20 × 2\n   Simulation Final_Benefit\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 1                 66519.\n 2 10                63883.\n 3 11                59601.\n 4 12                54435.\n 5 13                61801.\n 6 14                56251.\n 7 15                61980.\n 8 16                62505.\n 9 17                67551.\n10 18                58004.\n11 19                64008.\n12 2                 65062.\n13 20                57986.\n14 3                 61223.\n15 4                 60931.\n16 5                 60344.\n17 6                 58778.\n18 7                 58681.\n19 8                 61443.\n20 9                 60565.\n\n# Summary statistics for Final Benefit\nsummary_stats &lt;- final_benefits |&gt;\n  summarize(\n    Mean_Final_Benefit = mean(Final_Benefit),\n    Median_Final_Benefit = median(Final_Benefit),\n    Probability_Negative = mean(Final_Benefit &lt; 0)\n  )\n\n# Print summary statistics\nhead(summary_stats)\n\n# A tibble: 1 × 3\n  Mean_Final_Benefit Median_Final_Benefit Probability_Negative\n               &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n1             61078.               61077.                    0\n\n# Histogram of final benefits in the TRS Strategy\nggplot(final_benefits, aes(x = Final_Benefit)) +\n  geom_histogram(binwidth = 1000, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Final Benefits\", x = \"Final Benefit\", y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "mp04.html#trs-1",
    "href": "mp04.html#trs-1",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "TRS",
    "text": "TRS\nTeachers Retirement System Monte Carlo will be replicated across both a variance of salary and inflation as the two primary variables in this calculation.\n\n\nMonte Carlo situation creation\n\n\nSalary_for_individual_Monte_Carlo &lt;- replicate(100, {\n  \n  # Generate random `value_Inflation` for this simulation\n  Cost_of_capital |&gt;\n    mutate(\n      Salary = cumulative_wage_growth * rnorm(10,mean = 60000, sd = 5000), #performing a variance analysis of salary starting at $60,000 with a standard deviation of $5,000 \n      compound_Inflation = 1 + (value_Inflation / lag(value_Inflation, 12) - 1),\n      compound_Inflation = ifelse(is.na(compound_Inflation), cumulative_inflation, compound_Inflation),\n      compound_Inflation = rnorm(1,mean = compound_Inflation,sd = (compound_Inflation-1)), #performing a variance analysis of the Calculated compounded inflation to see effects if the value changes for each year.\n      check = (compound_Inflation - 1) * 100,\n      inflation_adjustment = calculate_inflation_adjustment(check)\n    ) |&gt;\n    filter(month(date) == 12) |&gt; # Performing annualized value by year end balance\n    cross_join(salary_table) |&gt;\n    filter(Salary &gt;= Lower_salary & Salary &lt;= Upper_Salary) |&gt;\n    mutate(pension_contribution = Salary * Percentage) |&gt;\n    mutate(\n      Benefit = accumulate(\n        seq_along(pension_contribution),\n        ~ .x * (1 + (inflation_adjustment[.y] / 100)) + pension_contribution[.y],\n        .init = 0\n      )[-1]\n    ) #Calculation of benefits\n}, simplify = FALSE) \n\n# Analyze Results\n# Combine results into a single data frame\nall_simulations &lt;- bind_rows(Salary_for_individual_Monte_Carlo, .id = \"Simulation\") #stacking simulation results\n\n\nGeneration of ending balances for the TRS approach, I am not going to spoil the story, but it is interesting to see how changing the inflation and starting salary can cause differences of $10,000 in savings, and that is prior to even analyzing the pension FAS.\n\n\nTRS Aggregation of the simulations\n\n\n# Example analysis: Final Benefit across simulations\nfinal_benefits &lt;- all_simulations |&gt;\n  group_by(Simulation) |&gt;\n  summarize(Final_Benefit = max(Benefit, na.rm = TRUE))\n\nfinal_benefits &lt;- final_benefits |&gt;  \n  mutate(Simulation = as.numeric(Simulation)) |&gt;\n  arrange(Simulation)\n        \nhead(n = 10,final_benefits)\n\n# A tibble: 10 × 2\n   Simulation Final_Benefit\n        &lt;dbl&gt;         &lt;dbl&gt;\n 1          1        65095.\n 2          2        61461.\n 3          3        54554.\n 4          4        60698.\n 5          5        66969.\n 6          6        63876.\n 7          7        67014.\n 8          8        64176.\n 9          9        63341.\n10         10        67396.\n\n# Summary statistics for Final Benefit\nsummary_stats &lt;- final_benefits |&gt;\n  summarize(\n    Mean_Final_Benefit = mean(Final_Benefit),\n    Median_Final_Benefit = median(Final_Benefit),\n    Probability_Negative = mean(Final_Benefit &lt; 0)\n  )\n\n# Print summary statistics\nhead(summary_stats)\n\n# A tibble: 1 × 3\n  Mean_Final_Benefit Median_Final_Benefit Probability_Negative\n               &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n1             61910.               62204.                    0\n\n# Histogram of final benefits in the TRS Strategy\nggplot(final_benefits, aes(x = Final_Benefit)) +\n  geom_histogram(binwidth = 1000, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Final Benefits\", x = \"Final Benefit\", y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "mp04.html#orp",
    "href": "mp04.html#orp",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "ORP",
    "text": "ORP\nThe Optional Retirement Plan (ORP) is the alternative which is more common in the private sector. Personally I am more biased to this approach, as there is more flexibility for where investments are allocated. The approach generally follows the rule that as people get older they reduce the allocation to Equities towards Bonds and Fixed Income products.\n\n\nshow the code for ORP year over year based on salary increases\n\n\nORP &lt;- Salary_for_individual |&gt;\n  mutate (row = seq_len(nrow(Salary_for_individual)),\n          U.S_Equity = Monthly_US_Equities^12,\n          International_Equity = Monthly_International_Equities^12,\n          Bond = Monthly_Bond^12,\n          salary_contribution = (ifelse(row &lt;= 7,0.08,0.1)+Percentage)*Salary,\n          ORP_Return = 0.54*U.S_Equity + 0.36*International_Equity +0.1*Bond)\n\n\nORP &lt;- ORP |&gt;\n  mutate(Benefit_ORP = accumulate(\n    seq_along(salary_contribution),\n    ~ .x * ORP_Return[.y] + salary_contribution[.y],\n    .init = 0\n  )[-1]) #Similar to before does the contribution +return multiplied by adjustment\n\nORP_Amount &lt;- max(ORP$Benefit_ORP)\n\n\nNext we will need to see across the 16 years how much money is saved by the end of the time period, the thing to note is given that no pension is paid in ORP, you would expect the ending value of ORP is higher than TRS, as we haven’t applied a Discounted Cash Flow method to the pension payments.\n\n\nShow the value of TRS and ORP\n\n\nhead(TRS)\n\n[1] 68580.8\n\nhead(ORP_Amount)\n\n[1] 383256.5\n\n\n\nNext we will be looking at the 20 years of the ORP with a consistent payout.\n\n\nshow the code for ORP payout\n\n\nPost_retirment_ORP &lt;- data.frame(Year = 1:20) |&gt;\n    mutate(Balance_increase = ifelse(Year &lt; 10, 0.47*(Monthly_US_Equities^12)+0.32*(Monthly_International_Equities^12)+0.21*(Monthly_Bond),0.34*(Monthly_US_Equities^12)+0.23*(Monthly_International_Equities^12)+0.43*(Monthly_Bond)))\n\ninitial_balance &lt;- 244235\ngrowth_rate_1 &lt;- 0.47*(Monthly_US_Equities^12)+0.32*(Monthly_International_Equities^12)+0.21*(Monthly_Bond)\ngrowth_rate_2 &lt;- 0.34*(Monthly_US_Equities^12)+0.23*(Monthly_International_Equities^12)+0.43*(Monthly_Bond)\ntotal_years &lt;- 20\nsplit_year &lt;- 10\n\nsimulate_balance &lt;- function(X, balance, rate_1, rate_2, split_year, total_years) {\n  for (year in 1:total_years) {\n    if (year &lt;= split_year) {\n      balance &lt;- balance * (rate_1) - X\n    } else {\n      balance &lt;- balance * (rate_2) - X\n    }\n  }\n  return(balance)\n}\n\n# Function to find consistent withdrawal amount\nfind_withdrawal &lt;- function(initial_balance, rate_1, rate_2, split_year, total_years) {\n  uniroot(\n    function(X) simulate_balance(X, initial_balance, rate_1, rate_2, split_year, total_years),\n    lower = 0, upper = initial_balance\n  )$root\n}\n\n# Calculate consistent withdrawal amount\nconsistent_withdrawal &lt;- find_withdrawal(initial_balance, growth_rate_1, growth_rate_2, split_year, total_years)\n\n# Output the result\ncat(\"The consistent annual withdrawal amount is:$\", round(consistent_withdrawal, 2))\n\nThe consistent annual withdrawal amount is:$ 26921.39"
  },
  {
    "objectID": "mp04.html#orp-1",
    "href": "mp04.html#orp-1",
    "title": "Will Peters STA9750-2024-FALL MP04",
    "section": "ORP",
    "text": "ORP\nNext is the Monte Carlo analysis for the Optional Retirement Plan which I have taken from end to end of starting salary to retirement payout.\n\n\nCreation of the ORP Balances at each Year\n\n\n# Constants\nInflation_rate &lt;- Monthly_average_Inflation^12  # Example inflation rate\nn_simulations &lt;- 20  # Number of Monte Carlo simulations per Final_Benefit\n\n# Perform Monte Carlo simulations for each `Final_Benefit`\nall_simulations &lt;- lapply(final_benefits$Final_Benefit, function(initial_balance) {\n  \n  # Run `n_simulations` for this specific initial balance\n  replicate(n_simulations, {\n    # Randomize parameters for this simulation\n    total_years &lt;- round(rnorm(1, mean = 20, sd = 5))  # Randomized total years\n    Inflation_rate_sim &lt;- rnorm(1, mean = Inflation_rate, sd = (Inflation_rate - 1) / 10)  # Randomized rate\n    withdrawal &lt;- rnorm(1, mean = 20000, sd = 5000)  # Randomized withdrawal\n    Pension_amount &lt;- rnorm(1, mean = Pension_Salary, sd = Pension_Salary / 10)  # Randomized Pension\n    \n    balance &lt;- initial_balance  # Use this Final_Benefit as the starting balance\n    \n    # Simulate balance year by year\n    for (year in 1:total_years) {\n      balance &lt;- balance * (Inflation_rate_sim) - withdrawal + Pension_amount\n      if (balance &lt;= 0) break  # Stop if balance is depleted\n    }\n    \n    return(balance)  # Return the final balance\n  })\n})\n\n# Combine all simulations into a data frame\nall_simulations_df &lt;- data.frame(\n  Simulation = rep(1:n_simulations, length(final_benefits$Final_Benefit)),\n  Final_Benefit_Index = rep(seq_along(final_benefits$Final_Benefit), each = n_simulations),\n  Final_Balance = unlist(all_simulations)\n)\n\n# Summary statistics for each Final_Benefit index\nsummary_stats &lt;- all_simulations_df |&gt;\n  group_by(Final_Benefit_Index) |&gt;\n  summarize(\n    Mean_Final_Balance = mean(Final_Balance),\n    Median_Final_Balance = median(Final_Balance),\n    Probability_Negative = mean(Final_Balance &lt; 0)\n  )\n\n# Print summary statistics\nhead(summary_stats)\n\n# A tibble: 6 × 4\n  Final_Benefit_Index Mean_Final_Balance Median_Final_Balance\n                &lt;int&gt;              &lt;dbl&gt;                &lt;dbl&gt;\n1                   1            206242.              207722.\n2                   2            221285.              238889.\n3                   3            146312.               77072.\n4                   4            219075.              183404.\n5                   5            246173.              262675.\n6                   6            155935.              150039.\n# ℹ 1 more variable: Probability_Negative &lt;dbl&gt;\n\n# Plot the distribution of simulated final balances for all `Final_Benefit`\nggplot(all_simulations_df, aes(x = Final_Balance)) +\n  geom_histogram(binwidth = 100000, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Simulated Final Balances for Each Final Benefit\",\n    x = \"Final Balance\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNext will be combining the simulations and showing the ending balances for each of the simulations run.\n\n\nShowing ending balances of the ORP across the Monte Carlo applications\n\n\n#Simulation of Benefits for ORP \nMonte_carlo_ORP &lt;- replicate(20, {\n  Salary_for_individual &lt;- Salary_for_individual |&gt;\n  mutate (Salary = (rnorm(1,mean = Salary, sd = Salary/10)),\n          U.S_Equity = Monthly_US_Equities^12,\n          International_Equity = Monthly_International_Equities^12,\n          Bond = Monthly_Bond^12,\n          U.S_Equity = rnorm(n(), mean = U.S_Equity, sd = (U.S_Equity - 1)/10),\n          International_Equity = rnorm(n(), mean = International_Equity, sd = (International_Equity - 1)/10),\n          Bond = rnorm(n(), mean = Bond, sd = (Bond - 1)/10))\n  \n  ORP_MC &lt;-Salary_for_individual |&gt;\n    mutate (row = seq_len(n()),\n          U.S_Equity = Monthly_US_Equities^12,\n          International_Equity = Monthly_International_Equities^12,\n          Bond = Monthly_Bond^12,\n          salary_contribution = (ifelse(row &lt;= 7,0.08,0.1)+Percentage)*Salary,\n          ORP_Return = 0.54*U.S_Equity+0.36*International_Equity+0.1*Bond)|&gt;\n  mutate(Benefit_ORP = accumulate(\n    seq_along(salary_contribution),\n    ~ .x * ORP_Return[.y] + salary_contribution[.y],\n    .init = 0\n  )[-1])\n max(ORP_MC$Benefit_ORP)\n},simplify = TRUE)\n\nmonte_carlo_results_ORP &lt;- data.frame(\n  Simulation = 1:n_simulations,\n  ORP_Amount = Monte_carlo_ORP)  \n\nggplot(monte_carlo_results_ORP, aes(x = ORP_Amount)) +   \n  geom_histogram(binwidth = 50000, fill = \"blue\", color = \"black\", alpha = 0.7) +   \n  labs(title = \"Distribution of Simulated ORP Amounts\",x = \"ORP Amount\",y = \"Frequency\") +  \n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\n\nMonte carlo simulation of the ORP balances and stacking in easier to read format\n\n\nall_simulations_ORP_List &lt;- lapply(seq_along(monte_carlo_results_ORP$ORP_Amount), function(index) {\n  ORP_Amount &lt;- monte_carlo_results_ORP$ORP_Amount[index]\n  \n  # Define the simulation function\n  simulate_post_retirement &lt;- function(sim_id) {\n    # Randomize parameters for this simulation\n    total_years &lt;- max(1, round(rnorm(1, mean = 20, sd = 5)))  # Ensure at least 1 year\n    withdrawal &lt;- rnorm(1, mean = 30000, sd = 100)\n    U.S_Equity &lt;- Monthly_US_Equities^12\n    International_Equity &lt;- Monthly_International_Equities^12\n    Bond &lt;- Monthly_Bond^12\n    U.S_Equity &lt;- rnorm(1, mean = U.S_Equity, sd = (U.S_Equity - 1)/10)\n    International_Equity &lt;- rnorm(1, mean = International_Equity, sd = (International_Equity - 1)/10)\n    Bond &lt;- rnorm(1, mean = Bond, sd = (Bond - 1)/10)\n\n    \n    # Define growth rates based on randomized parameters\n    growth_rate_1 &lt;- 0.47 * U.S_Equity + 0.32 * International_Equity + 0.21 * Bond\n    growth_rate_2 &lt;- 0.34 * U.S_Equity + 0.23 * International_Equity + 0.43 * Bond\n    split_year &lt;- min(10, total_years)  # Ensure split_year &lt;= total_years\n\n    # Simulate balance evolution\n    balance &lt;- ORP_Amount  # Use ORP_Amount as initial balance\n        balances &lt;- numeric(total_years)\n    \n    for (year in 1:total_years) {\n      if (year &lt;= split_year) {\n        balance &lt;- balance * (growth_rate_1) - withdrawal\n      } else {\n        balance &lt;- balance * (growth_rate_2) - withdrawal\n      }\n      balances[year] &lt;- balance\n    }\n    \n    # Return the evolution of balance along with simulation ID\n    data.frame(Simulation = sim_id, Year = seq_len(total_years), Balance = balances)\n  }\n    # Apply 20 simulations for the current ORP_Amount\n  bind_rows(lapply(1:20, simulate_post_retirement), .id = \"Simulation\")\n})\n\n\nNow to demonstrate the extract of the amounts if reduced as part of the benefits of a payout taken.\n\n\nVisualization of balances across ORP post retirement\n\n\n# Combine all results into a single data frame\nall_simulations_ORP &lt;- bind_rows(all_simulations_ORP_List, .id = \"ORP_Amount_Index\")\n\nORP_Balance_Monte &lt;- all_simulations_ORP |&gt;\n  group_by(ORP_Amount_Index, Simulation) |&gt;\n  slice_max(Year, n = 1, with_ties = FALSE) |&gt;  # Get the row with the maximum balance\n  ungroup()\n\n# Plot all simulations\nggplot(ORP_Balance_Monte, aes(x = Balance)) +\n  geom_histogram(binwidth = 100000, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Simulated Final Balances for Each Final Benefit\",\n    x = \"Final Balance\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTo avoid too much conflict I won’t share which one I think is better, and am providing this information so that people are able to make a judgement call on which is more apprORPiate for people’s situations.\nLet me know your thoughts either in the comments or feel free to message on Linkedin or review my previous project of Election Analysis."
  },
  {
    "objectID": "Countries Olympic Cultural Spend.html",
    "href": "Countries Olympic Cultural Spend.html",
    "title": "Olympics by Culture and Spending",
    "section": "",
    "text": "For the purposes of this document I am going to encourage you to read the overall report that the team and I created on why we cared about the Olympics and what we sought out to figure out about the influences over the Olympics.\nThe main four components of the research were focused around:\n\nThe country’s economy\nInvestment and culture of sportiness (the individual report provided below with the research).\nGeographic characteristics\nImpact of being the host country\n\n\n\n\nTo provide the introduction to this, it’s important to know what I was most curious about when preparing and some of the topical questions that I sought to answer. So the areas I was focusing on:\n\nSportiness of the population/culture, measured through:\n\nElite Sports ranking\nHobbies involving sports\n\nWatching Sport\nHealth & Fitness\nPlaying sport\n\nSports Participation rate\n\nFinancial Metrics of sportiness\n\nFitness Applications\nFitness Spend\nGym memberships (per 100K)\nOverall country spending on sports related equipment\n\n\nThis prelude to the topic was what influences from culture and spending most impact the ability to win an Olympic medal for a country. Leading to what is the correlation and causation of the factors and number of Olympic medals won and what is the likeliness of it being chance.\nTo start the analysis, if you are curious to know what libraries were used in this investigation expand the code below to see the list, that were either utilized or tested whilst developing the code.\n\n\nRelevant libraries\n\n\n# Setting a CRAN mirror to avoid errors\noptions(repos = c(CRAN = \"https://cloud.r-project.org/\"))\n\n# Install the required packages\n# List of packages to check and install if missing\npackages &lt;- c(\n  \"priceR\", \"rvest\", \"dplyr\", \"googlesheets4\", \n  \"rnaturalearth\", \"rnaturalearthdata\", \"sf\", \n  \"xml2\", \"tidyverse\"\n)\n\n# Loop through each package\nfor (pkg in packages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE) # Load the package after installation\n  }\n}\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(priceR)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(sf)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(httr)\nlibrary(quantmod)\n\n\n\n\nSo the first source I went to for my data source, was Myprotein, and I know what you are thinking an odd location for a research database, however the Research by Adele Halsall provided a fascinating insight into overall sportiness especially in trying to measure sports culture which is never easy. She was kind enough to store her data on a google sheet, and as such that company with the googlesheets4 library meant I could extract into my table.\n\n\nPulling in MyProtein Sporties Country data\n\n\nsheet_id &lt;- \"1JMS4D9Nx-qxokAkStZ-hJ9vCobphCVqWjZHSGsodlbE\"\n#this is the sheet utilized in googlesheets after the /spreadsheets/d/sheetid\n\ngs4_deauth()\n# As it is publically accessible do not need a google account \n\n# Read the data with read_sheet pertaining to the googlesheets4 library\ndata &lt;- read_sheet(sheet_id)\n\nhead(n = 10,data)\n\n# A tibble: 10 × 12\n   Country         `Olympic Medals` Winter Olympic Medal…¹ `Elite Sport Ranking`\n   &lt;chr&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Germany                     1346                    408                     1\n 2 United States …             2522                    305                     7\n 3 Sweden                       494                    158                     4\n 4 Norway                       152                    368                     3\n 5 Finland                      303                    167                    27\n 6 Canada                       302                    199                     2\n 7 Switzerland                  192                    153                     5\n 8 Austria                       87                    232                    10\n 9 Netherlands                  285                    130                    25\n10 Australia                    507                     15                    13\n# ℹ abbreviated name: ¹​`Winter Olympic Medals`\n# ℹ 8 more variables: `Sports Participation Rate` &lt;dbl&gt;,\n#   `Gym Memberships per 100k` &lt;dbl&gt;, `Hobbies - Health & Fitness` &lt;dbl&gt;,\n#   `Hobbies - Playing Sport` &lt;dbl&gt;, `Hobbies - Watching Sport` &lt;dbl&gt;,\n#   `Fitness Apps` &lt;dbl&gt;, `Fitness Spend` &lt;dbl&gt;,\n#   `Total Score (out of 100)` &lt;dbl&gt;\n\n\n\nNow whilst the above was useful for getting the data for analysis we needed to also enhance the information with Geospatial information for the purposes of mapping the information.\n\n\n\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\") |&gt;\n#This ne_countries is from the rnaturalearth library in R and contains country based information\n  select(name) |&gt;\n  mutate(name = case_when(\n    name == \"Czechia\" ~ \"Czech Republic\",\n    name == \"U.S. Virgin Is.\" ~ \"Virgin Islands\",\n    name == \"Côte d'Ivoire\" ~ \"Ivory Coast\",\n    name == \"Dominican Rep.\" ~ \"Dominican Republic\",\n    TRUE ~ name # Keep other values unchanged\n    ))\n#Tidied up the naming conventions of countries to align with the information provided in the MyProtein Sports review\n\nggplot(world) +\n  geom_sf(aes(geometry = geometry)) +\n  labs(title = \"Geometries Plot\", x = \"Longitude\", y = \"Latitude\")\n\n\n\n\n\n\n\n#this is the same visualization as below and uses a native R library\n\n\nI have provided a visualization of the bare bones map provided in this analysis, the logic is hidden above in the code but want to share the insights generated.\n\n\n\n\n\n\n\n\n\nThen after tidying everything up we combine the two pieces of data to present the information in a plotted format, instead of as a table, as a visual learner and observer it will help to make points clearer.\n\n\nData integration and mapping\n\n\nmerged_World_data &lt;- inner_join(world,data, by = c(\"name\" = \"Country\")) |&gt;\n      mutate(`Elite Sport Ranking` = 84-`Elite Sport Ranking`)\n# the purpose of the mutate in this case is given that the lower the number the better the performance i.e. Country ranked 1st performs better we took the inverse of worst performing country for the Elite Sports ranking as this aligns with other metrics where higher is better.\n\nggplot(data = merged_World_data) +\n  geom_sf(aes(fill = `Olympic Medals`)) + # Use the 'medals' column for fill color\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Medals\") + # Color gradient for medals\n  theme_minimal() +\n  labs(\n    title = \"Olympic Medals by Country\",\n    subtitle = \"Visualizing the Number of Medals Won\",\n    caption = \"MyProtein\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext Compare each of the factors to analyse culture by mapping to each other of the 9 factors being (Olympic Medals, Winter Olympic Medals, Elite Sport Ranking, Sports Participation Rate, Gym Memberships per 100k, Hobbies - Health & Fitness, Hobbies - Watching Sport, Fitness Apps, Fitness Spend). The code below shows how it was cleaned and tidied to analyse a country comparison.\n\n\n\nTidying up Factors and presenting\n\n\nlong_data &lt;- merged_World_data |&gt;\n  pivot_longer(\n    cols = c(\n      `Olympic Medals`, `Winter Olympic Medals`, `Elite Sport Ranking`,\n      `Sports Participation Rate`, `Gym Memberships per 100k`,\n      `Hobbies - Health & Fitness`, `Hobbies - Watching Sport`,\n      `Fitness Apps`, `Fitness Spend`\n    ),\n    names_to = \"Metric\",\n    values_to = \"Value\"\n  ) |&gt;\n  group_by(Metric) %&gt;%\n  mutate(Normalized_Value = (Value - min(Value, na.rm = TRUE)) / \n           (max(Value, na.rm = TRUE) - min(Value, na.rm = TRUE))) %&gt;%\n  ungroup()\n\n# Plot using normalized values\nggplot(data = long_data) +\n  geom_sf(aes(fill = Normalized_Value), color = \"black\") + # Fill with normalized value\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Relative Value\") +\n  facet_wrap(~ Metric, ncol = 3) + # Use shared scales for geometry\n  theme_minimal() +\n  labs(\n    title = \"Comparison of Fitness and Sports Metrics by Country\",\n    subtitle = \"Normalized Values by Metric\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe next section of the data implemented more rigorous analytics as to the overlap and started to quantify the regression and causation and correlation of the data against the numbers of medals won. This is where the visualization of this data moves from graph to Table and the data enthusiasts start to really pay attention.\n\n\n\nAs mentioned below the\n\n\nRegression code\n\n\nMerged_unmapped_World_Data &lt;- merged_World_data\n#Creating a separate data table to remove the geometry or location for each country to reduce the data size\n\nMerged_unmapped_World_Data$geometry &lt;- NULL\n#Setting the geometry to 0\ny &lt;- Merged_unmapped_World_Data$`Olympic Medals` \n# Dependent variable\n\n# Independent variables\nX &lt;- Merged_unmapped_World_Data |&gt;\n  select(\n    `Elite Sport Ranking`,\n    `Sports Participation Rate`,\n    `Gym Memberships per 100k`,\n    `Hobbies - Health & Fitness`,\n    `Hobbies - Playing Sport`,\n    `Hobbies - Watching Sport`,\n    `Fitness Apps`,\n    `Fitness Spend`\n  )\n\n# COmbining all the data\nX &lt;- cbind(X)\n\n\nFor the statistical analysis the Principal Component Analysis (PCA) was utilized across the data.\n\n\nPCA Analysis\n\n\n# This is possible because ot the stats library and so easily sharable\npca &lt;- prcomp(X, scale. = TRUE)\n\n# Summary of PCA\nsummary(pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.5805 1.3685 0.9906 0.85988 0.84611 0.70834 0.61731\nProportion of Variance 0.3122 0.2341 0.1227 0.09242 0.08949 0.06272 0.04763\nCumulative Proportion  0.3122 0.5463 0.6690 0.76142 0.85091 0.91362 0.96126\n                           PC8\nStandard deviation     0.55672\nProportion of Variance 0.03874\nCumulative Proportion  1.00000\n\n# Use the first few principal components for regression\npca_data &lt;- as.data.frame(pca$x[,1:8]) # Use the first 8 components\nmerged_data_pca &lt;- cbind(y, pca_data)\n\n# Fit the regression model\nmodel_pca &lt;- lm(y ~ ., data = merged_data_pca)\n\n# Summarize the model\nsummary(model_pca)\n\n\nCall:\nlm(formula = y ~ ., data = merged_data_pca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-579.26 -103.39   -0.92   46.84 1677.87 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   115.06      20.66   5.568 1.50e-07 ***\nPC1            74.32      13.12   5.663 9.68e-08 ***\nPC2           -78.80      15.16  -5.200 7.91e-07 ***\nPC3           -12.33      20.94  -0.589 0.557082    \nPC4           -79.48      24.12  -3.295 0.001281 ** \nPC5           -37.92      24.51  -1.547 0.124435    \nPC6           102.52      29.28   3.501 0.000643 ***\nPC7            85.95      33.60   2.558 0.011717 *  \nPC8            57.80      37.26   1.552 0.123300    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 239.2 on 125 degrees of freedom\nMultiple R-squared:  0.429, Adjusted R-squared:  0.3925 \nF-statistic: 11.74 on 8 and 125 DF,  p-value: 2.319e-12\n\nprint(pca$rotation[, 1:8])\n\n                                  PC1         PC2         PC3         PC4\nElite Sport Ranking         0.2552993 -0.46600902  0.13694475 -0.05252389\nSports Participation Rate   0.4068975 -0.41141655  0.04585138  0.10751556\nGym Memberships per 100k    0.2073760 -0.52553370  0.16840441 -0.36501189\nHobbies - Health & Fitness -0.3451803 -0.28080002  0.44697589  0.40931402\nHobbies - Playing Sport    -0.3299214  0.08662523  0.68519628 -0.11859951\nHobbies - Watching Sport   -0.4549719 -0.16705869 -0.02484538 -0.57265098\nFitness Apps               -0.4034741 -0.29291979 -0.45739913 -0.20661191\nFitness Spend              -0.3584182 -0.37057139 -0.26771456  0.54792931\n                                  PC5         PC6         PC7        PC8\nElite Sport Ranking        -0.5938219  0.50321975  0.18986713 -0.2328972\nSports Participation Rate   0.0144703 -0.29515644 -0.75097286 -0.0148991\nGym Memberships per 100k    0.3755100 -0.28057869  0.47025568  0.2798986\nHobbies - Health & Fitness  0.4742064  0.21368650 -0.01411703 -0.4048995\nHobbies - Playing Sport    -0.4554738 -0.42451938 -0.04530815  0.1017949\nHobbies - Watching Sport    0.1090661  0.43008519 -0.41071805  0.2664349\nFitness Apps               -0.1442053 -0.41068154  0.05398164 -0.5541982\nFitness Spend              -0.2028292 -0.03764218  0.07067093  0.5610389\n\nmodel_refined &lt;- lm(y ~ PC1 + PC2 + PC4 + PC6 + PC7, data = merged_data_pca)\nsummary(model_refined)\n\n\nCall:\nlm(formula = y ~ PC1 + PC2 + PC4 + PC6 + PC7, data = merged_data_pca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.21 -101.61   -4.96   48.21 1691.77 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   115.06      20.84   5.522 1.79e-07 ***\nPC1            74.32      13.23   5.616 1.16e-07 ***\nPC2           -78.80      15.28  -5.156 9.31e-07 ***\nPC4           -79.48      24.32  -3.268 0.001392 ** \nPC6           102.52      29.53   3.472 0.000705 ***\nPC7            85.95      33.88   2.537 0.012383 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 241.2 on 128 degrees of freedom\nMultiple R-squared:  0.4055,    Adjusted R-squared:  0.3823 \nF-statistic: 17.46 on 5 and 128 DF,  p-value: 3.725e-13\n\nplot(model_refined$fitted.values, residuals(model_refined), xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\nqqnorm(residuals(model_refined))\nqqline(residuals(model_refined), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe next component of investigation was spending in Sports at the Retail Value RSP in local currency for each country. For this data Euromonitor, does a great job of consolidating this information so that it is easier to do an analysis and uses a variety of sources such as the SEC & IRS for the U.S numbers as official sources and many more. The first part of this was to normalize the numbers as they were in local currencies and aggregated in different capacity (some in millions, others in billions).\n\n\nEuromonitor Spend\n\n\nSports_spend &lt;- read.csv(\"Passport_Stats_10-12-2024_0544_GMT.csv\", skip = 5, header = TRUE) |&gt;\n  mutate(Currency =  substr(Unit, 1, 3)) |&gt;\n  mutate(X2023 = str_replace_all(X2023, \",\", \"\")) |&gt;\n  mutate(X2023 = as.numeric(X2023)) |&gt;\n  filter(!is.na(Category) & Category != \"\") \n#Given that Euromonitor does not have an API, and the shared license makes it difficult to parse the information, I downloaded and uploaded the information and focused on 2023\n\nhead(n = 5, Sports_spend)\n\n         Geography           Category        Data.Type        Unit\n1            China Outdoor and Sports Retail Value RSP CNY million\n2 Hong Kong, China Outdoor and Sports Retail Value RSP HKD million\n3            India Outdoor and Sports Retail Value RSP INR million\n4        Indonesia Outdoor and Sports Retail Value RSP IDR billion\n5            Japan Outdoor and Sports Retail Value RSP JPY billion\n  Current.Constant  X2023 Currency\n1   Current Prices 2458.7      CNY\n2   Current Prices   70.3      HKD\n3   Current Prices 1203.7      INR\n4   Current Prices  211.7      IDR\n5   Current Prices   13.1      JPY\n\nunique_currencies &lt;- Sports_spend |&gt;\n  distinct(Currency) |&gt;   # Get unique currencies\n  pull(Currency)    \n\n# Combine the unique currencies into a comma-separated string\ncurrency_string &lt;- paste(unique_currencies, collapse = \",\")\n\n#Setting up an api key for the extract of Exchange Rates\nExchange_rate_api &lt;- readLines(\"exchange rates api.txt\")\n\n#Downloading the exchange rates from the provided API\nExchange_rate_url &lt;- paste0(\"https://api.exchangeratesapi.io/v1/2023-12-31?access_key=\",Exchange_rate_api,\"&symbols=\",currency_string,\"&format=1\")\n\n#Download the information\nExchange_rate &lt;- GET(Exchange_rate_url)\n\n#Parse the information\nUsable_Exchange &lt;- content(Exchange_rate, \"parsed\")\n\nrates_table &lt;- as_tibble(Usable_Exchange$rates)\n\nlong_data &lt;- rates_table |&gt;\n  pivot_longer(\n    cols = everything(),    # Select all columns\n    names_to = \"Currency\",  # New column for the headers\n    values_to = \"Value\"     # New column for the values\n  )\n#Setting it up as USD related focus of investment\n\nUSD_Value &lt;- long_data |&gt;\n  filter(Currency == \"USD\") |&gt;\n  pull(Value)\n\nRates &lt;- long_data |&gt;\n  mutate(Value_USD = as.numeric(1/(Value/USD_Value)))\n#Converting everything to USD \n\nSports_spend &lt;- Sports_spend |&gt;\n  left_join(Rates,Sports_spend, by = (\"Currency\"))\n#Joining in the amount spent for Sports with the local currency adjustment to USD.\n\nSports_spend &lt;- Sports_spend |&gt;\n  mutate(X2023_USD = X2023*Value_USD,\n         Last7 = str_sub(Unit, -7),  # Extract the last 7 characters i.e. million or billion\n         Multiplier = ifelse(Last7 == \"million\", 1000000, 1000000000), # Set multiplier based on condition\n         X2023_USD = X2023_USD*Multiplier)\n\n\nAfter all of this sports spend we tie it back to the original question which is how does sports spending relate to Olympic medals won, we join the legacy data of olympic medals to the new sport spend data that was listed above.\n\n\nSport spend to Olympic Medals\n\n\nSports_spend_medals &lt;- Sports_spend |&gt;\n  mutate(Geography = case_when(\n    Geography == \"USA\" ~ \"United States of America\",\n    Geography == \"Hong Kong, China\" ~ \"Hong Kong\",\n    TRUE ~ Geography)) |&gt;\n  mutate(X2023_USD = X2023_USD/1000000) |&gt;\n  left_join(data |&gt;\n              select(`Olympic Medals`,Country), by = c(\"Geography\" = \"Country\"))\n\nmodel &lt;- lm(`Olympic Medals` ~ X2023_USD, data = Sports_spend_medals)\n\n# Display the summary of the regression model\nsummary(model)\n\n\nCall:\nlm(formula = `Olympic Medals` ~ X2023_USD, data = Sports_spend_medals)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-227.21 -189.68 -115.48   58.82 1153.54 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 203.2603    60.9761   3.333  0.00229 ** \nX2023_USD     1.2624     0.1642   7.689 1.41e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 320.4 on 30 degrees of freedom\nMultiple R-squared:  0.6634,    Adjusted R-squared:  0.6521 \nF-statistic: 59.11 on 1 and 30 DF,  p-value: 1.412e-08\n\n# Extract the formula\nformula_text &lt;- paste0(\"y = \", round(coef(model)[1], 2), \n                       \" + \", round(coef(model)[2], 2), \"x\")\n\n\nggplot(Sports_spend_medals, aes(x = X2023_USD, y = `Olympic Medals`)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  annotate(\"text\", x = 1000, y = 2000, label = formula_text, color = \"black\", size = 5, hjust = 0) +\n  labs(\n    title = \"Sports Spending vs Olympic Medals\",\n    x = \"Sports Spending in 2023 (Million USD)\",\n    y = \"Olympic Medals\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "Countries Olympic Cultural Spend.html#why-do-we-care-about-the-olympics",
    "href": "Countries Olympic Cultural Spend.html#why-do-we-care-about-the-olympics",
    "title": "Olympics by Culture and Spending",
    "section": "",
    "text": "For the purposes of this document I am going to encourage you to read the overall report that the team and I created on why we cared about the Olympics and what we sought out to figure out about the influences over the Olympics.\nThe main four components of the research were focused around:\n\nThe country’s economy\nInvestment and culture of sportiness (the individual report provided below with the research).\nGeographic characteristics\nImpact of being the host country"
  },
  {
    "objectID": "Countries Olympic Cultural Spend.html#countrys-investment-into-sports-culture",
    "href": "Countries Olympic Cultural Spend.html#countrys-investment-into-sports-culture",
    "title": "Olympics by Culture and Spending",
    "section": "",
    "text": "To provide the introduction to this, it’s important to know what I was most curious about when preparing and some of the topical questions that I sought to answer. So the areas I was focusing on:\n\nSportiness of the population/culture, measured through:\n\nElite Sports ranking\nHobbies involving sports\n\nWatching Sport\nHealth & Fitness\nPlaying sport\n\nSports Participation rate\n\nFinancial Metrics of sportiness\n\nFitness Applications\nFitness Spend\nGym memberships (per 100K)\nOverall country spending on sports related equipment\n\n\nThis prelude to the topic was what influences from culture and spending most impact the ability to win an Olympic medal for a country. Leading to what is the correlation and causation of the factors and number of Olympic medals won and what is the likeliness of it being chance.\nTo start the analysis, if you are curious to know what libraries were used in this investigation expand the code below to see the list, that were either utilized or tested whilst developing the code.\n\n\nRelevant libraries\n\n\n# Setting a CRAN mirror to avoid errors\noptions(repos = c(CRAN = \"https://cloud.r-project.org/\"))\n\n# Install the required packages\n# List of packages to check and install if missing\npackages &lt;- c(\n  \"priceR\", \"rvest\", \"dplyr\", \"googlesheets4\", \n  \"rnaturalearth\", \"rnaturalearthdata\", \"sf\", \n  \"xml2\", \"tidyverse\"\n)\n\n# Loop through each package\nfor (pkg in packages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE) # Load the package after installation\n  }\n}\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(priceR)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(sf)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(httr)\nlibrary(quantmod)\n\n\n\n\nSo the first source I went to for my data source, was Myprotein, and I know what you are thinking an odd location for a research database, however the Research by Adele Halsall provided a fascinating insight into overall sportiness especially in trying to measure sports culture which is never easy. She was kind enough to store her data on a google sheet, and as such that company with the googlesheets4 library meant I could extract into my table.\n\n\nPulling in MyProtein Sporties Country data\n\n\nsheet_id &lt;- \"1JMS4D9Nx-qxokAkStZ-hJ9vCobphCVqWjZHSGsodlbE\"\n#this is the sheet utilized in googlesheets after the /spreadsheets/d/sheetid\n\ngs4_deauth()\n# As it is publically accessible do not need a google account \n\n# Read the data with read_sheet pertaining to the googlesheets4 library\ndata &lt;- read_sheet(sheet_id)\n\nhead(n = 10,data)\n\n# A tibble: 10 × 12\n   Country         `Olympic Medals` Winter Olympic Medal…¹ `Elite Sport Ranking`\n   &lt;chr&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Germany                     1346                    408                     1\n 2 United States …             2522                    305                     7\n 3 Sweden                       494                    158                     4\n 4 Norway                       152                    368                     3\n 5 Finland                      303                    167                    27\n 6 Canada                       302                    199                     2\n 7 Switzerland                  192                    153                     5\n 8 Austria                       87                    232                    10\n 9 Netherlands                  285                    130                    25\n10 Australia                    507                     15                    13\n# ℹ abbreviated name: ¹​`Winter Olympic Medals`\n# ℹ 8 more variables: `Sports Participation Rate` &lt;dbl&gt;,\n#   `Gym Memberships per 100k` &lt;dbl&gt;, `Hobbies - Health & Fitness` &lt;dbl&gt;,\n#   `Hobbies - Playing Sport` &lt;dbl&gt;, `Hobbies - Watching Sport` &lt;dbl&gt;,\n#   `Fitness Apps` &lt;dbl&gt;, `Fitness Spend` &lt;dbl&gt;,\n#   `Total Score (out of 100)` &lt;dbl&gt;\n\n\n\nNow whilst the above was useful for getting the data for analysis we needed to also enhance the information with Geospatial information for the purposes of mapping the information.\n\n\n\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\") |&gt;\n#This ne_countries is from the rnaturalearth library in R and contains country based information\n  select(name) |&gt;\n  mutate(name = case_when(\n    name == \"Czechia\" ~ \"Czech Republic\",\n    name == \"U.S. Virgin Is.\" ~ \"Virgin Islands\",\n    name == \"Côte d'Ivoire\" ~ \"Ivory Coast\",\n    name == \"Dominican Rep.\" ~ \"Dominican Republic\",\n    TRUE ~ name # Keep other values unchanged\n    ))\n#Tidied up the naming conventions of countries to align with the information provided in the MyProtein Sports review\n\nggplot(world) +\n  geom_sf(aes(geometry = geometry)) +\n  labs(title = \"Geometries Plot\", x = \"Longitude\", y = \"Latitude\")\n\n\n\n\n\n\n\n#this is the same visualization as below and uses a native R library\n\n\nI have provided a visualization of the bare bones map provided in this analysis, the logic is hidden above in the code but want to share the insights generated.\n\n\n\n\n\n\n\n\n\nThen after tidying everything up we combine the two pieces of data to present the information in a plotted format, instead of as a table, as a visual learner and observer it will help to make points clearer.\n\n\nData integration and mapping\n\n\nmerged_World_data &lt;- inner_join(world,data, by = c(\"name\" = \"Country\")) |&gt;\n      mutate(`Elite Sport Ranking` = 84-`Elite Sport Ranking`)\n# the purpose of the mutate in this case is given that the lower the number the better the performance i.e. Country ranked 1st performs better we took the inverse of worst performing country for the Elite Sports ranking as this aligns with other metrics where higher is better.\n\nggplot(data = merged_World_data) +\n  geom_sf(aes(fill = `Olympic Medals`)) + # Use the 'medals' column for fill color\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Medals\") + # Color gradient for medals\n  theme_minimal() +\n  labs(\n    title = \"Olympic Medals by Country\",\n    subtitle = \"Visualizing the Number of Medals Won\",\n    caption = \"MyProtein\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext Compare each of the factors to analyse culture by mapping to each other of the 9 factors being (Olympic Medals, Winter Olympic Medals, Elite Sport Ranking, Sports Participation Rate, Gym Memberships per 100k, Hobbies - Health & Fitness, Hobbies - Watching Sport, Fitness Apps, Fitness Spend). The code below shows how it was cleaned and tidied to analyse a country comparison.\n\n\n\nTidying up Factors and presenting\n\n\nlong_data &lt;- merged_World_data |&gt;\n  pivot_longer(\n    cols = c(\n      `Olympic Medals`, `Winter Olympic Medals`, `Elite Sport Ranking`,\n      `Sports Participation Rate`, `Gym Memberships per 100k`,\n      `Hobbies - Health & Fitness`, `Hobbies - Watching Sport`,\n      `Fitness Apps`, `Fitness Spend`\n    ),\n    names_to = \"Metric\",\n    values_to = \"Value\"\n  ) |&gt;\n  group_by(Metric) %&gt;%\n  mutate(Normalized_Value = (Value - min(Value, na.rm = TRUE)) / \n           (max(Value, na.rm = TRUE) - min(Value, na.rm = TRUE))) %&gt;%\n  ungroup()\n\n# Plot using normalized values\nggplot(data = long_data) +\n  geom_sf(aes(fill = Normalized_Value), color = \"black\") + # Fill with normalized value\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Relative Value\") +\n  facet_wrap(~ Metric, ncol = 3) + # Use shared scales for geometry\n  theme_minimal() +\n  labs(\n    title = \"Comparison of Fitness and Sports Metrics by Country\",\n    subtitle = \"Normalized Values by Metric\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe next section of the data implemented more rigorous analytics as to the overlap and started to quantify the regression and causation and correlation of the data against the numbers of medals won. This is where the visualization of this data moves from graph to Table and the data enthusiasts start to really pay attention.\n\n\n\nAs mentioned below the\n\n\nRegression code\n\n\nMerged_unmapped_World_Data &lt;- merged_World_data\n#Creating a separate data table to remove the geometry or location for each country to reduce the data size\n\nMerged_unmapped_World_Data$geometry &lt;- NULL\n#Setting the geometry to 0\ny &lt;- Merged_unmapped_World_Data$`Olympic Medals` \n# Dependent variable\n\n# Independent variables\nX &lt;- Merged_unmapped_World_Data |&gt;\n  select(\n    `Elite Sport Ranking`,\n    `Sports Participation Rate`,\n    `Gym Memberships per 100k`,\n    `Hobbies - Health & Fitness`,\n    `Hobbies - Playing Sport`,\n    `Hobbies - Watching Sport`,\n    `Fitness Apps`,\n    `Fitness Spend`\n  )\n\n# COmbining all the data\nX &lt;- cbind(X)\n\n\nFor the statistical analysis the Principal Component Analysis (PCA) was utilized across the data.\n\n\nPCA Analysis\n\n\n# This is possible because ot the stats library and so easily sharable\npca &lt;- prcomp(X, scale. = TRUE)\n\n# Summary of PCA\nsummary(pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.5805 1.3685 0.9906 0.85988 0.84611 0.70834 0.61731\nProportion of Variance 0.3122 0.2341 0.1227 0.09242 0.08949 0.06272 0.04763\nCumulative Proportion  0.3122 0.5463 0.6690 0.76142 0.85091 0.91362 0.96126\n                           PC8\nStandard deviation     0.55672\nProportion of Variance 0.03874\nCumulative Proportion  1.00000\n\n# Use the first few principal components for regression\npca_data &lt;- as.data.frame(pca$x[,1:8]) # Use the first 8 components\nmerged_data_pca &lt;- cbind(y, pca_data)\n\n# Fit the regression model\nmodel_pca &lt;- lm(y ~ ., data = merged_data_pca)\n\n# Summarize the model\nsummary(model_pca)\n\n\nCall:\nlm(formula = y ~ ., data = merged_data_pca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-579.26 -103.39   -0.92   46.84 1677.87 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   115.06      20.66   5.568 1.50e-07 ***\nPC1            74.32      13.12   5.663 9.68e-08 ***\nPC2           -78.80      15.16  -5.200 7.91e-07 ***\nPC3           -12.33      20.94  -0.589 0.557082    \nPC4           -79.48      24.12  -3.295 0.001281 ** \nPC5           -37.92      24.51  -1.547 0.124435    \nPC6           102.52      29.28   3.501 0.000643 ***\nPC7            85.95      33.60   2.558 0.011717 *  \nPC8            57.80      37.26   1.552 0.123300    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 239.2 on 125 degrees of freedom\nMultiple R-squared:  0.429, Adjusted R-squared:  0.3925 \nF-statistic: 11.74 on 8 and 125 DF,  p-value: 2.319e-12\n\nprint(pca$rotation[, 1:8])\n\n                                  PC1         PC2         PC3         PC4\nElite Sport Ranking         0.2552993 -0.46600902  0.13694475 -0.05252389\nSports Participation Rate   0.4068975 -0.41141655  0.04585138  0.10751556\nGym Memberships per 100k    0.2073760 -0.52553370  0.16840441 -0.36501189\nHobbies - Health & Fitness -0.3451803 -0.28080002  0.44697589  0.40931402\nHobbies - Playing Sport    -0.3299214  0.08662523  0.68519628 -0.11859951\nHobbies - Watching Sport   -0.4549719 -0.16705869 -0.02484538 -0.57265098\nFitness Apps               -0.4034741 -0.29291979 -0.45739913 -0.20661191\nFitness Spend              -0.3584182 -0.37057139 -0.26771456  0.54792931\n                                  PC5         PC6         PC7        PC8\nElite Sport Ranking        -0.5938219  0.50321975  0.18986713 -0.2328972\nSports Participation Rate   0.0144703 -0.29515644 -0.75097286 -0.0148991\nGym Memberships per 100k    0.3755100 -0.28057869  0.47025568  0.2798986\nHobbies - Health & Fitness  0.4742064  0.21368650 -0.01411703 -0.4048995\nHobbies - Playing Sport    -0.4554738 -0.42451938 -0.04530815  0.1017949\nHobbies - Watching Sport    0.1090661  0.43008519 -0.41071805  0.2664349\nFitness Apps               -0.1442053 -0.41068154  0.05398164 -0.5541982\nFitness Spend              -0.2028292 -0.03764218  0.07067093  0.5610389\n\nmodel_refined &lt;- lm(y ~ PC1 + PC2 + PC4 + PC6 + PC7, data = merged_data_pca)\nsummary(model_refined)\n\n\nCall:\nlm(formula = y ~ PC1 + PC2 + PC4 + PC6 + PC7, data = merged_data_pca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.21 -101.61   -4.96   48.21 1691.77 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   115.06      20.84   5.522 1.79e-07 ***\nPC1            74.32      13.23   5.616 1.16e-07 ***\nPC2           -78.80      15.28  -5.156 9.31e-07 ***\nPC4           -79.48      24.32  -3.268 0.001392 ** \nPC6           102.52      29.53   3.472 0.000705 ***\nPC7            85.95      33.88   2.537 0.012383 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 241.2 on 128 degrees of freedom\nMultiple R-squared:  0.4055,    Adjusted R-squared:  0.3823 \nF-statistic: 17.46 on 5 and 128 DF,  p-value: 3.725e-13\n\nplot(model_refined$fitted.values, residuals(model_refined), xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\nqqnorm(residuals(model_refined))\nqqline(residuals(model_refined), col = \"red\")"
  },
  {
    "objectID": "Countries Olympic Cultural Spend.html#population-sport-spending",
    "href": "Countries Olympic Cultural Spend.html#population-sport-spending",
    "title": "Olympics by Culture and Spending",
    "section": "",
    "text": "The next component of investigation was spending in Sports at the Retail Value RSP in local currency for each country. For this data Euromonitor, does a great job of consolidating this information so that it is easier to do an analysis and uses a variety of sources such as the SEC & IRS for the U.S numbers as official sources and many more. The first part of this was to normalize the numbers as they were in local currencies and aggregated in different capacity (some in millions, others in billions).\n\n\nEuromonitor Spend\n\n\nSports_spend &lt;- read.csv(\"Passport_Stats_10-12-2024_0544_GMT.csv\", skip = 5, header = TRUE) |&gt;\n  mutate(Currency =  substr(Unit, 1, 3)) |&gt;\n  mutate(X2023 = str_replace_all(X2023, \",\", \"\")) |&gt;\n  mutate(X2023 = as.numeric(X2023)) |&gt;\n  filter(!is.na(Category) & Category != \"\") \n#Given that Euromonitor does not have an API, and the shared license makes it difficult to parse the information, I downloaded and uploaded the information and focused on 2023\n\nhead(n = 5, Sports_spend)\n\n         Geography           Category        Data.Type        Unit\n1            China Outdoor and Sports Retail Value RSP CNY million\n2 Hong Kong, China Outdoor and Sports Retail Value RSP HKD million\n3            India Outdoor and Sports Retail Value RSP INR million\n4        Indonesia Outdoor and Sports Retail Value RSP IDR billion\n5            Japan Outdoor and Sports Retail Value RSP JPY billion\n  Current.Constant  X2023 Currency\n1   Current Prices 2458.7      CNY\n2   Current Prices   70.3      HKD\n3   Current Prices 1203.7      INR\n4   Current Prices  211.7      IDR\n5   Current Prices   13.1      JPY\n\nunique_currencies &lt;- Sports_spend |&gt;\n  distinct(Currency) |&gt;   # Get unique currencies\n  pull(Currency)    \n\n# Combine the unique currencies into a comma-separated string\ncurrency_string &lt;- paste(unique_currencies, collapse = \",\")\n\n#Setting up an api key for the extract of Exchange Rates\nExchange_rate_api &lt;- readLines(\"exchange rates api.txt\")\n\n#Downloading the exchange rates from the provided API\nExchange_rate_url &lt;- paste0(\"https://api.exchangeratesapi.io/v1/2023-12-31?access_key=\",Exchange_rate_api,\"&symbols=\",currency_string,\"&format=1\")\n\n#Download the information\nExchange_rate &lt;- GET(Exchange_rate_url)\n\n#Parse the information\nUsable_Exchange &lt;- content(Exchange_rate, \"parsed\")\n\nrates_table &lt;- as_tibble(Usable_Exchange$rates)\n\nlong_data &lt;- rates_table |&gt;\n  pivot_longer(\n    cols = everything(),    # Select all columns\n    names_to = \"Currency\",  # New column for the headers\n    values_to = \"Value\"     # New column for the values\n  )\n#Setting it up as USD related focus of investment\n\nUSD_Value &lt;- long_data |&gt;\n  filter(Currency == \"USD\") |&gt;\n  pull(Value)\n\nRates &lt;- long_data |&gt;\n  mutate(Value_USD = as.numeric(1/(Value/USD_Value)))\n#Converting everything to USD \n\nSports_spend &lt;- Sports_spend |&gt;\n  left_join(Rates,Sports_spend, by = (\"Currency\"))\n#Joining in the amount spent for Sports with the local currency adjustment to USD.\n\nSports_spend &lt;- Sports_spend |&gt;\n  mutate(X2023_USD = X2023*Value_USD,\n         Last7 = str_sub(Unit, -7),  # Extract the last 7 characters i.e. million or billion\n         Multiplier = ifelse(Last7 == \"million\", 1000000, 1000000000), # Set multiplier based on condition\n         X2023_USD = X2023_USD*Multiplier)\n\n\nAfter all of this sports spend we tie it back to the original question which is how does sports spending relate to Olympic medals won, we join the legacy data of olympic medals to the new sport spend data that was listed above.\n\n\nSport spend to Olympic Medals\n\n\nSports_spend_medals &lt;- Sports_spend |&gt;\n  mutate(Geography = case_when(\n    Geography == \"USA\" ~ \"United States of America\",\n    Geography == \"Hong Kong, China\" ~ \"Hong Kong\",\n    TRUE ~ Geography)) |&gt;\n  mutate(X2023_USD = X2023_USD/1000000) |&gt;\n  left_join(data |&gt;\n              select(`Olympic Medals`,Country), by = c(\"Geography\" = \"Country\"))\n\nmodel &lt;- lm(`Olympic Medals` ~ X2023_USD, data = Sports_spend_medals)\n\n# Display the summary of the regression model\nsummary(model)\n\n\nCall:\nlm(formula = `Olympic Medals` ~ X2023_USD, data = Sports_spend_medals)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-227.21 -189.68 -115.48   58.82 1153.54 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 203.2603    60.9761   3.333  0.00229 ** \nX2023_USD     1.2624     0.1642   7.689 1.41e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 320.4 on 30 degrees of freedom\nMultiple R-squared:  0.6634,    Adjusted R-squared:  0.6521 \nF-statistic: 59.11 on 1 and 30 DF,  p-value: 1.412e-08\n\n# Extract the formula\nformula_text &lt;- paste0(\"y = \", round(coef(model)[1], 2), \n                       \" + \", round(coef(model)[2], 2), \"x\")\n\n\nggplot(Sports_spend_medals, aes(x = X2023_USD, y = `Olympic Medals`)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  annotate(\"text\", x = 1000, y = 2000, label = formula_text, color = \"black\", size = 5, hjust = 0) +\n  labs(\n    title = \"Sports Spending vs Olympic Medals\",\n    x = \"Sports Spending in 2023 (Million USD)\",\n    y = \"Olympic Medals\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "Olympic_Spend.html",
    "href": "Olympic_Spend.html",
    "title": "Olympics by Culture and Spending - The Olympic Advantage",
    "section": "",
    "text": "For the purposes of this document I am going to encourage you to read the overall report that the team and I created on why we cared about the Olympics and what we sought out to figure out about the influences over the Olympics.\nThe main four components of the research were focused around:\n\nThe country’s economy\nInvestment and culture of sportiness (the individual report provided below with the research).\nGeographic characteristics\nImpact of being the host country"
  },
  {
    "objectID": "Olympic_Spend.html#why-do-we-care-about-the-olympics",
    "href": "Olympic_Spend.html#why-do-we-care-about-the-olympics",
    "title": "Olympics by Culture and Spending - The Olympic Advantage",
    "section": "",
    "text": "For the purposes of this document I am going to encourage you to read the overall report that the team and I created on why we cared about the Olympics and what we sought out to figure out about the influences over the Olympics.\nThe main four components of the research were focused around:\n\nThe country’s economy\nInvestment and culture of sportiness (the individual report provided below with the research).\nGeographic characteristics\nImpact of being the host country"
  },
  {
    "objectID": "Olympic_Spend.html#countrys-investment-into-sports-culture",
    "href": "Olympic_Spend.html#countrys-investment-into-sports-culture",
    "title": "Olympics by Culture and Spending - The Olympic Advantage",
    "section": "Country’s investment into sports culture",
    "text": "Country’s investment into sports culture\nTo provide the introduction to this, it’s important to know what I was most curious about when preparing and some of the topical questions that I sought to answer. So the areas I was focusing on:\n\nSportiness of the population/culture, measured through:\n\nElite Sports ranking\nHobbies involving sports\n\nWatching Sport\nHealth & Fitness\nPlaying sport\n\nSports Participation rate\n\nFinancial Metrics of sportiness\n\nFitness Applications\nFitness Spend\nGym memberships (per 100K)\nOverall country spending on sports related equipment\n\n\nThis prelude to the topic was what influences from culture and spending most impact the ability to win an Olympic medal for a country. Leading to what is the correlation and causation of the factors and number of Olympic medals won and what is the likeliness of it being chance.\nTo start the analysis, if you are curious to know what libraries were used in this investigation expand the code below to see the list, that were either utilized or tested whilst developing the code.\n\n\nRelevant libraries\n\n\n# Setting a CRAN mirror to avoid errors\noptions(repos = c(CRAN = \"https://cloud.r-project.org/\"))\n\n# Install the required packages\n# List of packages to check and install if missing\npackages &lt;- c(\n  \"priceR\", \"rvest\", \"dplyr\", \"googlesheets4\", \n  \"rnaturalearth\", \"rnaturalearthdata\", \"sf\", \n  \"xml2\", \"tidyverse\"\n)\n\n# Loop through each package\nfor (pkg in packages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE) # Load the package after installation\n  }\n}\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(priceR)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(sf)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(httr)\nlibrary(quantmod)\nlibrary(knitr)\n\n\n\nGeospatial Analysis\nSo the first source I went to for my data source, was Myprotein, and I know what you are thinking an odd location for a research database, however the Research by Adele Halsall provided a fascinating insight into overall sportiness especially in trying to measure sports culture which is never easy. She was kind enough to store her data on a google sheet, and as such that company with the googlesheets4 library meant I could extract into my table.\n\n\nPulling in MyProtein Sporties Country data\n\n\nsheet_id &lt;- \"1JMS4D9Nx-qxokAkStZ-hJ9vCobphCVqWjZHSGsodlbE\"\n#this is the sheet utilized in googlesheets after the /spreadsheets/d/sheetid\n\ngs4_deauth()\n# As it is publically accessible do not need a google account \n\n# Read the data with read_sheet pertaining to the googlesheets4 library\ndata &lt;- read_sheet(sheet_id)\n\nhead(n = 10,data)\n\n# A tibble: 10 × 12\n   Country         `Olympic Medals` Winter Olympic Medal…¹ `Elite Sport Ranking`\n   &lt;chr&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Germany                     1346                    408                     1\n 2 United States …             2522                    305                     7\n 3 Sweden                       494                    158                     4\n 4 Norway                       152                    368                     3\n 5 Finland                      303                    167                    27\n 6 Canada                       302                    199                     2\n 7 Switzerland                  192                    153                     5\n 8 Austria                       87                    232                    10\n 9 Netherlands                  285                    130                    25\n10 Australia                    507                     15                    13\n# ℹ abbreviated name: ¹​`Winter Olympic Medals`\n# ℹ 8 more variables: `Sports Participation Rate` &lt;dbl&gt;,\n#   `Gym Memberships per 100k` &lt;dbl&gt;, `Hobbies - Health & Fitness` &lt;dbl&gt;,\n#   `Hobbies - Playing Sport` &lt;dbl&gt;, `Hobbies - Watching Sport` &lt;dbl&gt;,\n#   `Fitness Apps` &lt;dbl&gt;, `Fitness Spend` &lt;dbl&gt;,\n#   `Total Score (out of 100)` &lt;dbl&gt;\n\n\n\nNow whilst the above was useful for getting the data for analysis we needed to also enhance the information with Geospatial information for the purposes of mapping the information.\n\n\nPulling in the world map\n\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\") |&gt;\n#This ne_countries is from the rnaturalearth library in R and contains country based information\n  select(name) |&gt;\n  mutate(name = case_when(\n    name == \"Czechia\" ~ \"Czech Republic\",\n    name == \"U.S. Virgin Is.\" ~ \"Virgin Islands\",\n    name == \"Côte d'Ivoire\" ~ \"Ivory Coast\",\n    name == \"Dominican Rep.\" ~ \"Dominican Republic\",\n    TRUE ~ name # Keep other values unchanged\n    ))\n#Tidied up the naming conventions of countries to align with the information provided in the MyProtein Sports review\n\nggplot(world) +\n  geom_sf(aes(geometry = geometry)) +\n  labs(title = \"Geometries Plot\", x = \"Longitude\", y = \"Latitude\")\n#this is the same visualization as below and uses a native R library\n\n\nI have provided a visualization of the bare bones map provided in this analysis, the logic is hidden above in the code but want to share the insights generated.\n\n\n\n\n\n\n\n\n\nThen after tidying everything up we combine the two pieces of data to present the information in a plotted format, instead of as a table, as a visual learner and observer it will help to make points clearer.\n\n\nData integration and mapping\n\n\nmerged_World_data &lt;- inner_join(world,data, by = c(\"name\" = \"Country\")) |&gt;\n      mutate(`Elite Sport Ranking` = 84-`Elite Sport Ranking`)\n# the purpose of the mutate in this case is given that the lower the number the better the performance i.e. Country ranked 1st performs better we took the inverse of worst performing country for the Elite Sports ranking as this aligns with other metrics where higher is better.\n\nggplot(data = merged_World_data) +\n  geom_sf(aes(fill = `Olympic Medals`)) + # Use the 'medals' column for fill color\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Medals\") + # Color gradient for medals\n  theme_minimal() +\n  labs(\n    title = \"Olympic Medals by Country\",\n    subtitle = \"Visualizing the Number of Medals Won\",\n    caption = \"MyProtein\"\n  )\n#This is a duplication of the visualization naturally occuring however wanted to keep both so the code could be seen all together.\n\n\n\n\n\n\n\n\n\n\n\nNext Compare each of the factors to analyse culture by mapping to each other of the 9 factors being (Olympic Medals, Winter Olympic Medals, Elite Sport Ranking, Sports Participation Rate, Gym Memberships per 100k, Hobbies - Health & Fitness, Hobbies - Watching Sport, Fitness Apps, Fitness Spend). The code below shows how it was cleaned and tidied to analyse a country comparison.\n\n\nTidying up Factors and presenting\n\n\nlong_data &lt;- merged_World_data |&gt;\n  pivot_longer(\n    cols = c(\n      `Olympic Medals`, `Winter Olympic Medals`, `Elite Sport Ranking`,\n      `Sports Participation Rate`, `Gym Memberships per 100k`,\n      `Hobbies - Health & Fitness`, `Hobbies - Watching Sport`,\n      `Fitness Apps`, `Fitness Spend`\n    ),\n    names_to = \"Metric\",\n    values_to = \"Value\"\n  ) |&gt;\n  group_by(Metric) %&gt;%\n  mutate(Normalized_Value = (Value - min(Value, na.rm = TRUE)) / \n           (max(Value, na.rm = TRUE) - min(Value, na.rm = TRUE))) %&gt;%\n  ungroup()\n\n# Plot using normalized values\nggplot(data = long_data) +\n  geom_sf(aes(fill = Normalized_Value), color = \"black\") + # Fill with normalized value\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Relative Value\") +\n  facet_wrap(~ Metric, ncol = 3) + # Use shared scales for geometry\n  theme_minimal() +\n  labs(\n    title = \"Comparison of Fitness and Sports Metrics by Country\",\n    subtitle = \"Normalized Values by Metric\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nAs you can see above the data shows a lot of correlations that can be visually observed before being analysed so some primary comments would be between the Gym Members per 100K and the Sports Participation rate.\nAs an aside and an Australian I will note that in all of these images we perform quite successfully except for the performance in Winter Olympic Medals, which is understandable especially living so close to the beach and water it is easier for summer sports.\nSome notes for the top performer of the Number of Olympic medals of the U.S, it appears that the Elite Sports Ranking, Gym memberships per 100k and other factors quite evidently overlap, but I am interesting in hearing everyone thoughts on what other factors you would have included to account for the performance in the olympics.\nThe next section is more rigorous analytics around these factors and how they overlap by starting to quantify the regression with a causation and correlation focus of the supporting factors around the numbers of Olympic medals won. This is where the visualization of this data moves from graph to Table and the data enthusiasts start to really pay attention.\n\n\nRegression\nAs mentioned below the first part is to ensure that the data quality is high and a successful regression analysis can be performed. This is largely analyzing the table above with some additional data cleansing and pivoting to allow it to flow into the data analytics format provided by the R libraries.\n\n\nRegression code\n\n\nMerged_unmapped_World_Data &lt;- merged_World_data\n#Creating a separate data table to remove the geometry or location for each country to reduce the data size\n\nMerged_unmapped_World_Data$geometry &lt;- NULL\n#Setting the geometry to 0\ny &lt;- Merged_unmapped_World_Data$`Olympic Medals` \n# Dependent variable\n\n# Independent variables\nX &lt;- Merged_unmapped_World_Data |&gt;\n  select(\n    `Elite Sport Ranking`,\n    `Sports Participation Rate`,\n    `Gym Memberships per 100k`,\n    `Hobbies - Health & Fitness`,\n    `Hobbies - Playing Sport`,\n    `Hobbies - Watching Sport`,\n    `Fitness Apps`,\n    `Fitness Spend`\n  )\n\n# COmbining all the data\nX &lt;- cbind(X)\n\n\nFor the statistical analysis the Principal Component Analysis (PCA) was utilized across the data.\n\n\nPCA Analysis\n\n\n# This is possible because ot the stats library and so easily sharable\npca &lt;- prcomp(X, scale. = TRUE)\n\n# Use the first few principal components for regression\npca_data &lt;- as.data.frame(pca$x[,1:8]) # Use the first 8 components\nmerged_data_pca &lt;- cbind(y, pca_data)\n\n# Fit the regression model\nmodel_pca &lt;- lm(y ~ ., data = merged_data_pca)\n\n# Summarize the model code is suppressed below but used in a hidden run to show the table \nsummary(model_pca)\n\n\nCall:\nlm(formula = y ~ ., data = merged_data_pca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-579.26 -103.39   -0.92   46.84 1677.87 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   115.06      20.66   5.568 1.50e-07 ***\nPC1            74.32      13.12   5.663 9.68e-08 ***\nPC2           -78.80      15.16  -5.200 7.91e-07 ***\nPC3           -12.33      20.94  -0.589 0.557082    \nPC4           -79.48      24.12  -3.295 0.001281 ** \nPC5           -37.92      24.51  -1.547 0.124435    \nPC6           102.52      29.28   3.501 0.000643 ***\nPC7            85.95      33.60   2.558 0.011717 *  \nPC8            57.80      37.26   1.552 0.123300    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 239.2 on 125 degrees of freedom\nMultiple R-squared:  0.429, Adjusted R-squared:  0.3925 \nF-statistic: 11.74 on 8 and 125 DF,  p-value: 2.319e-12\n\nkable(pca$rotation[, 1:8])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\n\n\n\n\nElite Sport Ranking\n0.2552993\n-0.4660090\n0.1369448\n-0.0525239\n-0.5938219\n0.5032198\n0.1898671\n-0.2328972\n\n\nSports Participation Rate\n0.4068975\n-0.4114166\n0.0458514\n0.1075156\n0.0144703\n-0.2951564\n-0.7509729\n-0.0148991\n\n\nGym Memberships per 100k\n0.2073760\n-0.5255337\n0.1684044\n-0.3650119\n0.3755100\n-0.2805787\n0.4702557\n0.2798986\n\n\nHobbies - Health & Fitness\n-0.3451803\n-0.2808000\n0.4469759\n0.4093140\n0.4742064\n0.2136865\n-0.0141170\n-0.4048995\n\n\nHobbies - Playing Sport\n-0.3299214\n0.0866252\n0.6851963\n-0.1185995\n-0.4554738\n-0.4245194\n-0.0453082\n0.1017949\n\n\nHobbies - Watching Sport\n-0.4549719\n-0.1670587\n-0.0248454\n-0.5726510\n0.1090661\n0.4300852\n-0.4107180\n0.2664349\n\n\nFitness Apps\n-0.4034741\n-0.2929198\n-0.4573991\n-0.2066119\n-0.1442053\n-0.4106815\n0.0539816\n-0.5541982\n\n\nFitness Spend\n-0.3584182\n-0.3705714\n-0.2677146\n0.5479293\n-0.2028292\n-0.0376422\n0.0706709\n0.5610389\n\n\n\n\nmodel_refined &lt;- lm(y ~ PC1 + PC2 + PC4 + PC6 + PC7, data = merged_data_pca)\nsummary(model_refined)\n\n\nCall:\nlm(formula = y ~ PC1 + PC2 + PC4 + PC6 + PC7, data = merged_data_pca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.21 -101.61   -4.96   48.21 1691.77 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   115.06      20.84   5.522 1.79e-07 ***\nPC1            74.32      13.23   5.616 1.16e-07 ***\nPC2           -78.80      15.28  -5.156 9.31e-07 ***\nPC4           -79.48      24.32  -3.268 0.001392 ** \nPC6           102.52      29.53   3.472 0.000705 ***\nPC7            85.95      33.88   2.537 0.012383 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 241.2 on 128 degrees of freedom\nMultiple R-squared:  0.4055,    Adjusted R-squared:  0.3823 \nF-statistic: 17.46 on 5 and 128 DF,  p-value: 3.725e-13\n\nplot(model_refined$fitted.values, residuals(model_refined), xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\nqqnorm(residuals(model_refined))\nqqline(residuals(model_refined), col = \"red\")\n\n\nIn defining the Principal components we can see the regression provided below, and so to talk to some of the key numbers that are present, focusing below on the variables where the “loading” listed indicated the correlation or contribution of each variable to each principal component. With the following being true\n\nA high positive loading (close to +1) means the variable is strongly positively correlated with the component.\nA high negative loading (close to -1) means the variable is strongly negatively correlated with the component.\nA loading close to 0 indicates a weak relationship.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\n\n\n\n\nElite Sport Ranking\n0.26\n-0.47\n0.14\n-0.05\n-0.59\n0.50\n0.19\n-0.23\n\n\nSports Participation Rate\n0.41\n-0.41\n0.05\n0.11\n0.01\n-0.30\n-0.75\n-0.01\n\n\nGym Memberships per 100k\n0.21\n-0.53\n0.17\n-0.37\n0.38\n-0.28\n0.47\n0.28\n\n\nHobbies - Health & Fitness\n-0.35\n-0.28\n0.45\n0.41\n0.47\n0.21\n-0.01\n-0.40\n\n\nHobbies - Playing Sport\n-0.33\n0.09\n0.69\n-0.12\n-0.46\n-0.42\n-0.05\n0.10\n\n\nHobbies - Watching Sport\n-0.45\n-0.17\n-0.02\n-0.57\n0.11\n0.43\n-0.41\n0.27\n\n\nFitness Apps\n-0.40\n-0.29\n-0.46\n-0.21\n-0.14\n-0.41\n0.05\n-0.55\n\n\nFitness Spend\n-0.36\n-0.37\n-0.27\n0.55\n-0.20\n-0.04\n0.07\n0.56\n\n\n\n\n\nThis is where the analysis of probability of explanation comes into play, particularly noticed by the Pr(&gt;|t|) value and the R-squared adding some context to the explanation. With the importance of the Principal components defined by the signif. codes to show how explainable it was.\nAgain this is primarily focused on the coding for the analysis however part of analyzing any good data source is the statistics behind it and as such I won’t talk in too much depth, however provided is the statistics behind the claims made (enjoy).\n\n\n\nCall:\nlm(formula = y ~ ., data = merged_data_pca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-579.26 -103.39   -0.92   46.84 1677.87 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   115.06      20.66   5.568 1.50e-07 ***\nPC1            74.32      13.12   5.663 9.68e-08 ***\nPC2           -78.80      15.16  -5.200 7.91e-07 ***\nPC3           -12.33      20.94  -0.589 0.557082    \nPC4           -79.48      24.12  -3.295 0.001281 ** \nPC5           -37.92      24.51  -1.547 0.124435    \nPC6           102.52      29.28   3.501 0.000643 ***\nPC7            85.95      33.60   2.558 0.011717 *  \nPC8            57.80      37.26   1.552 0.123300    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 239.2 on 125 degrees of freedom\nMultiple R-squared:  0.429, Adjusted R-squared:  0.3925 \nF-statistic: 11.74 on 8 and 125 DF,  p-value: 2.319e-12\n\n\nNext stages would be looking at the refined table by removing non critical factors these would be the values where Pr(&gt;|t|) is higher than its counterparts or closer to 1 which indicates that the relationship is higher likelihood of being based on chance than actual relationship. The removed data would be PC3,5 and 8 and result in the below Regression analysis resulting in an Adjusted R-Squared value of 0.38 which is not the greatest explanation however given that this is a macro level view (i.e. country) and focusing on sports culture a social science it is a reasonable rate.\n\n\n\nCall:\nlm(formula = y ~ PC1 + PC2 + PC4 + PC6 + PC7, data = merged_data_pca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.21 -101.61   -4.96   48.21 1691.77 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   115.06      20.84   5.522 1.79e-07 ***\nPC1            74.32      13.23   5.616 1.16e-07 ***\nPC2           -78.80      15.28  -5.156 9.31e-07 ***\nPC4           -79.48      24.32  -3.268 0.001392 ** \nPC6           102.52      29.53   3.472 0.000705 ***\nPC7            85.95      33.88   2.537 0.012383 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 241.2 on 128 degrees of freedom\nMultiple R-squared:  0.4055,    Adjusted R-squared:  0.3823 \nF-statistic: 17.46 on 5 and 128 DF,  p-value: 3.725e-13\n\n\nSo some of the key takeaways across the data focusing on three of the variables:\n\nElite Sport Ranking - Countries with strong elite sport rankings tend to perform better\nSports Participation rate - Higher Sports participation rate correlate with better olympic results\nHobbies - Health & Fitness - Countries where most people focus on personal fitness as a hobby may see indirect benefits in Olympic results."
  },
  {
    "objectID": "Olympic_Spend.html#population-sport-spending",
    "href": "Olympic_Spend.html#population-sport-spending",
    "title": "Olympics by Culture and Spending - The Olympic Advantage",
    "section": "Population Sport Spending",
    "text": "Population Sport Spending\nThe next component of investigation was spending in Sports at the Retail Value RSP in local currency for each country. For this data Euromonitor, does a great job of consolidating this information so that it is easier to do an analysis and uses a variety of sources such as the SEC & IRS for the U.S numbers as official sources and many more. The first part of this was to normalize the numbers as they were in local currencies and aggregated in different capacity (some in millions, others in billions).\n\n\nEuromonitor Sports Spend file read\n\n\nSports_spend &lt;- read.csv(\"Passport_Stats_10-12-2024_0544_GMT.csv\", skip = 5, header = TRUE) |&gt;\n  mutate(Currency =  substr(Unit, 1, 3)) |&gt;\n  mutate(X2023 = str_replace_all(X2023, \",\", \"\")) |&gt;\n  mutate(X2023 = as.numeric(X2023)) |&gt;\n  filter(!is.na(Category) & Category != \"\") \n#Given that Euromonitor does not have an API, and the shared license makes it difficult to parse the information, I downloaded and uploaded the information and focused on 2023\n\nhead(n = 5, Sports_spend)\n\n         Geography           Category        Data.Type        Unit\n1            China Outdoor and Sports Retail Value RSP CNY million\n2 Hong Kong, China Outdoor and Sports Retail Value RSP HKD million\n3            India Outdoor and Sports Retail Value RSP INR million\n4        Indonesia Outdoor and Sports Retail Value RSP IDR billion\n5            Japan Outdoor and Sports Retail Value RSP JPY billion\n  Current.Constant  X2023 Currency\n1   Current Prices 2458.7      CNY\n2   Current Prices   70.3      HKD\n3   Current Prices 1203.7      INR\n4   Current Prices  211.7      IDR\n5   Current Prices   13.1      JPY\n\n\n\n\n\nExtract FX rates\n\n\nunique_currencies &lt;- Sports_spend |&gt;\n  distinct(Currency) |&gt;   # Get unique currencies\n  pull(Currency)    \n\n# Combine the unique currencies into a comma-separated string\ncurrency_string &lt;- paste(unique_currencies, collapse = \",\")\n\n#Setting up an api key for the extract of Exchange Rates\nExchange_rate_api &lt;- readLines(\"exchange rates api.txt\")\n\n#Downloading the exchange rates from the provided API\nExchange_rate_url &lt;- paste0(\"https://api.exchangeratesapi.io/v1/2023-12-31?access_key=\",Exchange_rate_api,\"&symbols=\",currency_string,\"&format=1\")\n\n#Download the information\nExchange_rate &lt;- GET(Exchange_rate_url)\n\n#Parse the information\nUsable_Exchange &lt;- content(Exchange_rate, \"parsed\")\n\nrates_table &lt;- as_tibble(Usable_Exchange$rates)\n\nlong_data &lt;- rates_table |&gt;\n  pivot_longer(\n    cols = everything(),    # Select all columns\n    names_to = \"Currency\",  # New column for the headers\n    values_to = \"Value\"     # New column for the values\n  )\n#Setting it up as USD related focus of investment\n\nUSD_Value &lt;- long_data |&gt;\n  filter(Currency == \"USD\") |&gt;\n  pull(Value)\n\nRates &lt;- long_data |&gt;\n  mutate(Value_USD = as.numeric(1/(Value/USD_Value)))\n#Converting everything to USD \n\nSports_spend &lt;- Sports_spend |&gt;\n  left_join(Rates,Sports_spend, by = (\"Currency\"))\n#Joining in the amount spent for Sports with the local currency adjustment to USD.\n\nSports_spend &lt;- Sports_spend |&gt;\n  mutate(X2023_USD = X2023*Value_USD,\n         Last7 = str_sub(Unit, -7),  # Extract the last 7 characters i.e. million or billion\n         Multiplier = ifelse(Last7 == \"million\", 1000000, 1000000000), # Set multiplier based on condition\n         X2023_USD = X2023_USD*Multiplier)\n\nkable(Sports_spend)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeography\nCategory\nData.Type\nUnit\nCurrent.Constant\nX2023\nCurrency\nValue\nValue_USD\nX2023_USD\nLast7\nMultiplier\n\n\n\n\nChina\nOutdoor and Sports\nRetail Value RSP\nCNY million\nCurrent Prices\n2458.7\nCNY\n7.825079\n0.1412709\n347342777\nmillion\n1e+06\n\n\nHong Kong, China\nOutdoor and Sports\nRetail Value RSP\nHKD million\nCurrent Prices\n70.3\nHKD\n8.632571\n0.1280564\n9002365\nmillion\n1e+06\n\n\nIndia\nOutdoor and Sports\nRetail Value RSP\nINR million\nCurrent Prices\n1203.7\nINR\n92.027926\n0.0120122\n14459061\nmillion\n1e+06\n\n\nIndonesia\nOutdoor and Sports\nRetail Value RSP\nIDR billion\nCurrent Prices\n211.7\nIDR\n17013.075723\n0.0000650\n13755598\nbillion\n1e+09\n\n\nJapan\nOutdoor and Sports\nRetail Value RSP\nJPY billion\nCurrent Prices\n13.1\nJPY\n155.900774\n0.0070908\n92889042\nbillion\n1e+09\n\n\nMalaysia\nOutdoor and Sports\nRetail Value RSP\nMYR million\nCurrent Prices\n77.7\nMYR\n5.079529\n0.2176296\n16909822\nmillion\n1e+06\n\n\nPhilippines\nOutdoor and Sports\nRetail Value RSP\nPHP million\nCurrent Prices\n474.9\nPHP\n61.238406\n0.0180517\n8572742\nmillion\n1e+06\n\n\nSingapore\nOutdoor and Sports\nRetail Value RSP\nSGD million\nCurrent Prices\n12.2\nSGD\n1.458677\n0.7578484\n9245750\nmillion\n1e+06\n\n\nSouth Korea\nOutdoor and Sports\nRetail Value RSP\nKRW billion\nCurrent Prices\n15.1\nKRW\n1431.045823\n0.0007725\n11664466\nbillion\n1e+09\n\n\nTaiwan\nOutdoor and Sports\nRetail Value RSP\nTWD million\nCurrent Prices\n330.1\nTWD\n33.919310\n0.0325908\n10758209\nmillion\n1e+06\n\n\nThailand\nOutdoor and Sports\nRetail Value RSP\nTHB million\nCurrent Prices\n347.6\nTHB\n38.066335\n0.0290403\n10094392\nmillion\n1e+06\n\n\nAustralia\nOutdoor and Sports\nRetail Value RSP\nAUD million\nCurrent Prices\n133.7\nAUD\n1.622777\n0.6812125\n91078113\nmillion\n1e+06\n\n\nPoland\nOutdoor and Sports\nRetail Value RSP\nPLN million\nCurrent Prices\n202.3\nPLN\n4.344490\n0.2544501\n51475259\nmillion\n1e+06\n\n\nRomania\nOutdoor and Sports\nRetail Value RSP\nRON million\nCurrent Prices\n51.8\nRON\n4.978393\n0.2220508\n11502230\nmillion\n1e+06\n\n\nRussia\nOutdoor and Sports\nRetail Value RSP\nRUB million\nCurrent Prices\n14083.7\nRUB\n98.661662\n0.0112045\n157801018\nmillion\n1e+06\n\n\nUkraine\nOutdoor and Sports\nRetail Value RSP\nUAH million\nCurrent Prices\n211.2\nUAH\n42.111813\n0.0262505\n5544105\nmillion\n1e+06\n\n\nArgentina\nOutdoor and Sports\nRetail Value RSP\nARS million\nCurrent Prices\n7867.7\nARS\n896.324454\n0.0012333\n9703402\nmillion\n1e+06\n\n\nBrazil\nOutdoor and Sports\nRetail Value RSP\nBRL million\nCurrent Prices\n422.6\nBRL\n5.364221\n0.2060795\n87089198\nmillion\n1e+06\n\n\nMexico\nOutdoor and Sports\nRetail Value RSP\nMXN million\nCurrent Prices\n1182.3\nMXN\n18.764278\n0.0589128\n69652594\nmillion\n1e+06\n\n\nSouth Africa\nOutdoor and Sports\nRetail Value RSP\nZAR million\nCurrent Prices\n536.9\nZAR\n20.224535\n0.0546592\n29346500\nmillion\n1e+06\n\n\nUnited Arab Emirates\nOutdoor and Sports\nRetail Value RSP\nAED million\nCurrent Prices\n75.5\nAED\n4.060068\n0.2722752\n20556781\nmillion\n1e+06\n\n\nCanada\nOutdoor and Sports\nRetail Value RSP\nCAD million\nCurrent Prices\n210.8\nCAD\n1.464723\n0.7547202\n159095013\nmillion\n1e+06\n\n\nUSA\nOutdoor and Sports\nRetail Value RSP\nUSD million\nCurrent Prices\n1985.5\nUSD\n1.105456\n1.0000000\n1985500000\nmillion\n1e+06\n\n\nFrance\nOutdoor and Sports\nRetail Value RSP\nEUR million\nCurrent Prices\n356.6\nEUR\n1.000000\n1.1054560\n394205610\nmillion\n1e+06\n\n\nGermany\nOutdoor and Sports\nRetail Value RSP\nEUR million\nCurrent Prices\n176.6\nEUR\n1.000000\n1.1054560\n195223530\nmillion\n1e+06\n\n\nItaly\nOutdoor and Sports\nRetail Value RSP\nEUR million\nCurrent Prices\n90.8\nEUR\n1.000000\n1.1054560\n100375405\nmillion\n1e+06\n\n\nNetherlands\nOutdoor and Sports\nRetail Value RSP\nEUR million\nCurrent Prices\n57.5\nEUR\n1.000000\n1.1054560\n63563720\nmillion\n1e+06\n\n\nSpain\nOutdoor and Sports\nRetail Value RSP\nEUR million\nCurrent Prices\n84.9\nEUR\n1.000000\n1.1054560\n93853214\nmillion\n1e+06\n\n\nSweden\nOutdoor and Sports\nRetail Value RSP\nSEK million\nCurrent Prices\n397.2\nSEK\n11.152225\n0.0991243\n39372154\nmillion\n1e+06\n\n\nSwitzerland\nOutdoor and Sports\nRetail Value RSP\nCHF million\nCurrent Prices\n58.1\nCHF\n0.929683\n1.1890677\n69084832\nmillion\n1e+06\n\n\nTurkey\nOutdoor and Sports\nRetail Value RSP\nTRY million\nCurrent Prices\n386.3\nTRY\n32.463169\n0.0340526\n13154528\nmillion\n1e+06\n\n\nUnited Kingdom\nOutdoor and Sports\nRetail Value RSP\nGBP million\nCurrent Prices\n162.4\nGBP\n0.868386\n1.2730007\n206735316\nmillion\n1e+06\n\n\n\n\n\n\nAggregated USD amount spent on sports equipment in 2023 for 34 countries, to provide some context as to how much each country spends on sports. This will be cross referenced against the number of medals won.\n\n\n\n\n\nGeography\nUSD Amount\n\n\n\n\nUSA\n1985500000\n\n\nFrance\n394205610\n\n\nChina\n347342777\n\n\nUnited Kingdom\n206735316\n\n\nGermany\n195223530\n\n\nCanada\n159095013\n\n\nRussia\n157801018\n\n\nItaly\n100375405\n\n\nSpain\n93853214\n\n\nJapan\n92889042\n\n\nAustralia\n91078113\n\n\nBrazil\n87089198\n\n\nMexico\n69652594\n\n\nSwitzerland\n69084832\n\n\nNetherlands\n63563720\n\n\nPoland\n51475259\n\n\nSweden\n39372154\n\n\nSouth Africa\n29346500\n\n\nUnited Arab Emirates\n20556781\n\n\nMalaysia\n16909822\n\n\nIndia\n14459061\n\n\nIndonesia\n13755598\n\n\nTurkey\n13154528\n\n\nSouth Korea\n11664466\n\n\nRomania\n11502230\n\n\nTaiwan\n10758209\n\n\nThailand\n10094392\n\n\nArgentina\n9703402\n\n\nSingapore\n9245750\n\n\nHong Kong, China\n9002365\n\n\nPhilippines\n8572742\n\n\nUkraine\n5544105\n\n\n\n\n\nAfter all of this sports spend we tie it back to the original question which is how does sports spending relate to Olympic medals won, we join the legacy data of Olympic medals to the new sport spend data that was listed above.\nIt is interesting to compare the R-squared values in the Olympic medal regression analysis where this model of current spend (i.e. 2023 USD in millions) is a better estimator of winning an Olympic medal compared to the previous analysis focused on hobbies and International sports ranking. The confidence P-value is indicative of this, feel free to further dive into the data below.\n\n\nRegression of Medals to Amount spent\n\n\nSports_spend_medals &lt;- Sports_spend |&gt;\n  mutate(Geography = case_when(\n    Geography == \"USA\" ~ \"United States of America\",\n    Geography == \"Hong Kong, China\" ~ \"Hong Kong\",\n    TRUE ~ Geography)) |&gt;\n  mutate(X2023_USD = X2023_USD/1000000) |&gt;\n  left_join(data |&gt;\n              select(`Olympic Medals`,Country), by = c(\"Geography\" = \"Country\"))\n\nmodel &lt;- lm(`Olympic Medals` ~ X2023_USD, data = Sports_spend_medals)\n\n# Display the summary of the regression model (code suppressed here and hidden overall)\nsummary(model)\n\n\nCall:\nlm(formula = `Olympic Medals` ~ X2023_USD, data = Sports_spend_medals)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-227.21 -189.68 -115.48   58.82 1153.54 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 203.2603    60.9761   3.333  0.00229 ** \nX2023_USD     1.2624     0.1642   7.689 1.41e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 320.4 on 30 degrees of freedom\nMultiple R-squared:  0.6634,    Adjusted R-squared:  0.6521 \nF-statistic: 59.11 on 1 and 30 DF,  p-value: 1.412e-08\n\n\n\n\n\n\nCall:\nlm(formula = `Olympic Medals` ~ X2023_USD, data = Sports_spend_medals)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-227.21 -189.68 -115.48   58.82 1153.54 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 203.2603    60.9761   3.333  0.00229 ** \nX2023_USD     1.2624     0.1642   7.689 1.41e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 320.4 on 30 degrees of freedom\nMultiple R-squared:  0.6634,    Adjusted R-squared:  0.6521 \nF-statistic: 59.11 on 1 and 30 DF,  p-value: 1.412e-08\n\n\nTo further cement all of this, a graph has been provided below with 95% confidence intervals to highlight the data spent, obviously this is difficult due to the closer grouping of the data to the under $250 million USD spent on sport related items. Further analysis could definitely be performed here to complete a logarithmic analysis or some alternative type.\n\n\nPlotting Spend vs Olympic medals\n\n\n# Extract the formula\nformula_text &lt;- paste0(\"y = \", round(coef(model)[1], 2), \n                       \" + \", round(coef(model)[2], 2), \"x\")\n\n\nggplot(Sports_spend_medals, aes(x = X2023_USD, y = `Olympic Medals`)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  annotate(\"text\", x = 1000, y = 2000, label = formula_text, color = \"black\", size = 5, hjust = 0) +\n  labs(\n  title = \"Sports Spending vs Olympic Medals\",\n  x = \"Sports Spending in 2023 (Million USD)\",\n  y = \"Olympic Medals\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nAll of this shows the more that a country spends the more Olympic medals that are won with it only costing roughly a million USD for population per medal won.\nThank you for reading the analysis, I would highly recommend again reviewing my colleagues research into the other 4 components or the over-view non technical analysis. Otherwise feel free to connect via Linkedin to discuss this topic further, or generally to talk about coding, financial services or low-code no code tools as that is my specialty.\nReferences:\n\nHalsall, A. (2021, July 8). Which are the world’s sportiest countries? - MYPROTEINTM. MYPROTEIN. https://us.myprotein.com/thezone/motivation/which-are-the-worlds-sportiest-countries/\nEuromonitor International. (2022). Outdoor and sports market size dataset. [Data file]. Retrieved from https://www.euromonitor.com\nExchange Rates API. (n.d.). Documentation. Retrieved from https://exchangeratesapi.io/documentation"
  }
]